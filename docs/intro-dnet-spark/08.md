# 八、批处理模式处理

在这一章中，我们将学习如何使用。NET for Apache Spark。我们将展示典型的数据处理作业如何读取源数据并解析数据，包括处理源文件中可能存在的任何异常，然后将文件写出为其他数据使用者可以使用的通用格式。

## 不完整的源数据

当我们处理数据源时，文件很少处于可以处理的完美状态；我们经常要做一些整理数据的工作，在我们本章将要用到的例子中，情况一如既往。我们将使用天然气和电力市场的政府监管机构 Ofgem 在英国发布的一些数据。我是通过浏览英国政府开放数据网站找到这些数据的。这些文件是一个有用的例子，因为它们有几个典型的问题，我们需要在处理数据时处理。

## 源数据文件

本例所需的数据文件可以从

*   [T2`www.ofgem.gov.uk/system/files/docs/2017/05/12_mar_2017_over_25k_spend_report.csv`](https://www.ofgem.gov.uk/system/files/docs/2017/05/12_mar_2017_over_25k_spend_report.csv)

*   [T2`www.ofgem.gov.uk/system/files/docs/2017/05/01_apr_2017_over_25k_spend_report.csv`](http://www.ofgem.gov.uk/system/files/docs/2017/05/01_apr_2017_over_25k_spend_report.csv)

*   [T2`www.ofgem.gov.uk/system/files/docs/2017/06/02_may_2017_over_25k_spend_report.csv`](http://www.ofgem.gov.uk/system/files/docs/2017/06/02_may_2017_over_25k_spend_report.csv)

*   [T2`www.ofgem.gov.uk/system/files/docs/2017/09/03_jun_2017_over_25k_spend_report.csv`](http://www.ofgem.gov.uk/system/files/docs/2017/09/03_jun_2017_over_25k_spend_report.csv)

*   [T2`www.ofgem.gov.uk/system/files/docs/2017/09/04_jul_2017_over_25k_spend_report.csv`](http://www.ofgem.gov.uk/system/files/docs/2017/09/04_jul_2017_over_25k_spend_report.csv)

*   [T2`www.ofgem.gov.uk/system/files/docs/2017/03/11_feb_2017_over_25k_spend_report.csv`](https://www.ofgem.gov.uk/system/files/docs/2017/03/11_feb_2017_over_25k_spend_report.csv)

如果我们检查文件，它们都是 CSV 文件，第一个文件的前几行在清单 [8-1](#PC1) 中。

```cs
Over 25k Expenditure Report,,,,,,,,,
Date,Expense Type,Expense Area,Supplier,Reference, Amount ,,,,
March 2017,Building Rates,Corporate Services,CITY OF WESTMINSTER,58644," £1,807,657.66 ",,,,
March 2017,Building Rent,Corporate Services,CB RICHARD ELLIS,58332," £1,488,000.00 ",,,,
March 2017,Consultancy Fees, Ofgem ,PRICEWATERHOUSECOOPERS,58660," £187,870.80 ",,,,

Listing 8-1The first few lines of 12_mar_2017_over_25k_spend_report.csv

```

需要注意的是

1.  第一行是多余的；“Over 25k Expenditure Report，，，，，，，”使它在电子表格中看起来很好，但我们需要先读取第二行的实际列名。

2.  每行都有几个空列，每个文件都有不同数量的空列。

3.  日期格式很奇怪，因为大多数文件都遵循“月+年”的模式，但是至少有一个文件的日期格式是“月-年”。

4.  Amount 列包含填充符、符号和逗号，不便于转换为数值。

5.  在文件的底部，这里没有显示，有几个空行。

考虑到这些问题，当我们读取文件时，我们需要做一些额外的工作来使数据可供查询。

## 数据管道

我们将按照以下步骤创建一个数据管道，该管道将读入源文件，并一次一个地将它们处理到数据湖中:

1.  读取每个 CSV 源文件。

2.  删除空行。

3.  使用第二行中的列标题为每一列指定正确的名称。

4.  删除第一行，这是一个多余的标题。

5.  将日期转换成可用的日期类型。

6.  将金额转换成可用的数字类型。

7.  将更多可用的数据写入数据湖的“结构化”区域。

8.  使用结构化数据运行一些数据验证规则。

9.  将经过验证的数据写入数据湖的“管理”区域，按月和年对数据进行分区。

10.  最后，获取经过整理的数据，并将其写成 delta 格式，以便下游流程和报告可以使用这些数据。

获取源数据并将其转换为已知的结构，然后获取数据并进行管理，然后发布，这是使用数据湖的典型模式。您可能不会使用完全相同的术语“来源”、“结构化”、“策划”和“发布”,但可能会有一些变化。使用这些不同的区域可能看起来过于复杂，但是它允许我们确定我们在不同的区域中有什么。在表 [8-1](#Tab1) 中，我们看一下每个区域的用途，以及从中读取数据时我们可以期待什么。

表 8-1

数据湖的不同区域

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

面积

 | 

描述

 |
| --- | --- |
| 来源 | 这是原始的源数据，无论源系统以何种格式提供数据。它通常不能直接使用，需要经过处理才能使用 |
| 结构化的 | 原始数据已被解析为通用格式；该数据尚未经过验证，但将采用比原始数据更易于阅读的通用格式。该区域通常是数据最后一次以与接收时相同的方式存储时的位置，即列名和与接收时相同的文件集 |
| 当（博物馆、美术馆、图书馆）馆长 | 在这方面，数据已经过验证，可以使用了。我们通常会将数据视为一个完整的数据集，包含所有日期、月份和年份的数据，而不是单独的文件 |
| 出版 | 在这个领域，数据通常被转换成某种模型，要么是维度建模，要么是数据仓库建模。该区域中的数据将由报告工具或高级用户使用 |

在本章中，我们将浏览清单 [8-1](#PC1) 和 [8-2](#PC2) ，它们是管道的完整 C#和 F#版本，但是我们将一步一步地解释每一个。首先，我们将通过 C#版本，然后是 F#版本，因为实际的实现是不同的。然而，两者都实现了相同的结果，尽管由于语言的差异而略有不同。要完成这些示例，您应该从已经给出的 URL 下载文件，并使用命令行参数将每个文件传递给您的应用程序。

编写数据管道时，有几条信息可以传递到应用程序中，其中最主要的是数据湖的路径。在我们的示例中，我们将引用一个本地文件夹，并使用它在本地测试管道。尽管如此，Apache Spark 的简单性意味着我们可以在开发和测试时写入本地文件系统，然后通过更改配置和数据存储的路径或 URL，在 Azure 存储帐户或 AWS S3 桶或 Hadoop 中传递数据湖的路径，而不必更改代码。

### C#数据管道

我们现在将看看如何用 C#构建这个数据管道；因为 F#实现略有不同，你可以在本章的后面找到 F#实现。

在清单 [8-2](#PC2) 中，我们验证了传入数据管道的参数，这些参数应该是数据湖的根路径、源文件以及文件所在的年份和月份。

```cs
if (args.Length != 4)
{
    Console.WriteLine($"Error, incorrect args. Expecting 'Data Lake Path' 'file path' 'year' 'month', got: {args}");
    return;
}

var spark = SparkSession.Builder()
    .Config("spark.sql.sources.partitionOverwriteMode", "dynamic")
    .Config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .GetOrCreate();

var dataLakePath = args[0];
var sourceFile = args[1];
var year = args[2];
var month = args[3];

const string sourceSystem = "ofgem";
const string entity = "over25kexpenses";

Listing 8-2Handling command-line arguments and getting setup

```

这里，我们对参数进行一些基本的验证，然后创建一个`SparkSession`。`SparkSession`上设置了一个选项。选项是`"spark.sql.sources.partitionOverwriteMode"`，它允许我们覆盖一个特定的分区，而不是一个完整的表。这很有用，因为当我们写入数据湖的管理部分时，我们将按年和月对数据进行分区。因为每个档案都是一个月的，如果有更正，档案可以重新印发；我们希望处理一个月的数据，并覆盖该月已经存在的任何内容，但不是整个表。如果没有这个选项，我们要么不能覆盖一个分区，要么用单个分区覆盖整个表。

然后我们做一些简单的参数解析。您可能会在参数解析中添加额外的验证，或者在实际应用程序中使用一个库来解析参数。

在清单 [8-3](#PC3) 中，我们创建了一个编排管道的方法；步骤是从源文件读取，写入结构化区域，然后写入管理区域，最后在发布区域创建最终结果。

```cs
private static void ProcessEntity(SparkSession spark, string sourceFile, string dataLakePath, string sourceSystem, string entity, string year, string month)
{
    var data = OfgemExpensesEntity.ReadFromSource(spark, sourceFile);

    OfgemExpensesEntity.WriteToStructured(data,$"{dataLakePath}/structured/{sourceSystem}/{entity}/{year}/{month}");
    OfgemExpensesEntity.WriteToCurated(data, $"{dataLakePath}/curated/{sourceSystem}/{entity}");

    OfgemExpensesEntity.WriteToPublish(spark, $"{dataLakePath}/curated/{sourceSystem}/{entity}", $"{dataLakePath}/publish/{sourceSystem}/{entity}");
}

Listing 8-3Orchestrate the pipeline

```

我们做的第一件事是读取源文件，并做一些处理以获得正确的列和数据类型，这样我们就可以在结构化区域中写入我们喜欢的格式。清单 [8-4](#PC4) 显示了使用我们需要的选项读取源文件。

```cs
var dataFrame = spark.Read().Format("csv").Options(
    new Dictionary<string, string>()
    {
        {"inferSchema", "false"},
        {"header", "false" },
        {"encoding", "ISO-8859-1"},
        {"locale", "en-GB"},
        {"quote", "\""},
        {"ignoreLeadingWhiteSpace", "true"},
        {"ignoreTrailingWhiteSpace", "true"},
        {"dateFormat", "M y"}
    }
).Load(path);

Listing 8-4Reading the source file with the correct options

```

这组特殊的文件有一个额外的标题行，我们需要跳过。这里我们可以采用几种不同的方法，比如在使用 Apache Spark 之前，对文件进行预处理以删除多余的标题行。如果我用类似 SSIS 的语言编写这个数据管道，那么我可能会对文件进行预处理。在清单 [8-5](#PC5) 中，我们展示了如何在不使用列标题的情况下读入整个文件，方法是给行添加一个索引，这样我们可以过滤掉前两行，但使用第二行作为列名的来源。此类文件的一个潜在问题是数据源提供者添加或更改列，因此我们需要小心依赖名称的列排序。使用提供的列名通常更可靠。

```cs
private static readonly List<string> ColumnsToKeep = new List<string>()
    {
        "Date","Expense Type","Expense Area","Supplier","Reference", "Amount"
    };

//Add an index column with an increasing id for each row
var dataFrameWithId = dataFrame.WithColumn("index", MonotonicallyIncreasingId());

//Pull out the column names
var header = dataFrameWithId.Filter(Col("index") == 1).Collect();

//filter out the header rows
var filtered = dataFrameWithId.Filter(Col("index") > 1).Drop("index");

var columnNames = new List<string>();
var headerRow = header.First();

for (int i = 0; i < headerRow.Values.Length; i++)
{
    if (headerRow[i] == null || !ColumnsToKeep.Contains(headerRow[i]))
    {
        Console.WriteLine($"DROPPING: _c{i}");
        filtered = filtered.Drop($"_c{i}");
    }
    else
    {

        columnNames.Add((headerRow[i] as string).Replace(" ", "_"));
    }
}

var output = filtered.ToDF(columnNames.ToArray());

Listing 8-5Using an index to skip over the additional header row and then using the column headers to name the columns and ignore any additional empty columns

```

这里的关键是我们使用了函数`MonotonicallyIncreasingId()`，它为每一行提供了一个索引号，我们可以用它来过滤。然后我们删除任何我们不需要的列，将实际的标题行作为一个字符串数组读取，并获取过滤后的数据，即没有两个标题行的实际数据，并调用`filtered.ToDF`，传入列名。这给了我们一个`DataFrame`，在这里我们可以使用列名来引用列。

因为源数据文件包括几行完全为空的数据，所以在清单 [8-6](#PC6) 中，我们过滤掉没有供应商或参考的数据。

```cs
output = output.Filter(Col("Reference").IsNotNull() & Col("Supplier").IsNotNull());

Listing 8-6Filtering out rows that are empty

```

现在我们只有感兴趣的行。我们将修复金额列。在清单 [8-7](#PC7) 中，我们展示了如何删除 Amount 列中多余的" "和" "，并将数据转换为实际的数值，在本例中是一个浮点数。

```cs
output = output.WithColumn("Amount", RegexpReplace(Col("Amount"), "[£,]", ""));
output = output.WithColumn("Amount", Col("Amount").Cast("float"));

Listing 8-7Turning the Amount string into a usable value

```

下一个要处理的列是“日期”列，他们通常使用一种日期格式，但是在一些文件中，他们使用不同的日期格式，所以我们需要能够满足两种可能性。为了处理这个问题，在清单 [8-8](#PC8) 中，我们复制了一个现有的列，然后尝试转换日期列。如果转换的结果是日期列中的所有值都为空，那么我们将再次尝试使用备用日期格式。

```cs
output = output.WithColumn("OriginalDate", Col("Date"));
output = output.WithColumn("Date", ToDate(Col("Date"), "MMMM yyyy"));

if (output.Filter(Col("Date").IsNull()).Count() == output.Count())
{
    Console.WriteLine("Trying alternate date format...");
    output = output.WithColumn("Date", ToDate(Col("OriginalDate"), "MMM-yy"));
}

output = output.Drop("OriginalDate");

return output;

Listing 8-8Dealing with multiple date formats

```

最后，我们返回应该具有的数据帧

*   正确的列标题

*   删除任何空列/行

*   正确的数据类型

因为我们已经做了相当多的工作才能够读取这些文件，所以用相同的原始数据保存这些文件，但以一种更容易读取的格式保存，通常是有用的。在清单 [8-9](#PC9) 中，我们将把文件作为一个拼花文件写到数据湖的“结构化”区域。

```cs
    public static void WriteToStructured(DataFrame data, string path)
    {
        data.Write().Mode("overwrite").Format("parquet").Save(path);
    }

Listing 8-9Writing out the raw data in a format that can be easily consumed

```

我们传入的路径已经限定了年和月的范围，所以我们可以覆盖那里的任何内容；否则，我们希望确保不会覆盖其他月份的数据。

在下一阶段，我们将写入“管理的”区域，这意味着我们需要执行一些验证，以确保我们只引入有效的数据。

在清单 [8-10](#PC10) 中，我们将展示第一次验证，以确保我们拥有的模式与预期的模式相匹配。这将验证是否存在正确的列以及数据类型是否正确。在这种情况下，我们只进行等式匹配，以确保模式是相同的。在一些系统中，我们可能想要迭代列，并检查我们至少有 x 个 y 类型的列。

```cs
StructType _expectedSchema = new StructType(new List<StructField>()
    {
        new StructField("Date", new DateType()),
        new StructField("Expense_Type", new StringType()),
        new StructField("Expense_Area", new StringType()),
        new StructField("Supplier", new StringType()),
        new StructField("Reference", new StringType()),
        new StructField("Amount", new FloatType())
    });

if (data.Schema().Json != _expectedSchema.Json)
        {
            Console.WriteLine("Expected Schema Does NOT Match");
            Console.WriteLine("Actual Schema: " + data.Schema().SimpleString);
            Console.WriteLine("Expected Schema: " + _expectedSchema.SimpleString);
            ret = false;
        }

Listing 8-10Validating the DataFrame schema

```

在清单 [8-11](#PC11) 中，我们将展示接下来的两个检查，即验证我们在日期列的每一行中都有一个值，并检查我们是否有任何数据。

```cs
if (data.Filter(Col("Date").IsNotNull()).Count() == 0)
        {
            Console.WriteLine("Date Parsing resulted in all NULL's");
            ret = false;
        }

        if (data.Count() == 0)
        {
            Console.WriteLine("DataFrame is empty");
            ret = false;
        }

Listing 8-11Validating the date column, and we have at least one row

```

最终的检查更多的是一个业务规则，数据包含每个在一个月内收费超过 25，000 的供应商，所以我们检查每个金额是否超过 25，000。然而，有时一个供应商提供不同的服务。每项服务可能少于 25，000，因此我们需要聚合供应商列并合计金额，然后过滤以查看是否有低于 25，000 的服务。在清单 [8-12](#PC12) 中，我们将展示如何使用 GroupBy 函数进行聚合。

```cs
var amountBySuppliers = data.GroupBy(Col("Supplier")).Sum("Amount")
    .Filter(Col("Sum(Amount)") < 25000);

if (amountBySuppliers.Count() > 0)
{
    Console.WriteLine("Amounts should only ever be over 25k");
    amountBySuppliers.Show();
    ret = false;
}

Listing 8-12Using GroupBy and Sum to get a total amount for each supplier

```

一旦数据得到验证，我们要么写出正确的数据，要么如果验证失败，就把它作为错误写出，以便以后调查。在清单 [8-13](#PC13) 中，我们展示了如何写数据，但是我们写的不是单个文件，而是文件的其余数据，并按月和年对其进行分区。如果任何人需要读取数据，他们可以从一个地方读取，使用过滤只读取他们感兴趣的年份和月份。

```cs
if (ValidateEntity(data))
{
    data.WithColumn("year", Year(Col("Date")))
        .WithColumn("month", Month(Col("Date")))
        .Write()
        .PartitionBy("year", "month")
        .Mode("overwrite")
        .Parquet(path);
}
else
{
    Console.WriteLine("Validation Failed, writing failed file.");
    data.Write().Mode("overwrite").Parquet($"{path}-failed");
}

Listing 8-13Writing the data into a common area using partitioning to keep it isolated from other years and months

```

在这一点上，我们有原始数据，在“结构化”区域中我们有更直接的格式供其他人阅读的数据，在“策划”区域中我们有经过验证的数据。最后一步是写入数据湖的“发布”区域。

“发布”区域中的数据有两个特征。首先，我们将应用一些数据建模，而不是只有一个大表，我们将使用一个事实表和一个维度表，用于可以移动到维度中的每个属性。第二个特性是我们将使用 delta 格式来写文件。delta 格式允许我们合并更改，所以如果我们重新处理一个文件，那么任何更新都将被合并到数据中。delta 格式为我们提供了各种有用的有趣属性，比如我们通常与 RDBMS 联系在一起的 ACID 属性。这些酸性给了我们

*   **原子性**—写操作完成或未完成，没有部分完成。

*   **一致性**–无论何时有人试图读取数据，数据总是处于有效状态。

*   **隔离**—多个并发写入不会损坏数据。

*   **耐久性**—一旦写入操作完成，它将保持写入状态，无论系统是否出现故障。

知道了我们可以让多个 ETL 作业同时处理不同的文件后，编写数据管道就简单多了。

delta 格式使用它所写的事务日志文件来实现对 ACID 属性的支持，该事务日志进一步为我们提供了在某个时间点读取数据的能力，因此我们可以从表中读取数据，但需要上周出现的数据，这对生产故障排除很有用。

在清单 [8-14](#PC14) 中，我们展示了发布过程的第一部分；我们从“策划”区域读入数据。虽然我们有一个单一的程序用于整个过程，但这通常被分成多个作业来处理每一步，所以我们显示了各部分之间的完全划分。

```cs
var data = spark.Read().Parquet(rootPath);

Listing 8-14Read the data back in from the “Curated” area

```

现在我们有了数据，我们将从数据中提取维属性，并将它们拆分到各自的增量表中。这样做的最终目标是拥有一个包含日期和金额等值的事实增量表，而“供应商”和“费用类型”等属性将位于它们自己的增量表中。在清单 [8-15](#PC15) 中，我们将读取“Supplier”列并创建一个供应商名称的散列，这将是我们可以用来连接回主事实增量表的键。使用供应商名称的散列而不是递增键的原因是，如果我们愿意，我们可以在并行作业中加载任何维度和事实。如果数据必须在加载事实增量表之前存在于维度中，那么我们需要在处理顺序上更加严格。一旦我们向供应商添加了键列，如果这是我们第一次写入增量表，那么我们将创建一个新表。如果它不是我们正在处理的第一个文件，那么我们将使用“left_anti”连接来连接现有数据，这意味着只给出左边不存在的行。然后，我们将新行插入增量表。很明显，使用 delta 格式进行写入实际上开始使数据湖中的处理类似于 RDBMS 或 SQL 数据库中的处理。

```cs
var suppliers = data.Select(Col("Supplier")).Distinct()
                        .WithColumn("supplier_hash", Hash(Col("Supplier")));

var supplierPublishPath = $"{publishPath}-suppliers";

if (!Directory.Exists(supplierPublishPath))
{
    suppliers.Write().Format("delta").Save(supplierPublishPath);
}
else
{
    var existingSuppliers = spark.Read().Format("delta").Load(supplierPublishPath);
    var newSuppliers = suppliers.Join(existingSuppliers, existingSuppliers["Supplier"] == suppliers["Supplier"], "left_anti");
    newSuppliers.Write().Mode(SaveMode.Append).Format("delta").Save(supplierPublishPath);
}

Listing 8-15Storing each supplier in a dimension delta table

```

在清单 [8-16](#PC16) 中，我们做了同样的事情，但是使用了“费用类型”列；我们将数据移动到它自己的维增量表中。

```cs
var expenseTypePublishPath = $"{publishPath}-expense-type";

var expenseType = data.Select(Col("Expense_Type")).Distinct().WithColumn("expense_type_hash", Hash(Col("Expense_Type")));

if (!Directory.Exists(expenseTypePublishPath))
{
    expenseType.Write().Format("delta").Save(expenseTypePublishPath);
}
else
{
    var existingExpenseType = spark.Read().Format("delta").Load(expenseTypePublishPath);
    var newExpenseType = expenseType.Join(existingExpenseType, existingExpenseType["Expense_Type"] == expenseType["Expense_Type"], "left_anti");
    newExpenseType.Write().Mode(SaveMode.Append).Format("delta").Save(expenseTypePublishPath);
}

data = data.WithColumn("Expense_Type", Hash(Col("Expense_Type"))).WithColumn("Supplier", Hash(Col("Supplier")));

Listing 8-16Move the “Expense Type” column into its own dimension delta table

```

在清单 [8-17](#PC17) 中，这是发布阶段的最后一部分，如果这是我们处理的第一个文件，那么我们可以将数据写成 delta 格式；如果数据已经存在，那么我们将把数据合并在一起，如果有更新，这将更新任何现有的金额，或者插入新的行。

```cs
if (!Directory.Exists(publishPath))
{
    data.Write().Format("delta").Save(publishPath);
}
else
{
    var target = DeltaTable.ForPath(publishPath).Alias("target");
    target.Merge(
        data.Alias("source"), "source.Date = target.Date AND source.Expense_Type = target.Expense_Type AND source.Expense_Area = target.Expense_Area AND source.Supplier = target.supplier AND source.Reference = target.Reference"
        ).WhenMatched("source.Amount != target.Amount")
            .Update(new Dictionary<string, Column>(){{"Amount", data["Amount"]}}
    ).WhenNotMatched()
            .InsertAll()
    .Execute();
}

Listing 8-17Using a merge to write into the existing data

```

merge 语句本身很有趣。它允许我们通过指定哪些列应该匹配来合并源和目标数据帧；如果我们找到匹配，那么我们可以更新，或者可选地，提供一个额外的过滤器，然后像我们在这里所做的那样进行更新:`WhenMatched("source.Amount != target.Amount")`。如果合并条件确定一行不存在，我们可以选择做什么；这里，我们只想插入所有的行，但是我们可以更有选择地插入哪些列。最后，要运行 merge 语句，我们需要调用`Execute`。

需要注意的一点是，就目前而言，`Merge`语句有点混合了代码和 SQL，为了使 SQL 明确地表明哪个是源和目标，我在`DeltaTable`和`DataFrame`上都使用了 alias，以确保不会混淆我们正在比较的内容和时间。

也可以通过向 SparkSession 添加另一个选项“spark.sql.extensions”来使用 SQL 完整地编写 merge 语句，该选项应设置为“io . delta . SQL . deltasparksessionextension”。如果我们使用这个选项，我们可以用 SQL merge 语句替换我们的代码。

### F#数据管道

在清单 [8-18](#PC18) 中，我们验证了传入数据管道的参数，这些参数应该是数据湖的根路径、源文件以及文件所在的年份和月份。

```cs
let args = match argv with
            | [|dataLakePath; path; year; month|] -> {dataLakePath = argv.[0]; path = argv.[1]; year = argv.[2]; month = argv.[3]; success = true}
            | _ -> {success = false; dataLakePath= ""; path = ""; year = ""; month = "";}

match args.success with
    | false ->
        printfn "Error, incorrect args. Expecting 'Data Lake Path' 'file path' 'year' 'month', got: %A" argv
        -1

    | true ->
              let spark = SparkSession.Builder().Config("spark.sql.sources.partitionOverwriteMode", "dynamic").GetOrCreate()

Listing 8-18Handling command-line arguments and getting setup

```

这里，我们对参数进行一些基本的验证，然后创建一个`SparkSession`。`SparkSession`上设置了一个选项。选项是`"spark.sql.sources.partitionOverwriteMode"`，它允许我们覆盖一个特定的分区，而不是一个完整的表。这是有用的，因为当我们写入数据湖的管理部分时，我们将按年和月对数据进行分区，并且因为每个文件都是一个月的，如果有更正，文件可以重新发布；我们希望处理一个月的数据，并覆盖该月已经存在的任何内容，但不是整个表。如果没有这个选项，我们要么不能覆盖一个分区，要么用单个分区覆盖整个表。

然后我们做一些简单的参数解析。您可能会在参数解析中添加额外的验证，或者在实际应用程序中使用一个库来解析参数。

在清单 [8-19](#PC19) 中，我们创建了一个编排管道的方法；这些步骤从源文件中读取，写入结构化区域，然后写入管理区域，最后在发布区域创建最终结果。

```cs
let data = getData(spark, args.path)

              writeToStructured (data, (sprintf "%s/structured/%s/%s/%s/%s" args.dataLakePath "ofgem" "over25kexpenses" args.year args.month))
              match validateEntity data with
                | false -> writeToFailed(data, (sprintf "%s/failed/%s/%s" args.dataLakePath "ofgem" "over25kexpenses"))
                           -2
                | true -> writeToCurated(data, (sprintf "%s/curated/%s/%s" args.dataLakePath "ofgem" "over25kexpenses"))
                          writeToPublished(spark,  (sprintf "%s/curated/%s/%s" args.dataLakePath "ofgem" "over25kexpenses"), (sprintf "%s/publish/%s/%s" args.dataLakePath "ofgem" "over25kexpenses"))
                          0

Listing 8-19Orchestrate the pipeline

```

我们做的第一件事是读取源文件，并做一些处理以获得正确的列和数据类型，这样我们就可以在结构化区域中写入我们喜欢的格式。清单 [8-20](#PC20) 显示了用我们需要的选项读取源文件。

```cs
let getData(spark:SparkSession, path:string) =
    let readOptions =
        let options = [
            ("inferSchema","true")
            ("header","false")
            ("encoding","ISO-8859-1")
            ("locale","en-GB")
            ("quote","\"")
            ("ignoreLeadingWhiteSpace","true")
            ("ignoreTrailingWhiteSpace","true")
            ("dateFormat","M y")
        ]
        System.Linq.Enumerable.ToDictionary(options, fst, snd)

    spark.Read().Format("csv").Options(readOptions).Load(path)
        |> fun data -> data.WithColumn("index", Functions.MonotonicallyIncreasingId())
        |> dropIgnoredColumns
        |> fixColumnHeaders
        |> filterOutEmptyRows
        |> fixDateColumn
        |> fixAmountColumn

Listing 8-20Reading the source file with the correct options

```

这组特殊的文件有一个额外的标题行，我们需要跳过。这里我们可以采用几种不同的方法，比如在使用 Apache Spark 之前，对文件进行预处理以删除多余的标题行。如果我用类似 SSIS 的语言编写这个数据管道，那么我可能会对文件进行预处理。在清单 [8-21](#PC21) 中，我们展示了如何在不使用列标题的情况下读入整个文件，方法是给行添加一个索引，这样我们可以过滤掉前两行，但使用第二行作为列名的来源。此类文件的一个潜在问题是数据源提供者添加或更改列，因此我们需要小心依赖名称的列排序。使用提供的列名通常更可靠。

```cs
let dropIgnoredColumns (dataFrameToDropColumns:DataFrame) : DataFrame =

        let header = dataFrameToDropColumns.Filter(Functions.Col("index").EqualTo(1)).Collect()

        let shouldDropColumn (_:int, data:obj) =
            match data with
                | null -> true
                | _ -> match data.ToString() with
                            | "Date" -> false
                            | "Expense Type" -> false
                            | "Expense Area" -> false
                            | "Supplier" -> false
                            | "Reference" -> false
                            | "Amount" -> false
                            | "index" -> false
                            | null -> true
                            | _ -> true

        let dropColumns =
            let headerRow = header |> Seq.cast<Row> |> Seq.head
            headerRow

                |> fun row -> row.Values
                |> Seq.indexed
                |> Seq.filter shouldDropColumn
                |> Seq.map fst
                |> Seq.map(fun i -> "_c" + i.ToString())
                |> Seq.toArray

        dataFrameToDropColumns.Drop dropColumns

let fixColumnHeaders (dataFrame:DataFrame) : DataFrame =
    let header = getHeaderRow dataFrame
                    |> convertHeaderRowIntoArrayOfNames

    dataFrame.Filter(Functions.Col("index").Gt(1)).Drop("index").ToDF(header)

Listing 8-21Using an index to skip over the additional header row and then using the column headers to name the columns and ignore any additional empty columns

```

这里的关键是我们使用了函数`MonotonicallyIncreasingId()`，它为每一行提供了一个索引号，我们可以用它来过滤。然后，我们删除任何不需要的列，将实际的标题行作为一个字符串数组读取，并获取过滤后的数据，即没有两个标题行的实际数据，并调用`ToDF(header)`，其中标题是新的一组列名。这给了我们一个`DataFrame`，在这里我们可以使用列名来引用列。

因为源数据文件包括几行完全为空的行，在清单 [8-22](#PC22) 中，我们过滤掉了没有供应商或参考的行。

```cs
let filterOutEmptyRows (dataFrame:DataFrame) : DataFrame =
    dataFrame.Filter(Functions.Col("Reference").IsNotNull()).Filter(Functions.Col("Supplier").IsNotNull())

Listing 8-22Filtering out rows that are empty

```

现在我们只有感兴趣的行。我们将修复金额列。在清单 [8-23](#PC23) 中，我们展示了如何删除 Amount 列中多余的" "和" "，并将数据转换为实际的数值，在本例中是一个浮点数。

```cs
let fixAmountColumn (dataFrame:DataFrame) : DataFrame =
    dataFrame.WithColumn("Amount", Functions.RegexpReplace(Functions.Col("Amount"), "[£,]", ""))
    |> fun d -> d.WithColumn("Amount", Functions.Col("Amount").Cast("float"))

Listing 8-23Turning the Amount string into a usable value

```

下一个要处理的列是“日期”列，他们通常使用一种日期格式，但是在一些文件中，他们使用不同的日期格式，所以我们需要能够满足两种可能性。为了处理这个问题，在清单 [8-24](#PC24) 中，我们复制了一个现有的列，然后尝试转换日期列。如果转换的结果是日期列中的所有值都为空，那么我们将再次尝试使用备用日期格式。

```cs
let fixDateColumn (dataFrame:DataFrame) : DataFrame =
    dataFrame.WithColumn("__Date", Functions.Col("Date"))
     |> fun d -> d.WithColumn("Date", Functions.ToDate(Functions.Col("Date"), "MMMM yyyy"))
     |> fun d-> match d.Filter(Functions.Col("Date").IsNotNull()).Count() with
                    | 0L -> d.WithColumn("Date", Functions.ToDate(Functions.Col("__Date"), "MMM-yy"))
                    | _ -> d
     |> fun d -> d.Drop("__Date")

Listing 8-24Dealing with multiple date formats

```

最后，我们返回应该具有的数据帧

*   正确的列标题

*   删除任何空列/行

*   正确的数据类型

因为我们已经做了相当多的工作才能够读取这些文件，所以用相同的原始数据保存这些文件，但以一种更容易读取的格式保存，通常是有用的。在清单 [8-25](#PC25) 中，我们将把文件作为一个拼花文件写到数据湖的“结构化”区域。

```cs
let writeToStructured(dataFrame:DataFrame, path:string) : unit =
    dataFrame.Write().Mode("overwrite").Format("parquet").Save(path)

Listing 8-25Writing out the raw data in a format that can be easily consumed

```

我们传入的路径已经限定了年和月的范围，所以我们可以覆盖那里的任何内容；否则，我们希望确保不会覆盖其他月份的数据。

在下一阶段，我们将写入“管理的”区域，这意味着我们需要执行一些验证，以确保我们只引入有效的数据。

在清单 [8-26](#PC26) 中，我们将展示第一次验证，以确保我们拥有的模式与预期的模式相匹配。这将验证是否存在正确的列以及数据类型是否正确。在这种情况下，我们只进行等式匹配，以确保模式是相同的。在一些系统中，我们可能想要迭代列，并检查我们至少有 x 个 y 类型的列。

```cs
let expectedSchema = StructType(
                                   [|
                                       StructField("Date", DateType())
                                       StructField("Expense_Type", StringType())
                                       StructField("Expense_Area", StringType())
                                       StructField("Supplier", StringType())
                                       StructField("Reference", StringType())
                                       StructField("Amount", FloatType())
                                   |]
                               )

let validateSchema (dataFrame:DataFrame) = dataFrame.Schema().Json = expectedSchema.Json

Listing 8-26Validating the DataFrame schema

```

在清单 [8-27](#PC27) 中，我们将展示接下来的两个检查，即验证我们在日期列的每一行中都有一个值，并检查我们是否有任何数据。

```cs
let validateHaveSomeNonNulls (dataFrame:DataFrame) = dataFrame.Filter(Functions.Col("Date").IsNotNull()).Count() > 0L

Listing 8-27Validating the date column, and we have at least one row

```

最终检查更多的是一个业务规则；该数据包含每个在一个月内收费超过 25，000 英镑的供应商，因此我们检查每个金额是否超过 25，000 英镑。然而，有时，一个供应商提供不同的服务。每项服务可能少于 25，000，因此我们需要聚合供应商列并合计金额，然后过滤以查看是否有低于 25，000 的服务。在清单 [8-28](#PC28) 中，我们将展示如何使用 GroupBy 函数进行聚合。

```cs
let validateAmountsPerSupplierGreater25K (dataFrame:DataFrame) = dataFrame.GroupBy(Functions.Col("Supplier")).Sum("Amount").Filter(Functions.Col("Sum(Amount)").Lt(25000)).Count() = 0L

Listing 8-28Using GroupBy and Sum to get a total amount for each supplier

```

一旦数据得到验证，我们要么写出正确的数据，要么如果验证失败，就把它作为错误写出，以便以后调查。在清单 [8-29](#PC29) 中，我们展示了如何写数据，但是我们写的不是单个文件，而是文件的其余数据，并按月和年对其进行分区。如果任何人需要读取数据，他们可以从一个地方读取，使用过滤只读取他们感兴趣的年份和月份。

```cs
let writeToCurated (dataFrame:DataFrame, path:string) : unit =
    dataFrame.WithColumn("year", Functions.Year(Functions.Col("Date")))
    |> fun data -> data.WithColumn("month", Functions.Month(Functions.Col("Date")))
    |> fun data -> data.Write().PartitionBy("year", "month").Mode("overwrite").Parquet(path);

Listing 8-29Writing the data into a common area using partitioning to keep it isolated from other years and months

```

在这一点上，我们有原始数据，在“结构化”区域中我们有更直接的格式供其他人阅读的数据，在“策划”区域中我们有经过验证的数据。最后一步是写入数据湖的“发布”区域。

“发布”区域中的数据有两个特征。首先，我们将应用一些数据建模，而不是只有一个大表，我们将使用一个事实表和一个维度表，用于可以移动到维度中的每个属性。第二个特性是我们将使用 delta 格式来写文件。delta 格式允许我们合并更改，所以如果我们重新处理一个文件，那么任何更新都将被合并到数据中。delta 格式为我们提供了各种有用的有趣属性，比如我们通常与 RDBMS 联系在一起的 ACID 属性。这些酸性给了我们

*   **原子性**—写操作完成或未完成，没有部分完成。

*   **一致性**–无论何时有人试图读取数据，数据总是处于有效状态。

*   **隔离**—多个并发写入不会损坏数据。

*   **耐久性**—一旦写入操作完成，它将保持写入状态，无论系统是否出现故障。

知道了我们可以让多个 ETL 作业同时处理不同的文件后，编写数据管道就简单多了。

delta 格式使用它所写的事务日志文件来实现对 ACID 属性的支持，该事务日志进一步为我们提供了在某个时间点读取数据的能力，因此我们可以从表中读取数据，但需要上周出现的数据，这对生产故障排除很有用。

在清单 [8-30](#PC30) 中，我们展示了发布过程的第一部分；我们从“策划”区域读入数据。虽然我们有一个单一的程序用于整个过程，但这通常被分成多个作业来处理每一步，所以我们显示了各部分之间的完全划分。

```cs
let data = spark.Read().Parquet(source)

Listing 8-30Read the data back in from the “Curated” area

```

现在我们有了数据，我们将从数据中提取维属性，并将它们拆分到各自的增量表中。这样做的最终目标是拥有一个包含日期和金额等值的事实增量表，而“供应商”和“费用类型”等属性将位于它们自己的增量表中。在清单 [8-31](#PC31) 中，我们将读取“Supplier”列并创建一个供应商名称的散列，这将是我们可以用来连接回主事实增量表的键。使用供应商名称的散列而不是递增键的原因是，如果我们愿意，我们可以在并行作业中加载任何维度和事实。如果数据必须在加载事实增量表之前存在于维度中，那么我们需要在处理顺序上更加严格。一旦我们向供应商添加了键列，如果这是我们第一次写入增量表，那么我们将创建一个新表。如果它不是我们正在处理的第一个文件，那么我们将使用“left_anti”连接来连接现有数据，这意味着只给出左边不存在的行。然后，我们将新行插入增量表。很明显，使用 delta 格式进行写入实际上开始使数据湖中的处理类似于 RDBMS 或 SQL 数据库中的处理。

```cs
let saveSuppliers (spark: SparkSession, dataFrame:DataFrame, source:string, target:string) =

    let suppliers = dataFrame.Select(Functions.Col("Supplier")).Distinct()

    match Directory.Exists(sprintf "%s-suppliers" target) with
        | true -> let existingSuppliers = spark.Read().Format("delta").Load(sprintf "%s-suppliers" target)
                  existingSuppliers.Join(suppliers, existingSuppliers.Col("Supplier").EqualTo(suppliers.Col("Supplier")), "left_anti")
                    |> fun newSuppliers -> newSuppliers.WithColumn("Supplier_Hash", Functions.Hash(Functions.Col("Supplier"))).Write().Mode(SaveMode.Append).Format("delta").Save(sprintf "%s-suppliers" target)
         | false -> suppliers.WithColumn("Supplier_Hash", Functions.Hash(Functions.Col("Supplier"))).Write().Format("delta").Save(sprintf "%s-suppliers" target)

Listing 8-31Storing each supplier in a dimension delta table

```

在清单 [8-32](#PC32) 中，我们做了同样的事情，但是使用了“费用类型”列；我们将数据移动到它自己的维增量表中。

```cs
let saveExpenseType (spark: SparkSession, dataFrame:DataFrame, source:string, target:string) =

    let expenseType = dataFrame.Select(Functions.Col("Expense_Type")).Distinct()

    match Directory.Exists(sprintf "%s-expense-type" target) with
        | true -> let existingExpenseType = spark.Read().Format("delta").Load(sprintf "%s-expense-type" target)
                  existingExpenseType.Join(expenseType, existingExpenseType.Col("Expense_Type").EqualTo(expenseType.Col("Expense_Type")), "left_anti")
                    |> fun newExpenseType -> newExpenseType.WithColumn("Expense_Type_Hash", Functions.Hash(Functions.Col("Expense_Type"))).Write().Mode(SaveMode.Append).Format("delta").Save(sprintf "%s-expense-type" target)
         | false -> expenseType.WithColumn("Expense_Type_Hash", Functions.Hash(Functions.Col("Expense_Type"))).Write().Mode("overwrite").Format("delta").Save(sprintf "%s-expense-type" target)

Listing 8-32Move the “Expense Type” column into its own dimension delta table

```

在清单 [8-33](#PC33) 中，这是发布阶段的最后一部分，如果这是我们处理的第一个文件，那么我们可以将数据写成 delta 格式；如果数据已经存在，那么我们将把数据合并在一起，如果有更新，这将更新任何现有的金额，或者插入新的行。

```cs
let writeExpenses (dataFrame:DataFrame, target:string) =

    let data = dataFrame.WithColumn("Expense_Type_Hash", Functions.Hash(Functions.Col("Expense_Type"))).Drop("Expense_Type")
                |> fun data -> data.WithColumn("Supplier_Hash", Functions.Hash(Functions.Col("Supplier"))).Drop("Supplier").Alias("source")

    match Directory.Exists(target) with
        | false -> data.Write().Format("delta").Save(target)
        | true ->  DeltaTable.ForPath(target).Alias("target").Merge(data, "source.Date = target.Date AND source.Expense_Type_Hash = target.Expense_Type_Hash AND source.Expense_Area = target.Expense_Area AND source.Supplier_Hash = target.Supplier_Hash AND source.Reference = target.Reference")
                    |> fun merge -> let options = System.Linq.Enumerable.ToDictionary(["Amount", data.["Amount"]], fst, snd)
                                    merge.WhenMatched("source.Amount != target.Amount").Update(options)
                    |> fun merge -> merge.WhenNotMatched().InsertAll()
                    |> fun merge -> merge.Execute()

Listing 8-33Using a merge to write into the existing data

```

merge 语句本身很有趣。它允许我们通过指定哪些列应该匹配来合并源和目标数据帧；如果我们找到匹配，那么我们可以更新，或者可选地，提供一个额外的过滤器，然后像我们在这里所做的那样进行更新:`WhenMatched("source.Amount != target.Amount")`。如果合并条件确定一行不存在，我们可以选择做什么；这里，我们只想插入所有的行，但是我们可以更有选择地插入哪些列。最后，要运行 merge 语句，我们需要调用`Execute`。

需要注意的一点是，就目前而言，`Merge`语句有点混合了代码和 SQL，为了使 SQL 明确地表明哪个是源和目标，我在`DeltaTable`和`DataFrame`上都使用了 alias，以确保不会混淆我们正在比较的内容和时间。

也可以通过向 SparkSession 添加另一个选项“spark.sql.extensions”来使用 SQL 完整地编写 merge 语句，该选项应设置为“io . delta . SQL . deltasparksessionextension”。如果我们使用这个选项，我们可以用 SQL merge 语句替换我们的代码。

## 摘要

在 Apache Spark 中编写数据管道，要么使用。NET for Apache Spark 或 Python、Scala 等等，通常是将处理分成一系列更小的步骤，并在每个阶段验证数据。在接收数据时，几乎唯一不变的是数据在某一点上会是错误的，所以这是一个确保您能够有效地调试您的数据管道并了解它们何时何地失败的问题。

在本章中，我们展示了如何从包含大量挑战的数据文件中读取数据，并将这些单独的数据文件处理成一个完整的数据集，该数据集已经过验证，可供企业使用。