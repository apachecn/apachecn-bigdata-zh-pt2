# 第 6 章使用 Apache Pig 进行数据分析

在前面的章节中，我们探索了一些用于数据处理的 API。 MapReduce、Spark、Tez 和 Samza 是相当低级的，使用它们编写重要的业务逻辑通常需要大量的 Java 开发。 而且，不同的用户会有不同的需求。 对于分析师来说，编写 MapReduce 代码或构建输入和输出的 DAG 来回答一些简单的查询可能是不切实际的。 同时，软件工程师或研究人员可能希望在进入低级实现细节之前，使用高级抽象来构建想法和算法的原型。

在本章和下一章中，我们将探索一些工具，这些工具提供了一种使用更高级别的抽象在 HDFS 上处理数据的方法。 在本章中，我们将探讨 Apache Pig，特别是将涵盖以下主题：

*   什么是 Apache Pig 及其提供的数据流模型
*   猪拉丁语的数据类型和函数
*   如何使用自定义用户代码轻松增强 Pig
*   我们如何使用 Pig 来分析 Twitter 流

# 小猪一览

从历史上看，Pig 工具包由一个编译器组成，该编译器生成 MapReduce 程序，绑定它们的依赖项，并在 Hadoop 上执行它们。 PIG 作业在中用名为**Pig Lat****的语言编写，可以交互方式和批处理方式执行。 此外，可以使用用 Java、Python、Ruby、Groovy 或 JavaScript 编写的**用户定义函数**(**UDF**)来扩展 Pig 拉丁语。**

清管器使用案例包括以下内容：

*   数据处理
*   即席分析查询
*   算法的快速原型设计
*   提取变换加载管道

遵循我们在前几章中看到的趋势，Pig 正在向通用计算架构迈进。 从 0.13 版本开始，**ExecutionEngine**接口(`org.apache.pig.backend.executionengine`)充当 Pig 前端和后端之间的桥梁，允许在 MapReduce 以外的框架上编译和执行 Pig 拉丁文脚本。 在撰写本文时，版本 0.13 附带了**MRExecutionEngine**(`org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRExecutionEngine`)，并基于**TEZ**(`org.apache.pig.backend.hadoop.executionengine.tez.*`)开发了一个低延迟后端，预计版本 0.14(请参阅[https://issues.apache.org/jira/browse/PIG-3446](https://issues.apache.org/jira/browse/PIG-3446))中也将包含该版本(参见[MRExecutionEngine(`org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRExecutionEngine`))。 开发分支目前正在进行集成 Spark 的工作(参见](https://issues.apache.org/jira/browse/PIG-3446)[https://issues.apache.org/jira/browse/PIG-4059](https://issues.apache.org/jira/browse/PIG-4059))。

Pig0.13 为 MapReduce 后端提供了大量的性能增强，特别是减少小型作业延迟的两个特性：*直接 HDFS 访问*([https://issues.apache.org/jira/browse/PIG-3642](https://issues.apache.org/jira/browse/PIG-3642))和自动本地*模式*([https://issues.apache.org/jira/browse/PIG-3463](https://issues.apache.org/jira/browse/PIG-3463))。 默认情况下，直接 HDFS(`opt.fetch`属性)处于打开状态。 当在只包含`LIMIT`、`FILTER`、`UNION`、`STREAM`或`FOREACH`运算符的简单(仅地图)脚本中执行`DUMP`时，从 HDFS 获取输入数据，并绕过 MapReduce 直接在 Pig 中执行查询。 使用 AUTO LOCAL(`pig.auto.local.enabled`属性)，当数据大小小于`pig.auto.local.input.maxbytes`时，Pig 将在 Hadoop 本地模式下运行查询。 默认情况下，自动本地功能处于关闭状态。

如果两种模式都关闭，或者查询不符合任何一种条件，PIG 将启动 MapReduce 作业。 如果这两种模式都打开，Pig 将检查查询是否符合直接访问的条件，如果不符合，则回退到 AUTO LOCAL。 否则，它将在 MapReduce 上执行查询。

# 入门

我们将使用`stream.py`脚本选项来提取 JSON 数据并检索特定数量的 tweet；我们可以使用如下命令运行：

```
$ python stream.py -j -n 10000 > tweets.json

```

`tweets.json`文件的每一行将包含一个 JSON 字符串，表示一条 tweet。

请记住，Twitter API 凭据需要作为环境变量提供，或者硬编码在脚本本身中。

# 奔跑的猪

PIG 是一个工具，它翻译用 Pig 拉丁文编写的语句，并在独立模式下在单机上执行它们，或者在分布式模式下在完整的 Hadoop 集群上执行它们。 即使在后者中，Pig 的角色也是将 Pig 拉丁语语句转换为 MapReduce 作业，因此不需要安装额外的服务或守护程序。 它与其关联库一起用作命令行工具。

Cloudera CDH 附带 Apache Pig 版本 0.12。 或者，PIG 源代码和二进制分布可以在[https://pig.apache.org/releases.html](https://pig.apache.org/releases.html)处获得。

正如预期的那样，MapReduce 模式需要访问 Hadoop 集群和 HDFS 安装。 MapReduce 模式是在命令行提示下运行 Pig 命令时执行的默认模式。 可以使用以下命令执行脚本：

```
$ pig -f <script>

```

可以使用`-param <param>=<val>`通过命令行传递参数，如下所示：

```
$ pig –param input=tweets.txt

```

也可以在`param`文件中指定参数，该文件可以使用`-param_file <file>`选项传递给 Pig。 可以指定多个文件。 如果一个参数在文件中出现多次，将使用最后一个值，并显示警告。 参数文件每行包含一个参数。 允许空行和注释(通过以`#`开头的行来指定)。 在 Pig 脚本中，参数的形式为`$<parameter>`。 可以使用`default`语句分配默认值：`%default input tweets.json'`。 `default`命令在 Grunt 会话中不起作用；我们将在下一节中讨论 Grunt。

在本地模式下，所有文件都使用本地主机和文件系统安装和运行。 使用`-x`标志指定本地模式：

```
$ pig -x local

```

在这两种执行模式下，Pig 程序既可以在交互式外壳中运行，也可以在批处理模式下运行。

## Grunt-Pig 互动外壳

PIG 可以使用 Grunt shell 在交互模式下运行，当我们在终端提示符下使用`pig`命令时会调用该 shell。 在本章的其余部分中，我们将假设示例是在 Grunt 会话中执行的。 除了执行 Pig 拉丁语语句之外，Grunt 还提供了许多实用程序和对 shell 命令的访问：

*   `fs`：允许用户操作 Hadoop 文件系统对象，并具有与 Hadoop CLI 相同的语义
*   `sh`：通过操作系统外壳执行命令
*   `exec`：在交互式 Grunt 会话中启动 Pig 脚本
*   `kill`：终止 MapReduce 作业
*   `help`：打印所有可用命令的列表

### 弹性 MapReduce

通过使用`--applications Name=Pig,Args=--version,<version>`创建群集，可以在 EMR 上执行 PIG 脚本，如下所示：

```
$ aws emr create-cluster \
--name "Pig cluster" \
--ami-version <ami version> \
--instance-type <EC2 instance> \
--instance-count <number of nodes> \
--applications Name=Pig,Args=--version,<version>\
--log-uri <S3 bucket> \
--steps Type=PIG,\ 
Name="Pig script",\
Args=[-f,s3://<script location>,\
-p,input=<input param>,\
-p,output=<output param>]

```

前面的命令将提供一个新的 EMR 集群并执行`s3://<script location>`。 请注意，要执行的脚本以及输入(`-p input`)和输出(`-p output`)路径预计位于 S3 上。

作为创建新 EMR 集群的替代方法，可以使用以下命令将 Pig Steps 添加到已经实例化的 EMR 集群：

```
$ aws emr add-steps \
--cluster-id <cluster id>\
--steps Type=PIG,\ 
Name= "Other Pig script",\
Args=[-f,s3://<script location>,\
-p,input=<input param>,\
-p,output=<output param>]

```

在前面的命令中，`<cluster id>`是实例化集群的 ID。

还可以使用以下命令 ssh 进入主节点并在 Grunt 会话中运行 Pig 拉丁语语句：

```
$ aws emr ssh --cluster-id <cluster id> --key-pair-file <key pair>

```

# Apache Pig 基础知识

编程 Apache Pig 的主要接口是 Pig 拉丁语，这是一种实现数据流范例思想的过程性语言。

猪拉丁语项目一般按如下方式组织：

*   `LOAD`语句从 HDFS 读取数据
*   一系列语句聚合和操作数据
*   `STORE`语句将输出写入文件系统
*   或者，`DUMP`语句将输出显示到终端

下面的示例显示了一系列语句，这些语句输出从 tweet 数据集中提取的按频率排序的前 10 个标签：

```
tweets = LOAD 'tweets.json' 
  USING JsonLoader('created_at:chararray, 
    id:long, 
    id_str:chararray, 
    text:chararray');

hashtags = FOREACH tweets {
  GENERATE FLATTEN(
    REGEX_EXTRACT(
      text, 
      '(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)', 1)
    ) as tag;
}

hashtags_grpd = GROUP hashtags BY tag;
hashtags_count = FOREACH hashtags_grpd {
  GENERATE 
    group, 
    COUNT(hashtags) as occurrencies; 
}
hashtags_count_sorted = ORDER hashtags_count BY occurrencies DESC;
top_10_hashtags = LIMIT hashtags_count_sorted 10;
DUMP top_10_hashtags;
```

首先，我们从 HDFS 加载`tweets.json`数据集，反序列化 JSON 文件，并将其映射到一个四列模式，该模式包含 tweet 的创建时间、数字和字符串形式的 ID 以及文本。 对于每条 tweet，我们使用正则表达式从其文本中提取标签。 我们根据标签进行聚合，统计出现的次数，并按频率排序。 最后，我们将排序记录限制为最频繁的 10 个标签。

Pig 编译器提取一系列类似于前一条语句的语句，将其转换为 MapReduce 作业，然后在 Hadoop 集群上执行。 计划器和优化器将解析对输入和输出关系的依赖关系，并在可能的情况下并行化语句的执行。

**语句**是使用 Pig 处理数据的构建块。 它们把一种关系作为输入，产生另一种关系作为输出。 在 Pig 拉丁语中，关系可以定义为一个由**元组**组成的包，这两种数据类型我们将在本章余下的部分中使用。

熟悉 SQL 和关系数据模型的用户可能会发现 Pig 拉丁语的语法有些熟悉。 虽然语法本身确实有相似之处，但 Pig 拉丁语实现了完全不同的计算模型。 PIG 拉丁语是过程性的，它指定要执行的实际数据转换，而 SQL 是声明性的，描述了问题的性质，但没有指定实际的运行时处理。 在组织数据方面，可以将关系视为关系数据库中的表，其中包中的元组对应于表中的行。 关系是无序的，因此很容易并行化，而且它们比关系表的约束更少。 PIG 关系可以包含具有不同字段数的元组，而具有相同字段计数的元组可以在相应位置具有不同类型的字段。

SQL 和 Pig 拉丁语采用的数据流模型之间的一个关键区别在于如何管理数据管道中的拆分。 在关系世界中，SQL 等声明性语言实现并执行将生成单个结果的查询。 数据流模型将数据转换视为图形，其中输入和输出是由操作符连接的节点。 例如，查询的中间步骤可能需要按多个键对输入进行分组，并产生多个输出(`GROUP BY`)。 PIG 具有内置机制，可以通过在输入可用时立即执行运算符来管理此类图中的多个数据流，并可能对每个流应用不同的运算符。 例如，Pig 的`GROUP BY`操作符实现使用了并行特性([http://pig.apache.org/docs/r0.12.0/perf.html#parallel](http://pig.apache.org/docs/r0.12.0/perf.html#parallel))，允许用户为生成的 MapReduce 作业增加 Reduce 任务的数量，从而提高并发性。 该属性的另一个副作用是，当多个运算符可以在同一程序中并行执行时，Pig 会这样做(有关 Pig 的多查询实现的更多细节可以在[http://pig.apache.org/docs/r0.12.0/perf.html#multi-query-execution](http://pig.apache.org/docs/r0.12.0/perf.html#multi-query-execution)中找到)。 Pig 拉丁语的计算方法的另一个结果是，它允许在管道中的任何点上持久化数据。 它允许开发人员在必要时选择特定的运算符实现和执行计划，从而有效地覆盖优化器。

PIG 拉丁语允许甚至鼓励开发人员通过**用户定义函数**(**UDF**)以及利用 Hadoop 流将自己的代码插入到流水线中的几乎任何位置。 UDF 允许用户指定有关如何加载数据、如何存储数据以及如何处理数据的自定义业务逻辑，而流允许用户在数据流中的任何点启动可执行文件。

# 编程小猪

PIG 拉丁语附带了许多内置函数(eval、load/store、ath、string、Bag 和 tuple 函数)以及许多标量和复杂数据类型。 此外，Pig 允许通过 UDF 和 Java 方法的动态调用来扩展函数和数据类型。

## 猪数据类型

PIG支持以下标量数据类型：

*   `int`：带符号的 32 位整数
*   `long`：带符号的 64 位整数
*   `float`：32 位浮点
*   `double`：64 位浮点
*   `chararray`：Unicode UTF-8 格式的字符数组(字符串)
*   `bytearray`：字节数组(BLOB)
*   `boolean`：布尔值
*   发帖主题：Re：Колибри0.7.0
*   **T0*a Java BigInteger
*   **T0*

PIG 支持以下复杂数据类型：

*   `map`：用`[]`括起来的关联数组，键和值用`#`分隔，项用`,`分隔
*   `tuple`：数据的有序列表，其中元素可以是由`()`括起来的任何标量或复杂类型，项由`,`分隔
*   `bag`：由`{}`包围并由`,`分隔的元组的无序集合

默认情况下，Pig 将数据视为非类型化数据。 用户可以在加载时声明数据类型，也可以在必要时手动强制转换。 如果数据类型未声明，但脚本隐式地将值视为特定类型，则 Pig 将假定该值属于该类型，并相应地进行强制转换。 包或元组的字段可以通过名称`tuple.field`或位置`$<index>`来引用。 PIG 从 0 开始计数，因此第一个元素将表示为`$0`。

## PIG 函数

内置函数是用 Java 实现的，它们试图遵循标准的 Java 约定。 不过，我们要紧记以下几点不同之处：

*   函数名称区分大小写且大写
*   如果结果值为 NULL、空或**不是数字**(**NaN**)，则 Pig 返回 NULL
*   如果 Pig 无法处理该表达式，则返回异常

所有内置函数的列表可以在[http://pig.apache.org/docs/r0.12.0/func.html](http://pig.apache.org/docs/r0.12.0/func.html)中找到。

### 加载/存储

加载/存储函数确定数据如何进出 Pig。 函数`PigStorage`、`TextLoader`和`BinStorage`可分别用于读写 UTF-8 分隔文本、非结构化文本和二进制数据。 对压缩的支持由加载/存储功能决定。 函数`PigStorage`和`TextLoader`支持读(加载)和写(存储)的 gzip 和 bzip2 压缩。 `BinStorage`函数不支持压缩。

从 0.12 版开始，Pig 内置支持通过`AvroStorage`(加载/存储)、`JsonStorage`(存储)和`JsonLoader`(加载)加载和存储 Avro 和 JSON 数据。 在撰写本文时，对 JSON 的支持在某种程度上仍然是有限的。 特别地，Pig 期望将数据的模式作为参数提供给`JsonLoader/JsonStorage`，或者它假定`.pig_schema`(由`JsonStorage`生成)存在于包含输入数据的目录中。 在实践中，这使得处理不是由 Pig 本身生成的 JSON 转储变得困难。

如下面的示例所示，我们可以使用`JsonLoader`加载 JSON 数据集：

```
tweets = LOAD 'tweets.json' USING JsonLoader(
'created_at:chararray,  
id:long, 
id_str:chararray, 
text:chararray,
source:chararray');
```

我们提供了一个模式，以便映射 JSON 对象`created_id`、`id`、`id_str`、`text`和`source`的前五个元素。 我们可以使用`describe tweets`查看 tweet 的模式，它返回以下内容：

```
 tweets: {created_at: chararray,id: long,id_str: chararray,text: chararray,source: chararray} 
```

### 评估

Eval 函数实现一组要应用于返回包或地图数据类型的表达式的操作。 表达式结果在函数上下文中求值。

*   `AVG(expression)`：计算单列包中数值的平均值
*   `COUNT(expression)`：对包中第一个位置中具有非空值的所有元素进行计数
*   `COUNT_STAR(expression)`：对包中的所有元素进行计数
*   `IsEmpty(expression)`：检查包或地图是否为空
*   `MAX(expression)`、`MIN(expression)`和`SUM(expression)`：返回包中个元素的max、min 或总和[t4
*   `TOKENIZE(exp``ression)`：拆分字符串并输出一包单词

### 元组、包和映射函数

这些函数允许在包、元组和映射类型之间进行转换。 它们包括以下内容：

*   `TOTUPLE(expression)`、`TOMAP(expression)`和`TOBAG(expression)`：这些将`expression`强制为元组、映射或包
*   `TOP(n, column, relation)`：此函数返回元组包中的个顶部`n`个元组

### 数学、字符串和日期时间函数

PIG公开由`java.lang.Math`、`java.lang.String`、`java.util.Date`和 Joda-Time`DateTime`类(位于[http://www.joda.org/joda-time/](http://www.joda.org/joda-time/))提供的个函数。

### 动态调用器

动态调用程序允许执行 Java 函数，而不必将它们包装在 UDF 中。 它们可用于满足以下条件的任何静态函数：

*   不接受任何参数或接受具有这些相同类型的`string`、`int`、`long`、`double`、`float`或`array`的组合
*   返回`string`、`int`、`long`、`double`或`float`值

只有原语可以用于数字，而 Java 盒装类(如 Integer)不能用作参数。 根据返回类型，必须使用特定类型的调用器：`InvokeForString`、`InvokeForInt`、`InvokeForLong`、`InvokeForDouble`或`InvokeForFloat`。 有关动态调用器的更多详细信息可以在[http://pig.apache.org/docs/r0.12.0/func.html#dynamic-invokers](http://pig.apache.org/docs/r0.12.0/func.html#dynamic-invokers)中找到。

### 宏

从0.9 版开始，Pig 拉丁语的预处理器支持宏扩展。 宏是使用`DEFINE`语句定义的：

```
DEFINE macro_name(param1, ..., paramN) RETURNS output_bag { 
  pig_latin_statements 
};
```

宏内联展开，其参数在`{ }`内的 Pig 拉丁块中引用。

宏输出关系在`RETURNS`语句(`output_bag`)中给出。 `RETURNS void`用于没有输出关系的宏。

我们可以定义一个宏来计算关系中的行数，如下所示：

```
DEFINE count_rows(X) RETURNS cnt { 
  grpd = group $X all; 
  $cnt = foreach grpd generate COUNT($X); 
};
```

我们可以在 Pig 脚本或 Grunt 会话中使用它来计算 tweet 的数量：

```
tweets_count = count_rows(tweets);
DUMP tweets_count;
```

宏允许我们将代码放在单独的文件中并在需要的地方导入，从而使脚本模块化。 例如，我们可以将`count_rows`保存在一个名为`count_rows.macro`的文件中，然后使用命令`import 'count_rows.macro'`导入它。

宏有许多限制；尤其是，宏中只允许使用 Pig 拉丁语语句。 不能使用`REGISTER`语句和 shell 命令，不允许使用 UDF，也不支持宏内的参数替换。

## 使用数据

PIG 拉丁语提供了许多关系运算符来组合函数并对数据应用转换。 数据管道中的典型操作包括过滤关系(`FILTER`)、基于键聚合输入(`GROUP`)、基于数据列生成转换(`FOREACH`)以及基于共享键的连接关系(`JOIN`)。

在接下来的几节中，我们将演示通过加载 JSON 数据生成的 tweet 数据集上的这些操作符。

### 过滤

`FILTER`运算符根据表达式从关系中选择元组，如下所示：

```
relation = FILTER relation BY expression;
```

我们可以使用此运算符过滤其文本与 hashtag 正则表达式匹配的 tweet，如下所示：

```
tweets_with_tag = FILTER tweets BY 
    (text 
       MATCHES '(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)'
);
```

### 聚合

`GROUP`运算符根据表达式或键将一个或多个关系中的数据分组在一起，如下所示：

```
relation = GROUP relation BY expression;
```

我们可以按`source`字段将推文分组到一个新的关系`grpd`中，如下所示：

```
grpd = GROUP tweets BY source;
```

通过将元组指定为键，可以对多个维度进行分组，如下所示：

```
grpd = GROUP tweets BY (created_at, source);
```

`GROUP`运算的结果是一个关系，它包括组表达式的每个唯一值一个元组。 此元组包含两个字段。 第一个字段被命名为`group`，并且与组密钥的类型相同。 第二个字段采用原始关系的名称，类型为 Bag。 这两个字段的名称都由系统生成。

使用关键字`ALL`，Pig 将聚合整个关系。 `GROUP tweets ALL`方案将聚合同一组中的所有元组。

如前所述，Pig 允许使用`PARALLEL`运算符显式处理`GROUP`运算符的并发级别：

```
grpd = GROUP tweets BY (created_at, id) PARALLEL 10;
```

在前面的示例中，编译器生成的 MapReduce 作业将运行 10 个并发 Reduce 任务。 猪对使用多少减速机有一个试探性的估计。 全局强制执行 Reduce 任务数量的另一种方法是使用`set default_parallel <n>`命令。

### 永远

`FOREACH`运算符对列应用函数，如下所示：

```
relation = FOREACH relation GENERATE transformation;
```

`FOREACH`的输出取决于应用的转换。

我们可以使用运算符投影包含标签的所有 tweet 的文本，如下所示：

```
 t = FOREACH tweets_with_tag GENERATE text;
```

我们还可以将函数应用于投影柱。 例如，我们可以使用`REGEX_TOKENIZE`函数将每条推文拆分成单词，如下所示：

```
t = FOREACH tweets_with_tag GENERATE FLATTEN(TOKENIZE(text)) as word;
```

`FLATTEN`修饰符进一步将`TOKENIZE`生成的包解套成词的元组。

### 加入

`JOIN`运算符基于公共字段值执行两个或多个关系的内部联接。 其语法如下：

```
relation = JOIN relation1 BY expression1, relation2 BY expression2;
```

我们可以使用联接操作来检测包含正面单词的 tweet，如下所示：

```
positive = LOAD 'positive-words.txt' USING PigStorage() as (w:chararray);
```

过滤掉评论，如下所示：

```
positive_words = FILTER positive BY NOT w MATCHES '^;.*';
```

`positive_words`是一包元组，每个元组包含一个单词。 然后，我们对 tweet 的文本进行标记化，并创建一个新的(id_str，word)元组包，如下所示：

```
id_words = FOREACH tweets {
   GENERATE 
      id_str, 
      FLATTEN(TOKENIZE(text)) as word;
}
```

我们在`word`字段上连接这两个关系，并获得包含一个或多个肯定词的所有 tweet 之间的关系，如下所示：

```
positive_tweets = JOIN positive_words BY w, id_words BY word;
```

在此语句中，我们在`id_words.word`是正词的条件下将`positive_words`和`id_words`连接起来。 `positive_tweets`运算符是一个`{w:chararray,id_str:chararray, word:chararray}`形式的包，它包含符合联接条件的`positive_words`和`id_words`的所有元素。

我们可以结合`GROUP`和`FOREACH`运算符来计算每条 tweet 的正面单词数(至少包含一个正面单词)。 首先，我们将正面推文的关系按推文 ID 进行分组，然后统计每个 ID 在关系中出现的次数，如下所示：

```
grpd = GROUP positive_tweets BY id_str;
score = FOREACH grpd GENERATE FLATTEN(group), COUNT(positive_tweets);
```

`JOIN`操作符也可以利用并行化功能，如下所示：

```
positive_tweets = JOIN positive_words BY w, id_words BY word PARALLEL 10
```

前面的命令将执行具有 10 个减速器任务的联接。

可以使用`USING`关键字后跟专用联接的 ID 来指定操作员的行为。 有关更多详细信息，请访问[http://pig.apache.org/docs/r0.12.0/perf.html#specialized-joins](http://pig.apache.org/docs/r0.12.0/perf.html#specialized-joins)。

# 扩展猪(自定义项)

函数几乎可以是 Pig 中每个运算符的一部分。 UDF 和内置函数之间有两个主要区别。 首先，需要使用`REGISTER`关键字注册 UDF，以便使它们可供 Pig 使用。 其次，它们在使用时需要合格。 PIG UDF 目前可以在 Java、Python、Ruby、JavaScript 和 Groovy 中实现。 对 Java 函数提供了最广泛的支持，这些函数允许您定制流程的所有部分，包括数据加载/存储、转换和聚合。 此外，Java 函数的效率也更高，因为它们是用与 Pig 相同的语言实现的，而且还支持其他接口，如 Algebraic 和 Acumulator 接口。 另一方面，Ruby 和 PythonAPI 允许更快速的原型化。

自定义函数与 Pig 环境的集成主要由以下两个语句`REGISTER`和`DEFINE`管理：

*   `REGISTER`注册 JAR 文件，以便可以使用文件中的 UDF，如下所示：

    ```
    REGISTER 'piggybank.jar'

    ```

*   `DEFINE`创建函数或流命令的别名，如下所示：

    ```
    DEFINE MyFunction my.package.uri.MyFunction

    ```

Pig 的 0.12 版引入了 UDF 流，作为一种使用没有 JVM 实现的语言编写函数的机制。

## 贡献的自定义项

PIG 的代码base 托管一个名为**Piggybank**的 UDF 存储库。 其他受欢迎的贡献存储库是**Twitter 的 Elephant Bird**(位于[https://github.com/kevinweil/elephant-bird/](https://github.com/kevinweil/elephant-bird/))和**Apache DataFu**(位于[http://datafu.incubator.apache.org/](http://datafu.incubator.apache.org/))。

### 存钱罐

存钱罐是 Pig 用户共享功能的地方。 共享的代码位于位于[http://svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/](http://svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/)的官方 Pig Subversion 存储库中。 接口文档可以在**Conrib**部分下的[http://pig.apache.org/docs/r0.12.0/api/](http://pig.apache.org/docs/r0.12.0/api/)找到。 存钱罐 UDF 可以通过从 Subversion 存储库签出并编译源代码或使用 Pig 二进制版本附带的 JAR 文件来获得。 在 Cloudera CDH 中，`piggybank.jar`在`/opt/cloudera/parcels/CDH/lib/pig/piggybank.jar`可用。

### 象鸟

大象鸟是一个开源库，其中包含了 Hadoop 在 Twitter 生产中使用的所有东西。 该库包含许多序列化工具、自定义输入和输出格式、可写内容、Pig 加载/存储函数以及更多杂类。

大象鸟附带了一个极其灵活的 JSON 加载器函数，在撰写本文时，该函数是在 Pig 中操作 JSON 数据的首选资源。

### = 0= Apache DataF

Apache DataFu Pig 收集许多由 LinkedIn 开发和贡献的分析函数。 这些功能包括统计和估计函数、包和集合运算、采样、散列和链接分析。

# 分析 Twitter 流

在下面的示例中，我们将使用 Elephant Bird 提供的 JsonLoader 实现来加载和操作 JSON 数据。 我们将使用 Pig 来探索推特元数据并分析数据集中的趋势。 最后，我们将用户之间的交互建模为一个图，并使用 Apache DataFu 来分析这个社交网络。

## 必备条件

下载`elephant-bird-pig`([http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-pig/4.5/elephant-bird-pig-4.5.jar](http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-pig/4.5/elephant-bird-pig-4.5.jar))， `elephant-bird-hadoop-compat`([http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-hadoop-compat/4.5/elephant-bird-hadoop-compat-4.5.jar](http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-hadoop-compat/4.5/elephant-bird-hadoop-compat-4.5.jar))`,`和`elephant-bird-core`([http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-core/4.5/elephant-bird-core-4.5.jar](http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-core/4.5/elephant-bird-core-4.5.jar))JAR 文件。 从 Maven 中央存储库中，使用以下命令将它们复制到 HDFS：

```
$ hdfs dfs -put target/elephant-bird-pig-4.5.jar hdfs:///jar/
$ hdfs dfs –put target/elephant-bird-hadoop-compat-4.5.jar hdfs:///jar/
$ hdfs dfs –put elephant-bird-core-4.5.jar hdfs:///jar/ 

```

## 数据集探索

在深入研究数据集之前，我们需要注册对 Elephant Bird 和 DataFu 的依赖关系，如下所示：

```
REGISTER /opt/cloudera/parcels/CDH/lib/pig/datafu-1.1.0-cdh5.0.0.jar
REGISTER /opt/cloudera/parcels/CDH/lib/pig/lib/json-simple-1.1.jar
REGISTER hdfs:///jar/elephant-bird-pig-4.5.jar
REGISTER hdfs:///jar/elephant-bird-hadoop-compat-4.5.jar
REGISTER hdfs:///jar/elephant-bird-core-4.5.jar
```

然后，使用`com.twitter.elephantbird.pig.load.JsonLoader`加载 tweet 的 JSON 数据集，如下所示：

```
tweets = LOAD 'tweets.json' using  com.twitter.elephantbird.pig.load.JsonLoader('-nestedLoad');
```

`com.twitter.elephantbird.pig.load.JsonLoader`将输入文件的每一行解码为 JSON，并将结果值映射作为单元素元组传递给 Pig。 这样就可以访问 JSON 对象的元素，而不必预先指定模式。 参数`–nestedLoad`指示类加载嵌套数据结构。

## 推特元数据

在本章的剩余部分中，我们将使用来自 JSON 数据集的元数据来建模 tweet 流。 附加到 tweet 的元数据的一个例子是`Place`对象，它包含有关用户位置的地理信息。 `Place`包含描述其名称、ID、国家/地区、国家/地区代码等的字段。 完整的描述可以在[https://dev.twitter.com/docs/platform-objects/places](https://dev.twitter.com/docs/platform-objects/places)找到。

```
place = FOREACH tweets GENERATE (chararray)$0#'place' as place;
```

Entities提供来自 tweet、URL、标签和提及的结构化数据等信息，而不必从文本中提取它们。 有关实体的描述，请参阅[https://dev.twitter.com/docs/entities](https://dev.twitter.com/docs/entities)。 Hashtag 实体是从 tweet 中提取的标签数组。 每个实体都有以下两个属性：

*   **text**：是标签文本
*   **索引**：是从中提取标签的字符位置

以下代码使用实体：

```
hashtags_bag = FOREACH tweets {
    GENERATE 
      FLATTEN($0#'entities'#'hashtags') as tag;
}
```

然后，我们将展平`hashtags_bag`以提取每个标签的文本：

```
hashtags = FOREACH hashtags_bag GENERATE tag#'text' as topic;
```

用户对象的实体包含出现在用户配置文件和说明字段中的信息。 我们可以通过推文地图中的`user`字段提取推文作者的 ID：

```
users = FOREACH tweets GENERATE $0#'user'#'id' as id;
```

## 数据准备

内置运算符`SAMPLE`从数据集中选择概率为*p*的*n*元组的集合，如下所示：

```
sampled = SAMPLE tweets 0.01;
```

前面的命令将选择大约 1%的数据集。 假设`SAMPLE`是概率的([http://en.wikipedia.org/wiki/Bernoulli_sampling](http://en.wikipedia.org/wiki/Bernoulli_sampling))，则不能保证样本大小将是准确的。 此外，该函数使用替换进行采样，这意味着每个项可能出现多次。

Apache DataFu 实现了许多采样方法，用于具有精确样本大小且不需要替换的情况(`SimpleRandomSampling`)、使用替换进行采样(`SimpleRandomSampleWithReplacementVote`和`SimpleRandomSampleWithReplacementElect`)、当我们要考虑样本偏差时(`WeightedRandomSampling`)，或者跨多个关系进行采样(`SampleByKey`)。

我们可以使用`SimpleRandomSample`创建数据集恰好 1%的样本，每个项目都有相同的被选中概率。

### 备注

实际保证的样本大小为*个 ceil(p*n)*，概率至少为 99%。

首先，我们将采样概率 0.01 传递给 UDF 构造函数：

```
DEFINE SRS datafu.pig.sampling.SimpleRandomSample('0.01');
```

以及使用`(GROUP tweets ALL),`创建的要采样的包：

```
sampled = FOREACH (GROUP tweets ALL) GENERATE FLATTEN(SRS(tweets));
```

`SimpleRandomSample`UDF选择而不替换，这意味着每个项目将只出现一次。

### 备注

使用哪种抽样方法取决于我们正在处理的数据、关于项目如何分布的假设、数据集的大小，以及我们实际想要实现的目标。 通常，当我们想要探索数据集来阐明假设时，`SimpleRandomSample`可能是一个很好的选择。 但是，在几个分析应用程序中，通常使用假定替换的方法(例如，Bootstrapping)。

请注意，在处理非常大的数据集时，带替换的采样和不带替换的采样的行为往往类似。 从数十亿个项目中选择一个项目两次的概率将很低。

## 前 n 个统计数据

我们可能首先要问的问题之一是，某些事情发生的频率有多高。 例如，我们可能希望根据提及次数创建前 10 个主题的直方图。 同样，我们可能希望找到排名前 50 的国家/地区或排名前 10 的用户。 在查看推文数据之前，我们将定义一个宏，以便可以将相同的选择逻辑应用于不同的项目集合：

```
DEFINE top_n(rel, col, n) 
  RETURNS top_n_items {
    grpd = GROUP $rel BY $col;
    cnt_items = FOREACH grpd 
        GENERATE FLATTEN(group), COUNT($rel) AS cnt;
    cnt_items_sorted = ORDER cnt_items BY cnt DESC;
    $top_n_items = LIMIT cnt_items_sorted $n;
  }
```

`top_n`方法将关系`rel`、要计数的列`col`和要返回的项数`n`作为参数。 在 Pig 拉丁语块中，我们首先按`col`中的项对`rel`进行分组，计算每个项的出现次数，对它们进行排序，然后选择最频繁的`n`。

为了找到排名前 10 位的英语标签，我们按语言对它们进行过滤，并提取它们的文本：

```
tweets_en = FILTER tweets by $0#'lang' == 'en';
hashtags_bag = FOREACH tweets { 
    GENERATE
        FLATTEN($0#'entities'#'hashtags') AS tag;
}
hashtags = FOREACH hashtags_bag GENERATE tag#'text' AS tag;
```

并应用`top_n`宏：

```
top_10_hashtags = top_n(hashtags, tag, 10);
```

为了更好地描述什么是流行的，并使这些信息与用户更相关，我们可以深入数据集，查看每个地理位置的标签。

首先，我们生成(`place`，`hashtag`)元组的包，如下所示：

```
hashtags_country_bag = FOREACH tweets generate {
    0#'place' as place, 
    FLATTEN($0#'entities'#'hashtags') as tag;
}
```

然后，我们提取国家代码和标签文本，如下所示：

```
hashtags_country = FOREACH hashtags_country_bag {
  GENERATE 
    place#'country_code' as co, 
    tag#'text' as tag;
}
```

然后，我们计算每个国家/地区代码和标签一起出现的次数，如下所示：

```
hashtags_country_frequency = FOREACH (GROUP hashtags_country ALL) {
  GENERATE 
    FLATTEN(group), 
    COUNT(hashtags_country) as count;
}
```

最后，我们使用`TOP`函数计算每个标签的前 10 个国家/地区，如下所示：

```
hashtags_country_regrouped= GROUP hashtags_country_frequency BY cnt; 
top_results = FOREACH hashtags_country_regrouped {
    result = TOP(10, 1, hashtags_country_frequency);
    GENERATE FLATTEN(result);
} 
```

`TOP`的参数是要返回的元组数、要比较的列以及包含该列的关系：

```
top_results = FOREACH D {
  result = TOP(10, 1, C);
  GENERATE FLATTEN(result);
}
```

本例的源代码可以在[https://github.com/learninghadoop2/book-examples/blob/master/ch6/topn.pig](https://github.com/learninghadoop2/book-examples/blob/master/ch6/topn.pig)找到。

## 日期时间操作

JSON tweet 中的`created_at`字段为我们提供了关于 tweet 发布时间的时间戳信息。 不幸的是，它的格式与 Pig 的内置`datetime`类型不兼容。

储蓄罐通过`org.apache.pig.piggybank.evaluation.datetime.convert`中包含的大量时间操纵 UDF 来拯救我们。 其中之一是`CustomFormatToISO`，它将任意格式的时间戳转换为 ISO8601 日期时间字符串。

为了访问这些 UDF，我们首先需要注册`piggybank.jar`文件，如下所示：

```
REGISTER /opt/cloudera/parcels/CDH/lib/pig/piggybank.jar
```

为了使我们的代码不那么冗长，我们为`CustomFormatToISO`类的完全限定的 Java 名称创建一个别名：

```
DEFINE CustomFormatToISO org.apache.pig.piggybank.evaluation.datetime.convert.CustomFormatToISO();
```

通过了解如何操作时间戳，我们可以计算不同时间间隔的统计数据。 例如，我们可以查看每小时创建了多少条推文。 PIG 有一个内置的`GetHour`函数，可以从`datetime`类型中提取小时数。 为此，我们首先使用`CustomFormatToISO`将时间戳字符串转换为 ISO 8601，然后使用内置的`ToDate`函数将结果`chararray`转换为`datetime`，如下所示：

```
hourly_tweets = FOREACH tweets {
  GENERATE 
    GetHour(
      ToDate(
      CustomFormatToISO(
$0#'created_at', 'EEE MMMM d HH:mm:ss Z y')
      )
    ) as hour;
}
```

现在，只需按小时对`hourly_tweets`进行分组，然后按组生成推文计数，如下所示：

```
hourly_tweets_count =  FOREACH (GROUP hourly_tweets BY hour) { 
  GENERATE FLATTEN(group), COUNT(hourly_tweets);
}
```

### 会话

DataFu 的`Sessionize`类可以帮助我们更好地捕获随时间推移的用户活动。 会话表示用户在给定时间段内的活动。 例如，我们可以每隔 15 分钟查看每个用户的推文流，并测量这些会话以确定网络容量和用户活动：

```
DEFINE Sessionize datafu.pig.sessions.Sessionize('15m');
users_activity = FOREACH tweets {
      GENERATE 
        CustomFormatToISO($0#'created_at', 
                      'EEE MMMM d HH:mm:ss Z y') AS dt,
        (chararray)$0#'user'#'id' as user_id;
}
users_activity_sessionized = FOREACH 
    (GROUP users_activity BY user_id) {
    ordered = ORDER users_activity BY dt;
    GENERATE FLATTEN(Sessionize(ordered)) 
                    AS (dt, user_id, session_id);
}
```

`user_activity`只记录给定`user_id`发布状态更新的时间`dt`。

`Sessionize`将会话超时和包作为输入。 输入包的第一个元素是 ISO 8601 时间戳，必须按此时间戳对包进行排序。 彼此相隔 15 分钟的事件将属于同一会话。

它返回带有新字段`session_id`的输入包，该字段唯一地标识一个会话。 使用这些数据，我们可以计算会话的长度和其他一些统计数据。 有关`Sessionize`用法的更多示例，请参见[http://datafu.incubator.apache.org/docs/datafu/guide/sessions.html](http://datafu.incubator.apache.org/docs/datafu/guide/sessions.html)。

## 捕获用户交互

在本章的剩余部分中，我们将研究如何从用户交互中捕获模式。 作为这个方向的第一步，我们将创建一个适合对社交网络建模的数据集。 该数据集将包含时间戳、tweet 的 ID、发布 tweet 的用户、她回复的用户和 tweet，以及 tweet 中的标签。

Twitter 将任何以`@`字符开头的消息视为回复(`in_reply_to_status_id_str`)。 这样的推文被解释为给那个人的直接信息。 将`@`字符放在推文中的任何其他位置都会被解释为提及(`'entities'#'user_mentions`‘)，而不是回复。 不同之处在于，提及的内容会立即广播给一个人的追随者，而回复则不会。 然而，回复被认为是提及的。

在处理个人身份信息时，如果不能完全删除 IP 地址、姓名和用户 ID 等敏感数据，最好将其匿名。 一种常用的技术涉及一个`hash`函数，该函数将我们想要匿名的数据作为输入，将与称为 SALT 的附加随机数据连接起来。 以下代码显示了此类匿名的示例：

```
DEFINE SHA datafu.pig.hash.SHA();
from_to_bag = FOREACH tweets {
  dt = $0#'created_at';
  user_id = (chararray)$0#'user'#'id';
  tweet_id = (chararray)$0#'id_str';
  reply_to_tweet = (chararray)$0#'in_reply_to_status_id_str';
  reply_to = (chararray)$0#'in_reply_to_user_id_str';
  place = $0#'place';
  topics = $0#'entities'#'hashtags';

  GENERATE
    CustomFormatToISO(dt, 'EEE MMMM d HH:mm:ss Z y') AS dt,
    SHA((chararray)CONCAT('SALT', user_id)) AS source,  
    SHA(((chararray)CONCAT('SALT', tweet_id))) AS tweet_id,
    ((reply_to_tweet IS NULL) 
         ? NULL 
         : SHA((chararray)CONCAT('SALT', reply_to_tweet))) 
               AS  reply_to_tweet_id,
    ((reply_to IS NULL) 
         ? NULL 
         : SHA((chararray)CONCAT('SALT', reply_to))) 
                AS destination,
    (chararray)place#'country_code' as country,
    FLATTEN(topics) AS topic;
}

-- extract the hashtag text
from_to = FOREACH from_to_bag { 
  GENERATE 
    dt, 
    tweet_id, 
    reply_to_tweet_id, 
    source, 
    destination, 
    country,
    (chararray)topic#'text' AS topic;
}
```

在本例中，我们使用`CONCAT`将一个(不是很随机的)SALT 字符串附加到个人数据。 然后，我们使用 DataFu 的`SHA`函数生成加盐 ID 的散列。 `SHA`函数要求其输入参数为非空。 我们使用`if-then-else`语句强制执行此条件。 在猪拉丁语中，这表示为`<condition is true> ? <true branch> : <false branch>`。 如果字符串为空，则返回`NULL`，如果不为空，则返回加盐的散列。 为了使代码更具可读性，我们对 tweet JSON 字段使用别名，并在`GENERATE`块中引用它们。

## 链接分析

我们可以重新定义我们的方法来确定热门话题，以包括用户的反应。 第一种天真的方法可能是，如果一个话题导致的回复数量超过阈值，那么它就会被认为是重要的。

这种方法的一个问题是，tweet 生成的回复相对较少，因此生成的数据集的数量将会很低。 因此，需要非常大量的数据才能包含被回复的推文并产生任何结果。 在实践中，我们可能希望将此指标与其他指标(例如，提及)结合起来，以便执行更有意义的分析。

为了满足这个查询，我们将创建一个新的数据集，其中包括从 tweet 和用户回复的 tweet 中提取的 hashtag：

```
tweet_hashtag = FOREACH from_to GENERATE tweet_id, topic;
from_to_self_joined = JOIN from_to BY reply_to_tweet_id LEFT, 
tweet_hashtag BY tweet_id;

twitter_graph = FOREACH from_to_self_joined  { 
    GENERATE
        from_to::dt AS dt,
        from_to::tweet_id AS tweet_id,
        from_to::reply_to_tweet_id AS reply_to_tweet_id,
        from_to::source AS source,
        from_to::destination AS destination,
        from_to::topic AS topic,
        from_to::country AS country,
        tweet_hashtag::topic AS topic_replied;
}
```

请注意，Pig 不允许在同一关系上进行交叉联接，因此我们必须为联接的右侧创建`tweet_hashtag`。 在这里，我们使用`::`运算符来消除我们想要从哪个关系和列中选择记录的歧义。

同样，我们可以使用`top_n`宏按回复数量查找前 10 个主题：

```
top_10_topics = top_n(twitter_graph, topic_replied, 10);
```

数数东西只能带我们走到这一步。 我们可以使用 DataFu 在此数据集上计算更多描述性统计数据。 使用`Quantile`函数，我们可以计算标签反应数量的中位数、第 90 个、第 95 个和第 99 个百分位数，如下所示：

```
DEFINE Quantile datafu.pig.stats.Quantile('0.5','0.90','0.95','0.99');
```

由于 UDF期望整数值的有序包作为输入，我们首先计算每个`topic_replied`条目的频率，如下所示。

```
topics_with_replies_grpd = GROUP twitter_graph BY topic_replied;
topics_with_replies_cnt = FOREACH topics_with_replies_grpd {
  GENERATE
COUNT(twitter_graph) as cnt;
}
```

然后，我们对频率包应用`Quantile`，如下所示：

```
quantiles = FOREACH (GROUP topics_with_replies_cnt ALL) {
    sorted = ORDER topics_with_replies_cnt BY cnt;
    GENERATE Quantile(sorted);
}
```

本例的源代码可以在[https://github.com/learninghadoop2/book-examples/blob/master/ch6/graph.pig](https://github.com/learninghadoop2/book-examples/blob/master/ch6/graph.pig)找到。

## 有影响力的用户

我们将使用 GOOGLE 开发的网页排名算法 PageRank([http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf](http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf))在上一节生成的推特图表中识别有影响力的用户。

这种类型的分析有许多用例，例如定向和上下文广告、推荐系统、垃圾邮件检测，以及明显测量网页的重要性。 研究论文*wtf：在推特[http://stanford.edu/~rezab/papers/wtf_overview.pdf](http://stanford.edu/~rezab/papers/wtf_overview.pdf)找到的**要关注推特*的服务人员中描述了一种类似的方法，推特使用这种方法来实现“关注谁”功能(*WTF：The**Who to Follow Service)。*

PageRank 非正式地根据链接到该页面的其他页面的重要性来确定该页面的重要性，并为其分配一个介于 0 和 1 之间的分数。PageRank 分数高表示有很多页面指向该页面。 直观地说，被高 PageRank 的页面链接是一种高质量的认可。 根据 Twitter 图，我们假设收到大量回复的用户在社交网络中很重要或有影响力。 在 Twitter 的例子中，我们考虑了 PageRank 的扩展定义，其中两个用户之间的链接由直接回复给出，并由消息中出现的任何最终标签进行标记。 启发式地，我们希望确定在给定主题上有影响力的用户。

在 DataFu 的实现中，每个图都表示为一个由`(source, edges)`个元组组成的包。 `source`元组是表示源节点的整数 ID。 边缘是一个由`(destination, weight)`个元组组成的袋子。 `destination`是表示目的节点的整数 ID。 `weight`是一个双精度数，表示边缘应该加权多少。 UDF 的输出是一个由`(source, rank)`对组成的包，其中`rank`是图中源用户的 PageRank 值。 请注意，我们将节点、边和图作为抽象概念来讨论。 在谷歌的例子中，节点是网页，边是从一个页面到另一个页面的链接，而图形是直接或间接连接的一组页面。

在我们的例子中，节点表示用户，边表示`in_reply_to_user_id_str`个提及，边由 tweet 中的标签标记。 PageRank 的输出应该建议哪些用户在给定的交互模式下对给定的主题有影响力。

在本节中，我们将编写一条管道，以实现以下目标：

*   将数据表示为图形，其中每个节点都是用户，并用标签标记边
*   将 ID 和散列标签映射到整数，以便 PageRank 可以使用它们
*   应用 PageRank
*   以可互操作的格式(AVRO)将结果存储到 HDFS 中

我们将图表示为`(source, destination, topic)`形式的元组包，其中每个元组表示节点之间的交互。 本例的源代码可以在[https://github.com/learninghadoop2/book-examples/blob/master/ch6/pagerank.pig](https://github.com/learninghadoop2/book-examples/blob/master/ch6/pagerank.pig)找到。

我们将把用户和标签的文本映射到数字 ID。 我们使用 Java String`hashCode()`方法执行此转换步骤，并将逻辑包装在`Eval`UDF 中。

### 备注

整数的大小实际上是图中节点和边数的上限。 对于生产代码，建议您使用更健壮的散列函数。

`StringToInt`类接受字符串作为输入，调用`hashCode()`方法，并将方法输出返回给 Pig。 自定义函数代码可在[https://github.com/learninghadoop2/book-examples/blob/master/ch6/udf/com/learninghadoop2/pig/udf/StringToInt.java](https://github.com/learninghadoop2/book-examples/blob/master/ch6/udf/com/learninghadoop2/pig/udf/StringToInt.java)找到。

```
package com.learninghadoop2.pig.udf;
import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;

public class StringToInt extends EvalFunc<Integer> {
    public Integer exec(Tuple input) throws IOException {
        if (input == null || input.size() == 0)
            return null;
        try {
            String str = (String) input.get(0);
            return str.hashCode();
        } catch(Exception e) {
          throw 
             new IOException("Cannot convert String to Int", e);
        }
    }
}
```

我们扩展`org.apache.pig.EvalFunc`并覆盖`exec`方法，以在函数输入上返回`str.hashCode()`。 `EvalFunc<Integer>`类使用 UDF(`Integer`)的返回类型进行参数化。

接下来，我们编译类并将其归档到 JAR 中，如下所示：

```
$ javac -classpath /opt/cloudera/parcels/CDH/lib/pig/pig.jar:$(hadoop classpath) com/learninghadoop2/pig/udf/StringToInt.java
$ jar cvf myudfs-pig.jar com/learninghadoop2/pig/udf/StringToInt.class
```

我们现在可以在 Pig 中注册 UDF 并创建`StringToInt`的别名，如下所示：

```
REGISTER myudfs-pig.jar
DEFINE StringToInt com.learninghadoop2.pig.udf.StringToInt();
```

我们过滤掉没有`destination`和没有`topic`的推文，如下所示：

```
tweets_graph_filtered = FILTER twitter_graph by 
(destination IS NOT NULL) AND 
(topic IS NOT null);
```

然后，我们将`source`、`destination`和`topic`转换为整数 ID：

```
from_to = foreach tweets_graph_filtered {
  GENERATE 
    StringToInt(source) as source_id, 
    StringToInt(destination) as destination_id, 
    StringToInt(topic) as topic_id;
}
```

一旦数据采用适当的格式，我们就可以重用 PageRank 的实现和 DataFu 提供的示例代码(位于[https://github.com/apache/incubator-datafu/blob/master/datafu-pig/src/main/java/datafu/pig/linkanalysis/PageRank.java](https://github.com/apache/incubator-datafu/blob/master/datafu-pig/src/main/java/datafu/pig/linkanalysis/PageRank.java))，如以下代码所示：

```
DEFINE PageRank datafu.pig.linkanalysis.PageRank('dangling_nodes','true');
```

我们首先创建一个包含`(source_id, destination_id, topic_id)`个元组的包，如下所示：

```
reply_to = group from_to by (source_id, destination_id, topic_id); 
```

我们统计每个元组的出现次数，即两个人谈论一个主题的次数，如下所示：

```
topic_edges = foreach reply_to {
  GENERATE flatten(group), ((double)COUNT(from_to.topic_id)) as w;
}
```

请记住，主题是我们图形的边；我们首先在源节点和主题边之间创建一个关联，如下所示：

```
topic_edges_grouped = GROUP topic_edges by (topic_id, source_id);
```

然后，我们对其进行重组，目的是添加目的节点和边权重，如下所示：

```
topic_edges_grouped = FOREACH topic_edges_grouped {
  GENERATE
    group.topic_id as topic,
    group.source_id as source,
    topic_edges.(destination_id,w) as edges;
}
```

创建 Twitter 图表后，我们将计算所有用户的 PageRank(`source_id`)：

```
topic_rank = FOREACH (GROUP topic_edges_grouped BY topic) {
  GENERATE
    group as topic,
    FLATTEN(PageRank(topic_edges_grouped.(source,edges))) as (source,rank);
}
topic_rank = FOREACH topic_rank GENERATE topic, source, rank;
```

我们将结果以 Avro 格式存储在 HDFS 中。 如果类路径中不存在 avro 依赖项，那么在访问各个字段之前，我们需要将 avro MapReduce JAR 文件添加到我们的环境中。 在 Pig 中，例如在 Cloudera CDH5 虚拟机上：

```
REGISTER /opt/cloudera/parcels/CDH/lib/avro/avro.jar
REGISTER /opt/cloudera/parcels/CDH/lib/avro/avro-mapred-hadoop2.jar 
STORE topic_rank INTO 'replies-pagerank' using AvroStorage();    
```

### 备注

在最后这两个部分中，我们对 Twitter 图表可能是什么样子以及主题和用户交互的概念意味着什么做了一些隐含的假设。 考虑到我们提出的限制，我们分析的结果社交网络将相对较小，不一定代表整个 Twitter 社交网络。 不鼓励从该数据集中推断结果。 在实践中，要生成健壮的社会交互模型，还需要考虑许多其他因素。

# 摘要

在本章中，我们介绍了 Apache Pig，一个在 Hadoop 上进行大规模数据分析的平台。 我们特别讨论了以下主题：

*   Pig 的目标是以一种方式提供类似数据流的抽象，而不需要动手进行 MapReduce 开发
*   Pig 的数据处理方法与 SQL 相比如何？在 SQL 中，Pig 是过程性的，而 SQL 是声明性的
*   Pig 入门-这是一项简单的任务，因为它是一个生成自定义代码的库，不需要额外的服务
*   Pig 提供的数据类型、核心函数和扩展机制概述
*   应用 Pig 详细分析 Twitter 数据集的示例，展示了它以非常简洁的方式表达复杂概念的能力
*   Piggybank、Elephant Bird 和 DataFu 等库如何为许多有用的预写 Pig 函数提供存储库
*   在下一章中，我们将通过探索对 HDFS 中存储的数据显示类似 SQL 的抽象的工具来回顾 SQL 比较