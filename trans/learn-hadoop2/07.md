# 第 7 章：Hadoop 和 SQL

MapReduce是一个强大的范例，它支持复杂的数据处理，可以揭示有价值的见解。 然而，正如前面几章所讨论的，它确实需要不同的思维模式，以及在将处理分析分解为一系列映射和还原步骤的模型方面的一些培训和经验。 有几个构建在 Hadoop 之上的产品可以提供 HDFS 中保存的数据的更高级别或更熟悉的视图，Pig 就是一个非常流行的产品。 本章将探讨在 Hadoop 上实现的另一个最常见的抽象：SQL。

在本章中，我们将介绍以下主题：

*   Hadoop 上的 SQL 有哪些使用案例？为什么它如此受欢迎
*   HiveQL，Apache Have 引入的 SQL 方言
*   使用 HiveQL 对 Twitter 数据集执行类似 SQL 的分析
*   HiveQL 如何近似关系数据库的常见功能，如连接和视图
*   HiveQL 如何允许将用户定义的函数合并到其查询中
*   Hadoop 上的 SQL 如何补充 Pig
*   其他 SQL-on-Hadoop 产品(如 Impala)及其与配置单元的不同之处

# 为什么选择 Hadoop 上的 SQL

到目前为止，我们已经了解了如何使用 MapReduce API 编写 Hadoop 程序，以及 Pig 拉丁语如何通过 UDF 为自定义业务逻辑提供脚本抽象和包装器。 PIG是一个非常强大的工具，但大多数开发人员或业务分析师并不熟悉其基于数据流的编程模型。 这类人浏览数据的传统选择工具是 SQL。

早在 2008 年，Facebook 就发布了 Have，这是第一个在 Hadoop 上广泛使用的 SQL 实现。

配置单元没有提供一种更快速地开发 map 和 Reduce 任务的方法，而是提供了*HiveQL*的实现，这是一种基于 SQL 的查询语言。 HIVE 接受 HiveQL 语句，并立即自动将查询转换为一个或多个 MapReduce 作业。 然后，它执行整个 MapReduce 程序并将结果返回给用户。

这个到Hadoop 的接口不仅减少了从数据分析中产生结果所需的时间，而且还大大拓宽了谁可以使用 Hadoop 的网络。 任何熟悉 SQL 的人都可以使用配置单元，而不需要软件开发技能。

这些属性的组合是 HiveQL 经常被用作业务和数据分析师对存储在 HDFS 上的数据执行即席查询的工具。 有了蜂窝，数据分析师可以在不需要软件开发人员参与的情况下改进查询。 就像 Pig 一样，配置单元还允许通过用户定义的函数来扩展 HiveQL，从而使基础 SQL 方言能够使用特定于业务的功能进行自定义。

## 其他基于 Hadoop 的 SQL 解决方案

虽然蜂巢是第一个引入并支持 HiveQL 的产品，但它不再是唯一的产品。 在本章的后面，我们还将讨论 Impala，它发布于 2013 年，已经是一个非常流行的工具，特别是对于低延迟查询。 还有其他的，但我们将主要讨论蜂巢和黑斑羚，因为它们是最成功的。

然而，在介绍 Hadoop 上 SQL 的核心特性和功能时，我们将给出使用配置单元的示例；尽管配置单元和 Impala 共享许多 SQL 特性，但它们也有许多不同之处。 我们不想不断地警告每个新功能，说明蜂巢与黑斑羚相比是如何支持这些新功能的。 我们通常会查看这两种产品共有的功能集的各个方面，但如果您同时使用这两种产品，阅读最新的发行说明以了解它们之间的区别是很重要的。

# 必备条件

在深入研究特定技术之前，让我们先生成一些数据，我们将在本章的示例中使用这些数据。 我们将创建以前的 Pig 脚本的修改版本作为此脚本的主要功能。 本章中的脚本假设以前使用的 Elephant Bird Jars 位于 HDFS 的`/jar`目录中。 完整源代码在[https://github.com/learninghadoop2/book-examples/blob/master/ch7/extract_for_hive.pig](https://github.com/learninghadoop2/book-examples/blob/master/ch7/extract_for_hive.pig)，但`extract_for_hive.pig`的核心如下：

```
-- load JSON data
tweets = load '$inputDir' using  com.twitter.elephantbird.pig.load.JsonLoader('-nestedLoad');
-- Tweets
tweets_tsv = foreach tweets {
generate 
    (chararray)CustomFormatToISO($0#'created_at', 
'EEE MMMM d HH:mm:ss Z y') as dt, 
    (chararray)$0#'id_str', 
(chararray)$0#'text' as text, 
    (chararray)$0#'in_reply_to', 
(boolean)$0#'retweeted' as is_retweeted, 
(chararray)$0#'user'#'id_str' as user_id, (chararray)$0#'place'#'id' as place_id;
}
store tweets_tsv into '$outputDir/tweets' 
using PigStorage('\u0001');
-- Places
needed_fields = foreach tweets {
   generate 
(chararray)CustomFormatToISO($0#'created_at', 
'EEE MMMM d HH:mm:ss Z y') as dt, 
     (chararray)$0#'id_str' as id_str, 
$0#'place' as place;
}
place_fields = foreach needed_fields {
generate 
    (chararray)place#'id' as place_id, 
    (chararray)place#'country_code' as co, 
    (chararray)place#'country' as country, 
    (chararray)place#'name' as place_name, 
    (chararray)place#'full_name' as place_full_name, 
    (chararray)place#'place_type' as place_type;
}
filtered_places = filter place_fields by co != '';
unique_places = distinct filtered_places;
store unique_places into '$outputDir/places' 
using PigStorage('\u0001');

-- Users
users = foreach tweets {
   generate 
(chararray)CustomFormatToISO($0#'created_at', 
'EEE MMMM d HH:mm:ss Z y') as dt, 
(chararray)$0#'id_str' as id_str, 
$0#'user' as user;
}
user_fields = foreach users {
   generate 
    (chararray)CustomFormatToISO(user#'created_at', 
'EEE MMMM d HH:mm:ss Z y') as dt,
  (chararray)user#'id_str' as user_id, 
  (chararray)user#'location' as user_location, 
  (chararray)user#'name' as user_name, 
  (chararray)user#'description' as user_description, 
  (int)user#'followers_count' as followers_count, 
  (int)user#'friends_count' as friends_count, 
  (int)user#'favourites_count' as favourites_count, 
  (chararray)user#'screen_name' as screen_name, 
  (int)user#'listed_count' as listed_count;

}
unique_users = distinct user_fields;
store unique_users into '$outputDir/users' 
using PigStorage('\u0001');
```

按如下方式运行此脚本：

```
$ pig –f extract_for_hive.pig –param inputDir=<json input> -param outputDir=<output path>

```

前面的代码将数据写入 tweet、user 和 place 信息的三个单独的 TSV 文件中。 请注意，在`store`命令中，我们在调用`PigStorage`时传递一个参数。 这个参数将默认字段分隔符从制表符更改为 Unicode 值 U0001，或者您也可以使用*Ctrl*+*C*+*A*。 这通常用作配置单元表格中的分隔符，对我们特别有用，因为我们的推文数据可能包含其他字段中的制表符。

## 蜂窝概述

现在，我们将展示如何将数据导入配置单元，并针对配置单元提供的表抽象表对数据运行查询。 在本例中以及本章的其余部分中，我们将假设查询被键入到 shell 中，可以通过执行`hive`命令调用这些查询。

最近，一个名为 Beeline 的客户端也出现了，并且很可能在不久的将来成为首选的 CLI 客户端。

将任何新数据导入配置单元时，通常有三个阶段的流程：

*   创建要向其中导入数据的表的规范
*   将数据导入到创建的表中
*   对表执行 HiveQL 查询

大多数 HiveQL 语句直接类似于标准 SQL 中名称相似的语句。 在本章中，我们只假定您对 SQL 的了解不多，但是如果您需要复习一下，有很多很好的在线学习资源。

HIVE 提供了数据的结构化查询视图，要实现这一点，我们必须首先定义表列的规范，并将数据导入表中，然后才能执行任何查询。 表规范是使用`CREATE`语句生成的，该语句指定表名、表列的名称和类型以及有关表存储方式的一些元数据：

```
CREATE table tweets (
created_at string,
tweet_id string,
text string,
in_reply_to string,
retweeted boolean,
user_id string,
place_id string
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE;
```

该语句创建一个由数据集中列的名称及其数据类型列表定义的新表`tweets`。 我们指定字段由 Unicode U0001 字符分隔，用于存储数据的格式为`TEXTFILE`。

可以使用`LOAD DATA`语句从 HDFS`tweets/`中的某个位置导入数据：

```
LOAD DATA INPATH 'tweets' OVERWRITE INTO TABLE tweets;
```

默认情况下，配置单元表格的数据存储在 HDFS 的`/user/hive/warehouse`下。 如果为`LOAD`语句指定了 HDFS 上数据的路径，则它不会简单地将数据复制到`/user/hive/warehouse`中，而是会将其移动到那里。 如果要分析其他应用程序使用的 HDFS 上的数据，请创建副本或使用稍后介绍的`EXTERNAL`机制。

将数据导入配置单元后，我们可以对其运行查询。 例如：

```
SELECT COUNT(*) FROM tweets;
```

前面的代码将返回数据集中存在的 tweet 总数。 HiveQL 与 SQL 一样，在关键字、列或表名方面不区分大小写。 按照惯例，SQL 语句使用大写作为 SQL 语言关键字，当在文件中使用 HiveQL 时，我们通常会遵循这一点，稍后将展示这一点。 然而，在输入交互式命令时，我们会经常选择阻力最小的行，并使用小写。

如果仔细观察前面示例中各种命令所花费的时间，您会注意到，将数据加载到表中所需的时间与创建表规范的时间差不多，但即使是简单地计算所有行的时间也要长得多。 输出还显示，表创建和数据加载实际上并不会导致执行 MapReduce 作业，这就解释了执行时间非常短的原因。

## 蜂窝表的性质

尽管配置单元将数据文件复制到其工作目录中，但它在此时并不实际将输入数据处理成行。

`CREATE TABLE`和`LOAD DATA`语句本身并不真正创建具体的表数据；相反，它们生成的元数据将在配置单元生成 MapReduce 作业以访问表中概念上存储但实际驻留在 HDFS 上的数据时使用。 尽管 HiveQL 语句引用特定的表结构，但它由配置单元负责生成代码，将其正确映射到存储数据文件的实际磁盘格式。

这似乎表明配置单元不是*真正的*数据库；这是真的，事实并非如此。关系数据库需要在接收数据之前定义一个表模式，然后只接收符合该规范的数据，而配置单元要灵活得多。 配置单元表的不太具体的性质意味着模式可以基于数据已经到达时定义，而不是基于数据应该是如何的假设，这可能被证明是错误的。 尽管不管采用何种技术，可变的数据格式都很麻烦，但蜂窝模型在问题出现时(而不是在问题出现时)提供了额外的自由度。

# 蜂窝架构

在版本 2 之前，Hadoop 主要是批处理系统。 正如我们在前几章中看到的，MapReduce 作业往往有很高的延迟和提交和调度带来的开销。 在内部，配置单元将 HiveQL 语句编译成 MapReduce 作业。 传统上，配置单元查询的特点是高延迟。 随着毒刺计划和蜂巢 0.13 中引入的改进(我们将在后面讨论)，这一点已经改变。

HIVE 作为一个客户端应用程序运行，该应用程序处理 HiveQL 查询，将其转换为 MapReduce 作业，然后将这些作业提交给 Hadoop 集群，或者提交给 Hadoop 1 中的原生 MapReduce，或者提交给 Hadoop 2 中在 YAR 上运行的 MapReduce Application Master。

无论采用哪种模型，配置单元都使用一个称为元存储的组件，在该组件中，它保存有关系统中定义的表的所有元数据。 具有讽刺意味的是，这些数据存储在专门用于蜂巢的关系数据库中。 在配置单元的最早版本中，所有客户端都直接与元存储通信，但这意味着配置单元 CLI 工具的每个用户都需要知道元存储用户名和密码。

HiveServer 的创建目的是充当远程客户端的入口点，远程客户端也可以充当单个访问控制点，并控制对底层元存储的所有访问。 由于 HiveServer 的限制，访问配置单元的最新方式是通过多客户端 HiveServer2。

### 备注

HiveServer2引入了对其前身的许多改进，包括用户身份验证和对来自同一客户端的多个连接的支持。 有关的更多信息，请参见[https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2](https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2)。

可以分别使用`hive --service hiveserver`和`hive --service hiveserver2`命令手动执行`HiveServer`和`HiveServer2`的实例。

在本章前面和后面看到的示例中，我们隐式使用 HiveServer 通过配置单元命令行工具提交查询。 HiveServer2 与 Beeline 一起提供。 出于兼容性和成熟性的原因，Beeline 相对较新，这两个工具都可以在 Cloudera 和大多数其他主要发行版上使用。 Beeline 客户端是核心 Apache Have 发行版的一部分，因此也是完全开源的。 可以使用以下命令在嵌入式版本中执行直线：

```
$ beeline -u jdbc:hive2://

```

## 数据类型

HiveQL 支持标准数据库系统提供的许多常见数据类型。 这些类型包括原语类型(如`float`、`double`、`int,`和`string`)到到结构化集合类型，这些类型提供 SQL类似于`arrays`、`structs`和`unions`等类型(`structs`具有某些字段的选项)。 因为配置单元是用 Java 实现的，所以原语类型的行为将类似于它们的 Java 对应物。 我们可以将配置单元数据类型分为以下五大类：

*   **数值**：`tinyint`、`smallint`、`int`、`bigint`、`float`、`double`和`decimal`
*   **日期和时间**：`timestamp`和`date`
*   **字符串**：`string`、`varchar`和`char`
*   **集合**：`array`、`map`、`struct`和`uniontype`
*   **其他**：`boolean`、`binary`和`NULL`

## DDL 语句

HiveQL 提供了多个语句来创建、删除和更改数据库、表和视图。 `CREATE DATABASE <name>`语句创建具有给定名称的新数据库。 数据库表示包含表和视图元数据的命名空间。 如果存在多个数据库，则`USE <database name>`语句指定使用哪个数据库来查询表或创建新元数据。 如果未显式指定数据库，则配置单元将针对`default`数据库运行所有语句。 `SHOW [DATABASES, TABLES, VIEWS]`显示数据仓库中当前可用的数据库，以及当前使用的数据库中存在哪些表元数据和视图元数据：

```
CREATE DATABASE twitter;
SHOW databases;
USE twitter;
SHOW TABLES;
```

`CREATE TABLE [IF NOT EXISTS] <name>`语句创建具有给定名称的表。 正如前面提到的，真正创建的是表示表及其到 HDFS 上文件的映射的元数据，以及存储数据文件的目录。 如果已存在同名的表或视图，则配置单元将引发异常。

表名和列名都不区分大小写。 在较早版本的配置单元(0.12 和更早版本)中，表名和列名中只允许使用字母数字和下划线字符。 从配置单元 0.13 开始，系统支持列名中的 Unicode 字符。 保留字(如`load`和`create`)需要用反号(`字符)转义才能按字面处理。

关键字`EXTERNAL`指定表存在于配置单元无法控制的资源中，这是在基于 Hadoop 的**Extract-Transform-Load**(**ETL**)管道开始时从另一个源提取数据的有用机制。 `LOCATION`子句指定要在何处找到源文件(或目录)。 以下代码中使用了`EXTERNAL`关键字和`LOCATION`子句：

```
CREATE EXTERNAL TABLE tweets (
created_at string,
tweet_id string,
text string,
in_reply_to string,
retweeted boolean,
user_id string,
place_id string
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE
LOCATION '${input}/tweets';
```

此表将在元存储中创建，但不会将数据复制到`/user/hive/warehouse`目录。

### 提示

请注意，配置单元没有主键或唯一标识符的概念。 在将数据加载到数据仓库之前，唯一性和数据规范化是需要解决的问题。

`CREATE VIEW <view name> … AS SELECT`语句创建具有给定名称的视图。 例如，我们可以创建一个视图来将转发与其他消息隔离开来，如下所示：

```
CREATE VIEW retweets 
COMMENT 'Tweets that have been retweeted'
AS SELECT * FROM tweets WHERE retweeted = true;
```

除非另有指定，否则列名派生自定义的`SELECT`语句。 配置单元当前不支持实例化视图。

`DROP TABLE`和`DROP VIEW`语句删除给定表或视图的元数据和数据。 删除`EXTERNAL`表或视图时，只会删除元数据，实际数据文件不会受到影响。

配置单元允许通过`ALTER TABLE`语句更改表元数据，该语句可用于更改列类型、名称、位置和注释，或添加和替换列。

在添加列时，重要的是要记住，只有元数据会更改，而不是数据集本身。 这意味着，如果我们在表的中间添加旧文件中不存在的列，那么在从旧数据中进行选择时，我们可能会在错误的列中获得错误的值。 这是因为我们将使用新格式查看旧文件。 在讨论 Avro 时，我们将在[章](08.html "Chapter 8. Data Lifecycle Management")、*数据生命周期管理*中讨论数据和架构迁移。

同样，`ALTER VIEW <view name> AS <select statement>`会更改现有视图的定义。

## 文件格式和存储

配置单元表的底层数据文件与 HDFS 上的任何其他文件没有什么不同。 用户可以使用其他工具直接读取蜂窝表中的 HDFS 文件。 他们还可以使用其他工具写入 HDFS 文件，这些文件可以通过`CREATE EXTERNAL TABLE`或`LOAD DATA INPATH`加载到配置单元中。

配置单元使用`the Serializer`和`Deserializer`类、SerDe 以及`FileFormat`来读写表行。 如果未指定`ROW FORMAT`或在`CREATE TABLE`语句中指定了`ROW FORMAT DELIMITED`，则使用本机 SerDe。 `DELIMITED`子句指示系统读取分隔文件。 可以使用`ESCAPED BY`子句对分隔符字符进行转义。

配置单元当前使用以下`FileFormat`类来读写 HDFS 文件：

*   `TextInputFormat`和`HiveIgnoreKeyTextOutputFormat`：是否将以纯文本文件格式读取/写入数据
*   `SequenceFileInputFormat`和`SequenceFileOutputFormat`：类以 Hadoop`SequenceFile`格式读取/写入数据

此外，以下 SerDe 类可用于序列化和反序列化数据：

*   `MetadataTypedColumnsetSerDe`：将读/写分隔的记录，如 CSV 或制表符分隔的记录
*   `ThriftSerDe`和`DynamicSerDe`：将读/写个节约对象

### JSON

从版本 0.13 开始，配置单元将与本机`org.apache.hive.hcatalog.data.JsonSerDe`一起提供。 对于较旧版本的配置单元，hive-json-serde(位于[https://github.com/rcongiu/Hive-JSON-Serde](https://github.com/rcongiu/Hive-JSON-Serde))无疑是功能最丰富的 JSON 序列化/反序列化模块之一。

我们可以使用任一模块加载 JSON tweet，而不需要任何预处理，只需定义与 JSON 文档内容匹配的配置单元模式。 在下面的示例中，我们使用配置单元-JSON-SERDE。

与任何第三方模块一样，我们使用以下代码将 SerDe JAR 加载到配置单元中：

```
ADD JAR JAR json-serde-1.3-jar-with-dependencies.jar;
```

然后，我们发出通常的`CREATE`语句，如下所示：

```
CREATE EXTERNAL TABLE tweets (
   contributors string,
   coordinates struct <
      coordinates: array <float>,
      type: string>,
   created_at string,
   entities struct <
      hashtags: array <struct <
            indices: array <tinyint>,
            text: string>>,
…
)
ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
STORED AS TEXTFILE
LOCATION 'tweets';  
```

使用此 SerDe，我们可以将嵌套文档(如实体或用户)映射到`struct`或`map`类型。 我们告诉配置单元存储在`LOCATION 'tweets'`中的数据是文本(`STORED AS TEXTFILE`)，并且每一行都是一个 JSON 对象(`ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe`‘)。 在配置单元 0.13 和更高版本中，我们可以将此属性表示为`ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'`。

手动指定复杂文档的模式可能是一个乏味且容易出错的过程。 `hive-json`模块(位于[https://github.com/hortonworks/hive-json](https://github.com/hortonworks/hive-json))是分析大型文档并生成适当的配置单元模式的便捷实用程序。 根据文档集合的不同，可能需要进一步改进。

在我们的示例中，我们使用了用`hive-json`生成的模式，该模式将 twets JSON 映射到许多`struct`数据类型。 这允许我们使用方便的点符号来查询数据。 例如，我们可以使用以下代码提取用户对象的屏幕名称和描述字段：

```
SELECT user.screen_name, user.description FROM tweets_json LIMIT 10;
```

### _

**AvroSerde**([https://cwiki.apache.org/confluence/display/Hive/AvroSerDe](https://cwiki.apache.org/confluence/display/Hive/AvroSerDe))允许我们以Avro 格式读取和写入数据。 从 0.14 开始，可以使用`STORED AS AVRO`语句创建 Avro 支持的表，配置单元将负责为该表创建适当的 Avro 模式。 以前的配置单元版本稍微更冗长一些。

作为一个例子，让我们将我们在[章](06.html "Chapter 6. Data Analysis with Apache Pig")，*Data Analysis with Apache Pig*中生成的 PageRank 数据集加载到配置单元中。 此数据集是使用 Pig 的`AvroStorage`类创建的，具有以下架构：

```
{
  "type":"record",
  "name":"record",
  "fields": [
    {"name":"topic","type":["null","int"]},
    {"name":"source","type":["null","int"]},
    {"name":"rank","type":["null","float"]}
  ]
}  
```

表结构被捕获在 Avro 记录中，该记录包含标题信息(名称和限定名称的可选名称空间)和字段数组。 每个字段都指定了其名称和类型以及可选的文档字符串。

对于少数字段，类型不是单个值，而是一对值，其中一个为 NULL。 这是一个 Avro 联合，这是处理可能具有空值的列的惯用方式。 Avro 将 null 指定为具体类型，并且需要以这种方式指定其他类型可能具有 null 值的任何位置。 当我们使用以下架构时，这将为我们透明地处理。

有了这个定义，我们现在可以创建一个配置单元表格，该表格使用此模式作为其表格规范，如下所示：

```
CREATE EXTERNAL TABLE tweets_pagerank
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
WITH SERDEPROPERTIES ('avro.schema.literal'='{
    "type":"record",
    "name":"record",
    "fields": [
        {"name":"topic","type":["null","int"]},
        {"name":"source","type":["null","int"]},
        {"name":"rank","type":["null","float"]}
    ]
}')
STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION '${data}/ch5-pagerank';
```

然后，查看配置单元中的以下表定义(另请注意，我们将在[章](08.html "Chapter 8. Data Lifecycle Management")，*数据生命周期管理*中介绍的 HCatalog 也支持此类定义)：

```
DESCRIBE tweets_pagerank;
OK
topic                 int                   from deserializer   
source                int                   from deserializer   
rank                  float                 from deserializer  
```

在 DDL 中，我们告诉配置单元使用`AvroContainerInputFormat`和`AvroContainerOutputFormat`以 Avro 格式存储数据。 每行都需要使用`org.apache.hadoop.hive.serde2.avro.AvroSerDe`进行序列化和反序列化。 表模式由配置单元从嵌入在`avro.schema.literal`中的 Avro 模式中推断出来。

或者，我们可以在 HDFS 上存储模式，并让配置单元读取它以确定表结构。 在名为`pagerank.avsc`的文件中创建前面的模式-这是 Avro 模式的标准文件扩展名。 然后将其放在 HDFS 上；我们更希望有一个公共位置来存放模式文件，比如`/schema/avro`。 最后，使用`avro.schema.url`serDe 属性`WITH SERDEPROPERTIES ('avro.schema.url'='hdfs://<namenode>/schema/avro/pagerank.avsc')`定义表。

如果类路径中不存在 avro 依赖项，我们需要在访问单个字段之前将 avro`MapReduce`jar 添加到我们的环境中。 在配置单元内的 Cloudera CDH5 虚拟机上：

```
ADD JAR /opt/cloudera/parcels/CDH/lib/avro/avro-mapred-hadoop2.jar; 
```

我们也可以像使用其他桌子一样使用这张桌子。 例如，我们可以查询数据来选择 PageRank 较高的用户和主题对：

```
SELECT source, topic from tweets_pagerank WHERE rank >= 0.9;
```

在[章](08.html "Chapter 8. Data Lifecycle Management")，*数据生命周期管理*中，我们将看到 Avro 和`avro.schema.url`如何在支持模式迁移方面发挥重要作用。

### 柱状商店

配置单元还可以通过`ORC`([https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC))和`Parquet`([https://cwiki.apache.org/confluence/display/Hive/Parquet](https://cwiki.apache.org/confluence/display/Hive/Parquet))格式利用列式存储。

如果一个表定义了非常多的列，那么对于任何给定的查询来说，只处理这些列的一小部分是很常见的。 但即使在 SequenceFile 中，也会从磁盘读取、解压缩和处理每一整行及其所有列。 对于我们事先知道不感兴趣的数据，这会消耗大量系统资源。

传统的关系数据库也以行为单位存储数据，一种名为**Columnar****的数据库将改为以列为重点。 在最简单的模型中，表中的每列都有一个文件，而不是每个表都有一个文件。 如果查询只需要访问总共有 100 列的表中的 5 列，则只会读取这 5 列的文件。 ORC 和 Parquet 都使用这一原则以及其他优化来实现更快的查询。**

 **## 查询

可以使用熟悉的`SELECT … FROM`语句查询表。 `WHERE`语句允许指定过滤条件，`GROUP BY`聚合记录，`ORDER BY`指定排序条件，`LIMIT`指定要检索的记录数。 聚合函数(如`count`和`sum`)可以应用于聚合记录。 例如，下面的代码返回数据集中最多产的前 10 位用户：

```
SELECT user_id, COUNT(*) AS cnt FROM tweets GROUP BY user_id ORDER BY cnt DESC LIMIT 10
```

这将返回数据集中最多产的前 10 名用户：

```
2263949659 4
1332188053  4
959468857  3
1367752118  3
362562944  3
58646041  3
2375296688  3
1468188529  3
37114209  3
2385040940  3

```

我们可以通过设置以下内容来提高`hive`输出的可读性：

```
SET hive.cli.print.header=true;
```

这将指示`hive`(尽管不是`beeline`)打印列名作为输出的一部分。

### 提示

您可以将该命令添加到。 `hiverc`文件通常位于执行用户的主目录的根目录中，以便将其应用于所有`hive`CLI 会话。

HiveQL 实现了一个`JOIN`运算符，它使我们能够将表组合在一起。 在*先决条件*部分中，我们为 User 和 Place 对象生成了单独的数据集。 现在让我们使用外部表将它们加载到配置单元中。

我们首先创建一个`user`表来存储用户数据，如下所示：

```
CREATE EXTERNAL TABLE user (
created_at string,
user_id string,
`location` string,
name string,
description string,
followers_count bigint,
friends_count bigint,
favourites_count bigint,
screen_name string,
listed_count bigint
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE
LOCATION '${input}/users';
```

然后，我们创建一个`place`表来存储位置数据，如下所示：

```
CREATE EXTERNAL TABLE place (
place_id string,
country_code string,
country string,
`name` string,
full_name string,
place_type string
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE
LOCATION '${input}/places';
```

我们可以使用`JOIN`运算符显示最多产的 10 个用户的名称，如下所示：

```
SELECT tweets.user_id, user.name, COUNT(tweets.user_id) AS cnt 
FROM tweets 
JOIN user ON user.user_id  = tweets.user_id
GROUP BY tweets.user_id, user.user_id, user.name 
ORDER BY cnt DESC LIMIT 10; 
```

### 提示

配置单元中仅支持相等、外连接和左(半)连接。

请注意，可能有个条目具有给定的用户 ID，但`followers_count`、`friends_count`和`favourites_count`列的值不同。 为了避免重复的条目，我们只对`tweets`表中的`user_id`进行计数。

我们可以重写前面的查询，如下所示：

```
SELECT tweets.user_id, u.name, COUNT(*) AS cnt 
FROM tweets 
join (SELECT user_id, name FROM user GROUP BY user_id, name) u
ON u.user_id = tweets.user_id
GROUP BY tweets.user_id, u.name 
ORDER BY cnt DESC LIMIT 10;   
```

我们不是直接连接`user`表，而是执行子查询，如下所示：

```
SELECT user_id, name FROM user GROUP BY user_id, name;
```

子查询提取唯一的用户 ID 和名称。 请注意，配置单元对子查询的支持有限，以往仅允许在`SELECT`语句的`FROM`子句中使用子查询。 HIVE 0.13 还在`WHERE`子句中添加了对子查询的有限支持。

HiveQL是一种不断发展的丰富语言，对它的全面阐述超出了本章的范围。 其查询和动态链接库功能的描述可以在[https://cwiki.apache.org/confluence/display/Hive/LanguageManual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual)中找到。

## 针对给定工作负载构建配置单元表

通常，配置单元并不是单独使用的，而是在创建表时考虑到特定的工作负载或需要以适合包含在自动化流程中的方式调用。 我们现在将探讨其中的一些场景。

## 对表进行分区

对于列式文件格式，我们解释了在处理查询时尽早排除不需要的数据的好处。 在 SQL 中使用类似的概念已经有一段时间了：表分区。

创建分区表时，会将列指定为分区键。 然后将具有该键的所有值存储在一起。 在配置单元的例子中，每个分区键的不同子目录都是在 HDFS 上仓库位置的表目录下创建的。

了解分区列的基数很重要。 由于截然不同的值太少，由于文件仍然非常大，好处就会减少。 如果值太多，则查询可能需要扫描大量文件才能访问所有必需的数据。 也许最常见的分区键是基于日期的分区键。 例如，我们可以根据`created_at`列(即用户首次注册的日期)对前面的`user`表进行分区。 请注意，由于按定义对表进行分区会影响其文件结构，因此我们现在将该表创建为非外部表，如下所示：

```
CREATE TABLE partitioned_user (
created_at string,
user_id string,
`location` string,
name string,
description string,
followers_count bigint,
friends_count bigint,
favourites_count bigint,
screen_name string,
listed_count bigint
)  PARTITIONED BY (created_at_date string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE;
```

要将数据加载到分区中，我们可以显式地给出要向其中插入数据的分区的值，如下所示：

```
INSERT INTO TABLE partitioned_user
PARTITION( created_at_date = '2014-01-01')
SELECT 
created_at,
user_id,
location,
name,
description,
followers_count,
friends_count,
favourites_count,
screen_name,
listed_count
FROM user;
```

这充其量是冗长的，因为我们需要为每个分区键值编写一条语句；如果一条`LOAD`或`INSERT`语句包含多个分区的数据，那么它就不能工作。 HIVE 还有一个称为动态分区的功能，它可以在这方面为我们提供帮助。 我们设置了以下三个变量：

```
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;
SET hive.exec.max.dynamic.partitions.pernode=5000;
```

前两条语句使所有分区(`nonstrict`选项)都是动态的。 第三个允许在每个映射器和减少器节点上创建 5000 个不同的分区。

然后，我们只需使用要用作分区键的列名，配置单元将根据给定行的键的值将数据插入到分区中：

```
INSERT INTO TABLE partitioned_user
PARTITION( created_at_date )
SELECT 
created_at,
user_id,
location,
name,
description,
followers_count,
friends_count,
favourites_count,
screen_name,
listed_count,
to_date(created_at) as created_at_date
FROM user;
```

即使我们在这里只使用一个分区列，我们也可以通过多个列键对表进行分区；只需在`PARTITIONED BY`子句中将它们作为逗号分隔的列表。

请注意，分区键列需要作为用于插入到分区表中的任何语句的最后一列。 在前面的代码中，我们使用配置单元的`to_date`函数将`created_at`时间戳转换为`YYYY-MM-DD`格式的字符串。

分区数据在 HDFS 中存储为`/path/to/warehouse/<database>/<table>/key=<value>`。 在我们的示例中，`partitioned_user`表结构将类似于`/user/hive/warehouse/default/partitioned_user/created_at=2014-04-01`。

如果数据直接添加到文件系统中(例如，通过某个第三方处理工具或通过`hadoop fs -put`)，元存储将不会自动检测新分区。 用户将需要为每个新添加的分区手动运行如下所示的`ALTER TABLE`语句：

```
ALTER TABLE <table_name> ADD PARTITION <location>;
```

要为元存储中当前不存在的所有分区添加元数据，我们可以使用：`MSCK REPAIR TABLE <table_name>;`语句。 在 EMR 上，这相当于执行以下语句：

```
ALTER TABLE <table_name> RECOVER PARTITIONS; 
```

请注意，这两个语句也适用于`EXTERNAL`表。 在下一章中，我们将了解如何利用此模式创建灵活且可互操作的管道。

### 覆盖和更新数据

当我们需要更新表的一部分时，分区也很有用。 通常，以下形式的语句将替换目标表的所有数据：

```
INSERT OVERWRITE INTO <table>…
```

如果省略`OVERWRITE`，则每个`INSERT`语句都会向表中添加额外的数据。 有时，这是可取的，但通常情况下，被摄取到配置单元表中的源数据旨在完全更新数据的子集，并保持其余数据不受影响。

如果我们对表的分区执行`INSERT OVERWRITE`语句(或`LOAD OVERWRITE`语句)，那么只有指定的分区会受到影响。 因此，如果我们要插入用户数据，并且只想影响源文件中包含数据的分区，我们可以通过在前面的`INSERT`语句中添加`OVERWRITE`关键字来实现这一点。

我们还可以向`SELECT`语句添加警告。 例如，假设我们只想更新特定月份的数据：

```
INSERT INTO TABLE partitioned_user
PARTITION (created_at_date)
SELECT created_at ,
user_id,
location,
name,
description,
followers_count,
friends_count,
favourites_count,
screen_name,
listed_count,
to_date(created_at) as created_at_date
FROM user 
WHERE to_date(created_at) BETWEEN '2014-03-01' and '2014-03-31';
```

### 扣合和排序

对表进行分区是一种结构，您可以通过在针对表的查询的`WHERE`子句中使用分区列(或多个列)来显式地利用该结构。 还有另一种称为 Baketing 的机制，它可以进一步分割表的存储方式，并以一种允许配置单元自身优化其内部查询计划以利用该结构的方式来实现这一点。

让我们创建 tweet 和用户表的分桶版本；请注意`CREATE TABLE`语句中的以下附加`CLUSTER BY`和`SORT BY`语句：

```
CREATE table bucketed_tweets (
tweet_id string,
text string,
in_reply_to string,
retweeted boolean,
user_id string,
place_id string
)  PARTITIONED BY (created_at string)
CLUSTERED BY(user_ID) into 64 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE;

CREATE TABLE bucketed_user (
user_id string,
`location` string,
name string,
description string,
followers_count bigint,
friends_count bigint,
favourites_count bigint,
screen_name string,
listed_count bigint
)  PARTITIONED BY (created_at string)
CLUSTERED BY(user_ID) SORTED BY(name) into 64 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE;
```

请注意，我们将twets 表更改为也要分区；您只能存储已分区的表。

就像我们在插入分区表时需要指定分区列一样，我们还必须注意确保插入到存储桶表中的数据被正确地聚集在一起。 我们通过在将数据插入到表中之前设置以下标志来实现这一点：

```
SET hive.enforce.bucketing=true;
```

与分区表一样，在使用`LOAD DATA`语句时不能应用 Baketing 函数；如果希望将外部数据加载到分时段的表中，请首先将其插入到临时表中，然后使用`INSERT…SELECT…`语法填充分时段的表。

将数据插入到存储桶表中时，将根据应用于`CLUSTERED BY`子句中指定的列的散列函数的结果将行分配给存储桶。

当我们需要连接两个存储桶类似的表时，创建表的最大优势之一就出现在上面的示例中。 因此，例如，任何以下形式的查询都将得到极大的改进：

```
SET hive.optimize.bucketmapjoin=true;
SELECT …
FROM bucketed_user u JOIN bucketed_tweet t
ON u.user_id = t.user_id;
```

由于联接是在用于存储表的列上执行的，配置单元可以优化处理量，因为它知道每个存储桶在两个表中包含相同的一组`user_id`列。 在确定要匹配的行时，只需要比较存储桶中的行，而不需要比较整个表。 这确实要求两个表都聚集在同一列上，并且存储桶编号要么相同，要么是另一个的倍数。 在后一种情况下，假设一个表聚集到 32 个桶中，另一个表聚集到 64 个桶中，用于将数据分配给桶的默认哈希函数的性质意味着第一个表中的桶 3 中的 ID 将覆盖第二个表中的桶 3 和 35 中的 ID。

### 采样数据

在使用配置单元对表中的数据进行采样的功能时，对表进行分段化也会有所帮助。 采样允许查询仅收集表中全部行的指定子集。 当您有一个非常大的表，并且具有适度一致的数据模式时，这很有用。 在这种情况下，将查询应用于一小部分数据会快得多，并且仍然会给出具有广泛代表性的结果。 当然，请注意，这只适用于您希望确定表特征(如数据中的模式范围)的查询；如果您试图计算任何内容，则需要将结果缩放到完整的表大小。

对于未分桶的表，您可以通过指定查询应仅应用于表的特定子集，采用与我们前面看到的类似的机制进行采样：

```
SELECT max(friends_count)
FROM user TABLESAMPLE(BUCKET 2 OUT OF 64 ON name);
```

在此查询中，配置单元将根据 Name 列将表中的行有效地散列到 64 个存储桶中。 然后，它将只使用第二个存储桶进行查询。 可以指定多个存储桶，如果给出`RAND()`作为`ON`子句，则整行都由 bakting 函数使用。

虽然成功，但效率很低，因为需要扫描整个表才能生成所需的数据子集。 如果我们对存储桶表进行采样，并确保采样的存储桶数等于或等于表中存储桶的倍数，则配置单元将只读取相关的存储桶。 例如：

```
SELECT MAX(friends_count)
FROM bucketed_user TABLESAMPLE(BUCKET 2 OUT OF 32 on user_id);
```

在前面针对`bucketed_user`表的查询中，该表是在`user_id`列上使用 64 个存储桶创建的，由于它使用同一列，因此采样将只读取所需的存储桶。 在本例中，这些将是来自每个分区的存储桶 2 和 34。

抽样的最终形式是块抽样。 在这种情况下，我们可以指定要采样的表的所需数量，而配置单元将使用近似值，只在 HDFS 上读取足够的源数据块来满足所需的大小。 目前，数据大小可以指定为表的百分比、绝对数据大小或行数(在每个块中)。 `TABLESAMPLE`的语法如下所示，每个拆分将分别对表的 0.5%、1 GB 的数据或 100 行进行采样：

```
TABLESAMPLE(0.5 PERCENT)
TABLESAMPLE(1G)
TABLESAMPLE(100 ROWS)
```

如果您对后一种采样形式感兴趣，请参考文档，因为支持的输入格式和文件格式有一些特定限制。

## 编写脚本

我们可以将配置单元命令放在一个文件中，并在`hive`CLI 实用程序中使用`-f`选项运行它们：

```
$ cat show_tables.hql
show tables;
$ hive -f show_tables.hql 

```

我们可以通过`hiveconf`机制将 HiveQL 语句参数化。 这允许我们在使用环境变量时指定环境变量名，而不是在调用点指定环境变量名。 例如：

```
$ cat show_tables2.hql
show tables like '${hiveconf:TABLENAME}';
$ hive -hiveconf TABLENAME=user -f show_tables2.hql

```

也可以在配置单元脚本或交互式会话中设置该变量：

```
SET TABLE_NAME='user';
```

前面的`hiveconf`参数将在与配置单元配置选项相同的名称空间中添加任何新变量。 从配置单元 0.8 开始，有一个名为`hivevar`的类似选项，它可以将任何用户变量添加到不同的名称空间中。 使用`hivevar`时，前面的命令如下所示：

```
$ cat show_tables3.hql
show tables like '${hivevar:TABLENAME}';
$ hive -hivevar TABLENAME=user –f show_tables3.hql

```

或者我们可以交互地编写命令：

```
SET hivevar:TABLE_NAME='user';
```

# 配置单元和亚马逊网络服务

使用Elastic MapReduce 作为AWS Hadoop-on-Demand 服务，当然可以在 EMR 集群上运行配置单元。 但是也可以使用任何 Hadoop 集群中的 Amazon 存储服务，特别是 S3，无论是在 EMR 中还是您自己的本地集群中。

## 蜂窝和 S3

正如在[第 2 章](02.html "Chapter 2. Storage")，*存储*中提到的，可以为Hadoop 指定 HDFS 以外的默认文件系统，S3 是一个选项。 但是，它不一定是要么全有要么全无的事情；可以将特定的表存储在 S3 中。 这些表的数据将被检索到集群中进行处理，生成的任何数据都可以写入不同的 S3 位置(同一个表不能是单个查询的源和目标)，也可以写入 HDFS。

我们可以使用如下命令获取推文数据的文件并将其放到 S3 中的某个位置：

```
$ aws s3 put tweets.tsv s3://<bucket-name>/tweets/

```

我们首先需要指定可以访问存储桶的访问密钥和秘密访问密钥。 这可以通过三种方式实现：

*   在配置单元 CLI 中将`fs.s3n.awsAccessKeyId`和`fs.s3n.awsSecretAccessKey`设置为适当的值
*   在`hive-site.xml`中设置相同的值，但请注意，这将 S3 的使用限制为单组凭据
*   在表 URL 中显式指定表位置，即`s3n://<access key>:<secret access key>@<bucket>/<path>`

然后，我们可以创建一个引用此数据的表，如下所示：

```
CREATE table remote_tweets (
created_at string,
tweet_id string,
text string,
in_reply_to string,
retweeted boolean,
user_id string,
place_id string
)  CLUSTERED BY(user_ID) into 64 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION 's3n://<bucket-name>/tweets'
```

这可能是将 S3 数据拉入本地 Hadoop 集群进行处理的一种非常有效的方式。

### 备注

为了在 S3 位置的 URI 中使用 AWS 凭据，无论参数如何传递，密钥和访问密钥不得包含`/`、`+`、`=`或`\`字符。 如果需要，可以从位于[https://console.aws.amazon.com/iam/](https://console.aws.amazon.com/iam/)的 IAM 控制台生成一组新的凭证。

从理论上讲，您可以将数据留在外部表中，并在需要时引用它，以避免 WAN 数据传输延迟(和成本)，尽管将数据放入本地表并从那里进行未来处理通常是有意义的。 例如，如果表是分区的，那么您可能会发现自己每天都要检索一个新分区。

## 弹性 MapReduce 上的配置单元

在某种程度上，在 Amazon Elastic MapReduce 中使用配置单元与本章中讨论的所有内容一样，只需使用即可。 您可以创建持久集群，登录到主节点，并使用配置单元 CLI 创建表和提交查询。 执行所有这些操作将使用 EC2 实例上的本地存储来存储表数据。

毫不奇怪，EMR 集群上的作业也可以引用其数据存储在 S3(或 DynamoDB)上的表。 同样不足为奇的是，亚马逊对其版本的蜂巢进行了扩展，使这一切变得非常无缝。 在 EMR 作业中，从存储在 S3 中的表中提取数据、对其进行处理、将任何中间数据写入 EMR 本地存储，然后将输出结果写入 S3、DynamoDB 或不断增加的其他 AWS 服务列表中的一个，这非常简单。

前面提到的模式，即每天将新数据添加到表的新分区目录中，在 S3 中已被证明非常有效；它通常是大型和增量增长的数据集的存储位置选择。 使用 EMR 时存在语法差异；与前面提到的 MSCK 命令不同，使用添加到分区目录的新数据更新配置单元表的命令如下：

```
ALTER TABLE <table-name> RECOVER PARTITIONS;
```

有关[http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-hive-additional-features.html](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-hive-additional-features.html)的最新增强功能，请参阅电子病历文档。 此外，请参考更广泛的 EMR 文档。 特别是，与其他 AWS 服务的集成点是一个快速增长的领域。

# 扩展 HiveQL

HiveQL 语言可以通过插件和第三方函数进行扩展。 在配置单元中，有三种类型的函数，其特征在于它们作为输入和生成的行数：

*   **用户定义函数**(**UDF**)：是一次作用于一行的个更简单的函数。
*   **自定义聚合函数**(**UDAF**)：取多行作为输入，生成多行作为输出。 这些是与`GROUP BY`语句(类似于`COUNT()`、`AVG()`、`MIN()`、`MAX()`等)一起使用的聚合函数。
*   **User Defined Table Functions**(**UDTFs**)：接受个多行作为输入，并生成一个由多个行组成的逻辑表，该逻辑表可以在联接表达式中使用。

### 提示

这些 API 仅在 Java 中提供。 对于其他语言，可以使用`TRANSFORM`、`MAP`和`REDUCE`子句通过用户定义的脚本流式传输数据，这些子句充当 Hadoop 流功能的前端。

有两个 API 可用于编写 UDF。 一个简单的 API`org.apache.hadoop.hive.ql.exec.UDF`可用于获取和返回基本可写类型的函数。 在`org.apache.hadoop.hive.ql.udf.generic.GenericUDF`包中提供了更丰富的 API，它提供了对除 WRITABLE 之外的数据类型的支持。 现在我们将说明如何使用`org.apache.hadoop.hive.ql.exec.UDF`来实现类似于我们在[章](05.html "Chapter 5. Iterative Computation with Spark")，*使用 Spark*迭代计算中使用的 String to ID 函数，以将标签映射到 Pig 中的整数。 使用此 API 构建 UDF 只需要扩展 UDF 类和编写`evaluate()`方法，如下所示：

```
public class StringToInt extends UDF {
    public Integer evaluate(Text input) {
        if (input == null)
            return null;

         String str = input.toString();
         return str.hashCode();
    }
}
```

该函数接受`Text`对象作为输入，并使用`hashCode()`方法将其映射为整数值。 此函数的源代码可以在[https://github.com/learninghadoop2/book-examples/blob/master/ch7/udf/com/learninghadoop2/hive/udf/StringToInt.java](https://github.com/learninghadoop2/book-examples/blob/master/ch7/udf/com/learninghadoop2/hive/udf/StringToInt.java)中找到。

### 提示

正如在[第 6 章](06.html "Chapter 6. Data Analysis with Apache Pig")，*使用 Apache Pig 进行数据分析*中所指出的，在生产中应该使用更健壮的散列函数。

我们编译该类并将其存档到 JAR 文件中，如下所示：

```
$ javac -classpath $(hadoop classpath):/opt/cloudera/parcels/CDH/lib/hive/lib/* com/learninghadoop2/hive/udf/StringToInt.java 
$ jar cvf myudfs-hive.jar com/learninghadoop2/hive/udf/StringToInt.class

```

在使用 UDF 之前，必须使用以下命令在配置单元中注册 UDF：

```
ADD JAR myudfs-hive.jar;
CREATE TEMPORARY FUNCTION string_to_int AS 'com.learninghadoop2.hive.udf.StringToInt'; 
```

`ADD JAR`语句将 JAR 文件添加到分布式缓存。 语句的作用是：在配置单元中注册一个实现给定 Java 类的函数。 一旦配置单元会话关闭，该函数将被删除。 从配置单元 0.13 开始，可以使用`CREATE FUNCTION …` 创建永久函数，其定义保存在元存储中。

注册后，可以像任何其他函数一样在查询中使用`StringToInt`。 在下面的示例中，我们首先通过应用`regexp_extract`从 tweet 文本中提取一个标签列表。 然后，我们使用`string_to_int`将每个标记映射到一个数字 ID：

```
SELECT unique_hashtags.hashtag, string_to_int(unique_hashtags.hashtag) AS tag_id FROM
    (
        SELECT regexp_extract(text, 
            '(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)') as hashtag  
        FROM tweets 
        GROUP BY regexp_extract(text, 
        '(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)')
) unique_hashtags GROUP BY unique_hashtags.hashtag, string_to_int(unique_hashtags.hashtag);
```

正如我们在上一章中所做的那样，我们可以使用前面的查询来创建查找表：

```
CREATE TABLE lookuptable (tag string, tag_id bigint);
INSERT OVERWRITE TABLE lookuptable 
SELECT unique_hashtags.hashtag, 
    string_to_int(unique_hashtags.hashtag) as tag_id
FROM 
  (
    SELECT regexp_extract(text, 
        '(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)') AS hashtag  
         FROM tweets 
         GROUP BY regexp_extract(text, 
            '(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)')
   ) unique_hashtags 
GROUP BY unique_hashtags.hashtag, string_to_int(unique_hashtags.hashtag);
```

# 可编程接口

除了`hive`和`beeline`命令行工具之外，还可以通过 JDBC 和 Thrift 编程接口向系统提交 HiveQL 查询。 对 ODBC 的支持捆绑在较旧版本的配置单元中，但从配置单元 0.12 开始，它需要从头开始构建。 有关这一过程的更多信息，请参见[https://cwiki.apache.org/confluence/display/Hive/HiveODBC](https://cwiki.apache.org/confluence/display/Hive/HiveODBC)。

## JDBC

使用JDBC API 编写的配置单元客户端看起来与为其他数据库系统(例如 MySQL)编写的客户端程序完全相同。 以下是使用 JDBC API 的配置单元客户端程序示例。 这个示例的源代码可以在[https://github.com/learninghadoop2/book-examples/blob/master/ch7/clients/com/learninghadoop2/hive/client/HiveJdbcClient.java](https://github.com/learninghadoop2/book-examples/blob/master/ch7/clients/com/learninghadoop2/hive/client/HiveJdbcClient.java)中找到。

```
public class HiveJdbcClient {
     private static String driverName = " org.apache.hive.jdbc.HiveDriver";

     // connection string
     public static String URL = "jdbc:hive2://localhost:10000";

     // Show all tables in the default database
     public static String QUERY = "show tables";

     public static void main(String[] args) throws SQLException {
          try {
               Class.forName (driverName);
          } 
          catch (ClassNotFoundException e) {
               e.printStackTrace();
               System.exit(1);
          }
          Connection con = DriverManager.getConnection (URL);
          Statement stmt = con.createStatement();

          ResultSet resultSet = stmt.executeQuery(QUERY);
          while (resultSet.next()) {
               System.out.println(resultSet.getString(1));
          }
    }
}
```

`URL`部分是描述连接端点的 JDBC URI。 建立远程连接的格式为`jdbc:hive2:<host>:<port>/<database>`。 嵌入式模式下的连接可以通过不指定主机或端口(如`jdbc:hive2://`)来建立。

`hive`和`hive2`是连接到`HiveServer`和`HiveServer2`时要使用的驱动程序。 `QUERY`包含要执行的 HiveQL 查询。

### 提示

配置单元的 JDBC 接口仅公开默认数据库。 为了访问其他数据库，您需要在底层查询中使用`<database>.<table>`符号显式引用它们。

首先，我们加载`HiveServer2`JDBC 驱动程序`org.apache.hive.jdbc.HiveDriver`。

### 提示

使用`org.apache.hadoop.hive.jdbc.HiveDriver`连接到 HiveServer。

然后，与任何其他JDBC 程序一样，我们建立到`URL`的连接并使用它实例化一个`Statement`类。 我们在没有身份验证的情况下执行`QUERY`，并将输出数据集存储到`ResultSet`对象中。 最后，我们扫描`resultSet`并将其内容打印到命令行。

使用以下命令编译并执行该示例：

```
$ javac HiveJdbcClient.java
$ java -cp $(hadoop classpath):/opt/cloudera/parcels/CDH/lib/hive/lib/*:/opt/cloudera/parcels/CDH/lib/hive/lib/hive-jdbc.jar: com.learninghadoop2.hive.client.HiveJdbcClient

```

## 节俭

Thrift提供对配置单元的低级访问，并且与 HiveServer 的 JDBC实现相比具有许多优势。 首先，它允许来自同一客户端的多个连接，并且允许轻松使用 Java 以外的编程语言。 在 HiveServer2 中，这是一个不太常用的选项，但在兼容性方面仍然值得一提。 使用 Java API 实现的示例 Thrift 客户端可以在[https://github.com/learninghadoop2/book-examples/blob/master/ch7/clients/com/learninghadoop2/hive/client/HiveThriftClient.java](https://github.com/learninghadoop2/book-examples/blob/master/ch7/clients/com/learninghadoop2/hive/client/HiveThriftClient.java)中找到。 此客户端可用于连接到 HiveServer，但由于协议差异，客户端不能与 HiveServer2 一起工作。

在本例中，我们定义了一个`getClient()`方法，该方法接受 HiveServer 服务的主机和端口作为输入，并返回`org.apache.hadoop.hive.service.ThriftHive.Client`的实例。

通过首先实例化到 HiveServer 服务的套接字连接`org.apache.thrift.transport.TSocket`，并指定协议`org.apache.thrift.protocol.TBinaryProtocol`来序列化和传输数据，从而获得客户端，如下所示：

```
        TSocket transport = new TSocket(host, port);
        transport.setTimeout(TIMEOUT);
        transport.open();
        TBinaryProtocol protocol = new TBinaryProtocol(transport);
        client = new ThriftHive.Client(protocol);
```

我们从 Main 方法调用`getClient()`，并使用客户端对端口`11111`上本地主机上运行的 HiveServer 实例执行查询，如下所示：

```
     public static void main(String[] args) throws Exception {
          Client client = getClient("localhost", 11111);
          client.execute("show tables");
          List<String> results = client.fetchAll();           
for (String result : results) {
System.out.println(result);           
} 
     }
```

确保HiveServer 在端口`11111`上运行，如果没有，请使用以下命令启动实例：

```
$ sudo hive --service hiveserver -p 11111

```

使用以下命令编译并执行`HiveThriftClient.java`示例：

```
$ javac $(hadoop classpath):/opt/cloudera/parcels/CDH/lib/hive/lib/* com/learninghadoop2/hive/client/HiveThriftClient.java
$ java -cp $(hadoop classpath):/opt/cloudera/parcels/CDH/lib/hive/lib/*: com.learninghadoop2.hive.client.HiveThriftClient

```

# Stinger 计划

从最早的版本开始，HIVE就一直非常成功和强大，尤其是它能够在庞大的数据集上提供类似 SQL 的处理。 但其他技术并没有停滞不前，蜂巢获得了相对缓慢的名声，特别是在大型任务的启动时间较长，以及无法对概念上简单的查询做出快速响应方面。

这些感知到的限制与其说是由于配置单元本身的原因，不如说是因为与实现 SQL 查询的其他方式相比，将 SQL 查询转换为 MapReduce 模型的效率非常低。 特别是在非常大的数据集方面，MapReduce 看到了大量的 I/O(因此花费了大量时间)写出一个 MapReduce 作业的结果，只是为了让另一个作业读取它们。 正如在[第 3 章](03.html "Chapter 3. Processing – MapReduce and Beyond")，*Processing-MapReduce 和 Beyond*中所讨论的，这是 TEZ 设计中的主要驱动因素，它可以将 Hadoop 集群上的作业调度为任务图，而不需要在它们之间进行低效的读写。

以下是对 MapReduce 框架与 TEZ 的查询：

```
SELECT a.country, COUNT(b.place_id) FROM place a JOIN tweets b ON (a. place_id = b.place_id) GROUP BY a.country;
```

下图对比了 MapReduce 框架上前面查询的执行计划与 TEZ：

![Stinger initiative](graphics/5518OS_07_01.jpg)

MapReduce 上的配置单元与 TEZ

在普通 MapReduce 中，为`GROUP BY`和`JOIN`子句创建了两个作业。 第一个作业由一组 MapReduce 任务组成，这些任务从磁盘读取数据以执行分组。 还原器将中间结果写入磁盘，以便同步输出。 第二个作业中的映射器从磁盘读取中间结果以及从表 b 读取数据。然后将组合的数据集传递到还原器，在那里连接共享密钥。 如果我们执行一条`ORDER BY`语句，这将导致第三个作业和更多的 MapReduce 过程。 相同的查询由从磁盘读取数据的一组 Map 任务作为单个作业在 TEZ 上执行。 I/O 分组和连接在减速器之间以流水线方式进行。

除了这些架构限制外，围绕 SQL 语言支持的许多领域也可以提供更高的效率，2013 年初，Stinger 计划启动，明确目标是使蜂窝的速度提高 100 倍以上，并提供更丰富的 SQL 支持。 HIVE 0.13 拥有 Stinger 的三个阶段的所有功能，从而形成了更加完整的 SQL 方言。 此外，除了基于 MapReduce 的 YAINE 实现之外，TEZ 还作为一个执行框架提供，这比之前在 Hadoop1MapReduce 上的实现更高效。

使用 TEZ 作为执行引擎，配置单元不再局限于一系列线性 MapReduce 作业，而是可以构建一个处理图，其中任何给定的步骤都可以(例如)将结果流式传输到多个子步骤。

为了利用 TEZ 框架，有一个新的`hive`变量设置：

```
set hive.execution.engine=tez;
```

这个设置依赖于集群上安装的 tez；它的源代码形式可以从[Cloudera](http://tez.apache.org)获得，也可以在几个发行版中获得，不过在撰写本文时，还没有 http://tez.apache.org。

另一个值是`mr`，它使用经典的 MapReduce 模型(在纱线之上)，因此可以在单个安装中与使用 TEZ 的配置单元的性能进行比较。

# ==同步，由 Elderman 更正==@ELDER_MAN

HIVE 不是唯一提供 SQL-on-Hadoop 功能的产品。 第二个使用最广泛的可能是黑斑羚，它于 2012 年底宣布，并于 2013 年春季发布。 虽然最初是在 Cloudera 内部开发的，但它的源代码会定期推送到开源 Git 存储库([https://github.com/cloudera/impala](https://github.com/cloudera/impala))。

黑斑羚是基于对蜂巢弱点的认识而创造出来的，正是这种认识导致了毒刺计划。

Impala 还从 Google Dremel([http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf))中获得了一些灵感，2009 年发表的一篇论文首次公开描述了这一点。 Dremel 是由 Google 构建的，目的是解决在非常大的数据集上进行非常快速的查询的需求与当时支撑 Hive 的现有 MapReduce 模型固有的高延迟之间的差距。 Dremel 是解决此问题的一种复杂方法，它不是在 MapReduce(如由 Have 实现的)之上构建缓解，而是创建一个访问存储在 HDFS 中的相同数据的新服务。 Dremel 还受益于大量工作，优化了数据的存储格式，使其更易于接受非常快的分析查询。

## 黑斑羚的架构

基本架构有三个主要组件：黑斑羚守护进程、状态存储和客户端。 最近的版本添加了改进服务的附加组件，但我们将重点放在高级体系结构上。

应该在 DataNode 进程管理 HDFS 数据的每个主机上运行 Impala 守护程序(`impalad`)。 请注意，`impalad`并不通过完整的 HDFS 文件系统 API 访问文件系统块；相反，它使用一种称为短路读取的特性来提高数据访问效率。

当客户端提交查询时，它可以向任何正在运行的`impalad`进程提交查询，而这个进程将成为该查询执行的协调器。 Impala 性能的关键方面在于，对于每个查询，它都会生成自定义本机代码，然后将这些代码推送到系统上的所有`impalad`进程并由其执行。 这个高度优化的代码在本地数据上执行查询，然后每个`impalad`将其结果集的子集返回给协调器节点，协调器节点执行最终的数据合并以产生最终结果。 任何使用过目前可用的(通常是商用且昂贵的)**大规模并行处理**(**MPP**)(用于这种类型的共享横向扩展体系结构的术语)数据仓库解决方案的人都应该熟悉这种类型的体系结构。 在集群运行时，状态存储守护进程确保每个`impalad`进程都知道所有其他进程，并提供整体集群健康状况的视图。

## 与蜂窝共存

Impala 作为一种较新的产品，往往具有更多受限的 SQL 数据类型集，并且比配置单元支持更受限的 SQL 方言。 然而，随着每个新版本的发布，它正在扩展这一支持。 请参阅 Impala文档([http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/impala.html](http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/impala.html))，了解当前支持级别的概述。

Impala 支持配置单元使用的元数据存储机制，用于持久存储围绕其表结构和存储的元数据。 这意味着，在具有现有配置单元设置的群集上，应该可以立即使用 Impala，因为它将访问相同的元存储，从而提供对配置单元中可用的相同表的访问。

但需要注意的是，在组合的配置单元和黑斑羚环境中工作时，SQL 方言和数据类型的差异可能会导致意想不到的结果。 有些查询可能在其中一个上有效，但在另一个上无效，它们可能表现出非常不同的性能特征(稍后将对此进行详细介绍)，或者它们实际上可能会给出不同的结果。 最后一点在使用浮点和双精度这样的数据类型时可能会变得明显，这些数据类型在底层系统中只是被区别对待(配置单元是在 Java 上实现的，而 Impala 是用 C++编写的)。

从 1.2 版开始，它支持用 C++和 Java 编写的个 UDF，尽管强烈推荐 C++作为更快的解决方案。 如果您希望在蜂窝和黑斑羚之间共享自定义功能，请记住这一点。

## 一种不同的哲学

当 Impala 第一次发布时，它最大的好处在于它真正实现了通常所说的*思维速度*分析。 可以足够快地返回查询，以至于分析师可以以完全交互的方式探索分析线索，而不必一次等待几分钟才能完成每个查询。 公平地说，大多数采用黑斑羚的人有时会对它的表现感到震惊，特别是与当时的蜂巢发货版本相比。

Impala 的关注点主要停留在这些较短的查询上，这确实对系统造成了一些限制。 黑斑羚往往会占用大量内存，因为它依赖内存中的处理来实现其大部分性能。 如果查询需要将数据集保存在内存中，而不是在执行节点上可用，那么在 2.0 之前的 Impala 版本中，该查询将简单地失败。

将 Stinger 上的工作与 Impala 进行比较，可以说 Impala 更加注重在支持交互式数据分析的更短(也可以说是更常见)的查询中出类拔萃。 许多商业智能工具和服务现在都经过认证，可以直接在 Impala 上运行。 毒刺计划在黑斑羚擅长的领域减少了让蜂巢变得同样快的努力，而是(在不同程度上)改进了蜂巢在所有工作负载下的速度。 Impala 仍在快速发展，Stinger 已经为蜂巢注入了额外的动力，因此明智的做法是同时考虑这两种产品，并确定哪种产品最能满足您的项目和工作流程的性能和功能要求。

还应该记住的是，影响黑斑羚和蜂巢方向的是竞争的商业压力。 Impala 是由 Cloudera 创建的，并且仍然由 Cloudera 驱动，Cloudera 是 Hadoop 发行版中最受欢迎的供应商。 Stinger 计划虽然得到了微软等多家公司的支持(是的，真的！)。 Intel 是由 Hortonworks 领导的，可能是 Hadoop 发行版的第二大供应商。 事实是，如果您使用的是 Hadoop 的 Cloudera 发行版，那么 Have 的一些核心特性可能会出现得较慢，而 Impala 将始终是最新的。 相反，如果你使用另一个发行版，你可能会得到最新的蜂巢版本，但它可能有一个更旧的 Impala，或者就像目前的情况一样，你可能需要自己下载并安装它。

前面提到的拼花和 ORC 文件格式也出现了类似的情况。 镶木地板是黑斑羚的首选，由 Cloudera 领导的一群公司开发，而 ORC 是蜂巢的首选，是 Hortonworks 的拥护者。

不幸的是，现实情况是，Cloudera 发行版对 Parquet 的支持通常很快，但在 Hortonworks 发行版中就不是那么快了，在 Hortonworks 发行版中，ORC 文件格式是首选格式。

这些主题有点令人担忧，因为虽然这一领域的竞争是一件好事，而且可以说，Impala 的宣布帮助激发了蜂巢社区的活力，但与过去不同的是，您选择的分发版本可能会对完全支持的工具和文件格式产生更大的影响，这一风险更大。 希望目前的情况只是我们在所有这些新的和改进的技术的开发周期中所处位置的产物，但是一定要根据您的 SQL-on-Hadoop 需求仔细考虑您的发行版选择。

## Drill、Tajo 和 Beyond

您还应该考虑到 Hadoop 上的 SQL 不再只指配置单元或黑斑羚。 ApacheDrill([DREMEL](http://drill.apache.org))是 http://drill.apache.org 最先描述的 DREMEL模型的一个更完整的实现。 尽管 Impala 跨 HDFS 数据实现了 Dremel 体系结构，但 Drill 希望跨多个数据源提供类似的功能。 它还处于早期阶段，但如果你的需求比蜂巢或黑斑羚提供的更广泛，它可能是值得考虑的。

TAJO([Hadoop](http://tajo.apache.org))是另一个寻求成为 http://tajo.apache.org 数据上的完整数据仓库系统的Apache 项目。 通过与 Impala 类似的体系结构，它提供了一个更丰富的系统，其中包含多个优化器和 ETL 工具等组件，这些组件在传统数据仓库中很常见，但在 Hadoop 世界中很少捆绑在一起。 它的用户基础要小得多，但已经被某些公司成功地使用了很长时间，如果您需要更全面的数据仓库解决方案，可能值得考虑。

其他产品也在这个领域涌现，做一些研究是个好主意。 蜂巢和黑斑羚都是很棒的工具，但如果你发现它们不能满足你的需求，那就四处看看--其他的可能会。

# 摘要

在早期，Hadoop 有时被错误地视为最新的关系数据库杀手。 随着时间的推移，越来越明显的是，更明智的方法是将其视为 RDBMS 技术的补充，事实上，RDBMS 社区已经开发了 SQL 等工具，这些工具在 Hadoop 世界中也很有价值。

HiveQL 是 SQL 在 Hadoop 上的实现，是本章的重点。 关于 HiveQL 及其实现，我们讨论了以下主题：

*   HiveQL 如何在 HDFS 中存储的数据之上提供逻辑模型，这与预先强制实施表结构的关系数据库不同
*   HiveQL 如何支持许多标准 SQL 数据类型和命令，包括连接和视图
*   HiveQL 提供的类似 ETL 的特性，包括将数据导入到表中的能力，以及通过分区和类似机制优化表结构的能力
*   HiveQL 如何提供使用用户定义代码扩展其核心运算符集合的能力，以及这与 Pig UDF 机制有何不同
*   蜂窝开发的近期历史，例如 Stinger 计划，见证了蜂窝过渡到使用 TEZ 的更新实现
*   围绕 HiveQL 的更广泛的生态系统，现在包括 Impala、Tajo 和 Drill 等产品，以及这些产品如何专注于突出的特定领域

在 Pig and Have 中，我们引入了处理 MapReduce 数据的替代模型，但到目前为止，我们还没有研究另一个问题：需要什么方法和工具才能真正允许 Hadoop 中收集的海量数据集随着时间的推移保持有用和可管理？ 在下一章中，我们将略微提升抽象层次，看看如何管理这一巨大数据资产的生命周期。**