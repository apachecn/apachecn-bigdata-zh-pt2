# 安全考虑

对于大多数开源软件来说，安全是生产发布前需要解决的一个关键领域。 作为 Hadoop 数据的领先类 SQL 界面，配置单元必须确保数据得到安全保护和访问。 因此，蜂巢的安全一直被认为是生态系统不可或缺的重要组成部分。 早期版本的配置单元主要依靠 HDFS 来保证安全性。 蜂巢的安全性在`hiveserver2`发布后逐渐成熟。

本章将讨论以下方面的配置单元安全：

*   验证
*   （被）授权 / （被）批准
*   掩码和加密

# 验证

身份验证是通过获取用户凭据来验证用户身份的过程。 配置单元从`hiveserver2`开始提供身份验证。 在旧版本的配置单元中，`hiveserver1`不支持储蓄客户端的 Kerberos 身份验证。 因此，如果我们可以通过网络访问主机/端口，我们就可以访问服务器。 相反，我们可以利用支持 Kerberos 的`metastore`服务器进行身份验证。 在本节中，我们将简要介绍`metastore`服务器和`hiveserver2`服务器中的身份验证配置。

**Kerberos** is a network authentication protocol developed by MIT as part of Project Athena. It uses time-sensitive tickets that are generated using symmetric key cryptography to securely authenticate a user in an unsecured network environment. Kerberos, in Greek mythology, was the three-headed dog that guarded the gates of Hades. The three-headed part refers to the three parties involved in the Kerberos authentication process: client, server, and **Key Distribution Center** (**KDC**). All clients and servers registered to KDC are known as a realm, which is typically the domain's DNS name in all caps. For more information, please refer to the MIT Kerberos website: [http://web.mit.edu/kerberos/](http://web.mit.edu/kerberos/).

# 元存储区身份验证

要强制客户端使用 Kerberos 向`metastore`服务器进行身份验证，我们可以在`hive-site.xml`文件中设置以下三个属性，然后重新启动`metastore`服务器以使其正常工作：

1.  启用**简单身份验证和安全层**(**SASL**)框架以实施客户端 Kerberos 身份验证，如下所示：

```sql
      <property>
      <name>hive.metastore.sasl.enabled</name>
      <value>true</value>
      <description>If true, the metastore thrift interface will be 
      secured with SASL framework. Clients must authenticate with 
      Kerberos.</description>
      </property>
```

2.  指定生成的 Kerberos 密钥表。 如果要将文件保存在其他位置，请覆盖以下示例。 确保密钥表文件权限掩码设置为只读权限(600)，以避免意外更改或删除。 它还应该属于用于运行`metastore`服务器的同一帐户(默认情况下为配置单元)：

```sql
      <property>
      <name>hive.metastore.kerberos.keytab.file</name>
      <value>/etc/hive/conf/hive.keytab</value>
      <description>The sample path to the Kerberos Keytab file 
      containing the metastore thrift server's service principal.
      </description>
      </property>
```

3.  指定 Kerberos 主体模式字符串。 特殊字符串将自动替换为正确的主机名。 `YOUR-REALM.COM`值应替换为实际的域名：

```sql
      <property>
      <name>hive.metastore.kerberos.principal</name>
      <value>hive/_HOST@YOUR-REALM.COM</value>
      <description>The service principal for metastore server.
      </description>
      </property>
```

# Hiveserver2 身份验证

`hiveserver2`支持多种身份验证模式，如 Kerberos、LDAP、PAM 和自定义代码。 要将`hiveserver2`配置为使用这些身份验证模式之一，我们可以在`hive_site.xml`中设置正确的属性，如下所示，然后重新启动`hiveserver2`服务以使其工作：

*   `NONE`：默认设置中为无身份验证。 `None`这里的意思是允许使用以下设置进行匿名访问：

```sql
      <property>
      <name>hive.server2.authentication</name>
      <value>NONE</value>
      </property>
```

*   `KERBEROS`：如果使用 Kerberos 身份验证，则用于在储蓄客户端与`hiveserver2`和`hiveserver2`之间进行身份验证，并保护 HDFS。 要为`hiveserver2`启用 Kerberos 身份验证，我们可以通过指定`keytab`路径并在`YOUR-REALM.COM`中指定实际域名来设置以下属性：

```sql
      <property>
      <name>hive.server2.authentication</name>
      <value>KERBEROS</value>
      </property>

      <property>
      <name>hive.server2.authentication.kerberos.keytab</name>
      <value>/etc/hive/conf/hive.keytab</value>
      </property>

      <property>
      <name>hive.server2.authentication.kerberos.principal</name>
      <value>hive/_HOST@YOUR-REALM.COM</value>
      </property>
```

Once Kerberos is enabled, the JDBC client (such as Beeline) must include the principal parameter in the JDBC connection string, such as `jdbc:hive2://hiveserver2host:10000/default;principal=hive/_HOST@REALM`. For more examples of the supported connection string syntax, refer to [https://community.hortonworks.com/articles/4103/hiveserver2-jdbc-connection-url-examples.html](https://community.hortonworks.com/articles/4103/hiveserver2-jdbc-connection-url-examples.html).

*   `LDAP`：要将`hiveserver2`配置为使用`LDAP`支持的用户和密码验证(请参阅[https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol](https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol))，我们可以设置以下属性：

```sql
      <property>
      <name>hive.server2.authentication</name>
      <value>LDAP</value>
      </property>

      <property>
      <name>hive.server2.authentication.ldap.url</name>
      <value>LDAP_URL, such as ldap://ldaphost@company.com</value>
      </property>

      <property>
      <name>hive.server2.authentication.ldap.Domain</name>
      <value>Domain Name</value>
      </property>
```

要使用`OpenLDAP`([https://en.wikipedia.org/wiki/OpenLDAP](https://en.wikipedia.org/wiki/OpenLDAP))对其进行配置，我们可以添加`baseDN`设置，而不是前面的`Domain`属性，如下所示：

```sql
<property>
<name>hive.server2.authentication.ldap.baseDN</name>
<value>LDAP_BaseDN, such as ou=people,dc=packtpub,dc=com</value>
</property>
```

*   `CUSTOM`：这表示`hiveserver2`的自定义身份验证提供程序。 要启用它，请按如下方式配置设置：

```sql
      <property>
      <name>hive.server2.authentication</name>
      <value>CUSTOM</value>
      </property>

      <property>
      <name>hive.server2.custom.authentication.class</name>
      <value>pluggable-auth-class-name</value>
      <description>Customized authentication class name, such as 
      com.packtpub.hive.essentials.hiveudf.customAuthenticator
      </description>
      </property>
```

Pluggable authentication with a customized class did not work until the bug (see [https://issues.apache.org/jira/browse/HIVE-4778](https://issues.apache.org/jira/browse/HIVE-4778)) was fixed in Hive v0.13.0.

以下是实现`org.apache.hive.service.auth.PasswdAuthenticationProvider`接口的自定义类的示例。 被覆盖的`Authenticate(...)`方法具有如何验证用户名和密码的核心逻辑。 确保将编译的`JAR`文件复制到`$HIVE_HOME/lib/`，以便前面的设置可以工作：

```sql
*// customAuthenticator.java*
package com.packtpub.hive.essentials.hiveudf;

import java.util.Hashtable;
import javax.security.sasl.AuthenticationException;
import org.apache.hive.service.auth.PasswdAuthenticationProvider;

/*
 * *The customized class for hiveserver2 authentication*
 */

public class customAuthenticator implements PasswdAuthenticationProvider {

  Hashtable<String, String> authHashTable = null;

  public customAuthenticator () {
       authHashTable = new Hashtable<String, String>();
       authHashTable.put("user1", "passwd1");
       authHashTable.put("user2", "passwd2");
  }

  @Override
  public void Authenticate(String user, String password)
            throws AuthenticationException {

    String storedPasswd = authHashTable.get(user);

    if (storedPasswd != null && storedPasswd.equals(password))
         return;

    throw new AuthenticationException(
    "customAuthenticator Exception: Invalid user");
  }
}
```

*   `PAM`：从配置单元 v0.13.0 开始，配置单元支持**PAM**(**可插拔身份验证模块**)身份验证，这提供了将现有身份验证机制插入到配置单元的好处。 配置以下设置以启用 PAM 身份验证。 有关如何安装 PAM 的更多信息，请参阅位于[https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2#SettingUpHiveServer2-PluggableAuthenticationModules(PAM)](https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2#SettingUpHiveServer2-PluggableAuthenticationModules(PAM))的配置单元维基中的*设置**hiveserver2 和*文章。

```sql
      <property>
      <name>hive.server2.authentication</name>
      <value>PAM</value>
      </property>

      <property>
      <name>hive.server2.authentication.pam.services</name>
      <value>pluggable-auth-class-name</value>
      <description> Set this to a list of comma-separated PAM servicesthat 
      will be used. Note that a file with the same name as the PAMservice 
      must exist in /etc/pam.d.</description>
      </property>
```

# （被）授权 / （被）批准

授权用于验证用户是否有权执行特定操作，如创建、读取或写入数据或元数据。 HIVE 提供三种授权模式：传统模式、基于存储的模式和基于 SQL 标准的模式。

# 传统模式

这是配置单元中的默认授权模式，通过 HQL 语句提供列级和行级授权。 但是，它不是完全安全的授权模式，并且有几个限制。 它主要用于防止好用户意外做坏事，而不是防止恶意用户操作。 为了启用传统授权模式，我们需要在`hive-site.xml`*中设置以下属性：*

```sql
<property>
<name>hive.security.authorization.enabled</name>
<value>true</value>
<description>enables or disable the hive client authorization
</description>
</property>

<property>
<name>hive.security.authorization.createtable.owner.grants</name>
<value>ALL</value>
<description>the privileges automatically granted to the owner whenever a table gets created. An example like "select, drop" will grant select and drop privilege to the owner of the table.
</description>
</property>
```

由于这不是一种安全的授权模式，我们在此不再详细讨论。 有关在传统授权模式下对 HQL 的更多支持，请参阅位于[https://cwiki.apache.org/confluence/display/Hive/Hive+Default+Authorization+-+Legacy+Mode](https://cwiki.apache.org/confluence/display/Hive/Hive+Default+Authorization+-+Legacy+Mode)的配置单元维基。

# 基于存储的模式

基于存储的授权模式(从配置单元 v0.10.0 开始)依赖于存储层 HDFS 提供的授权，存储层 HDFS 同时提供 POSIX 和 ACL 权限(从配置单元 v0.14.0 开始提供；请参阅[https://issues.apache.org/jira/browse/HIVE-7583](https://issues.apache.org/jira/browse/HIVE-7583))。 在`metastore`服务器中启用了基于存储的授权；它具有跨生态系统中其他应用程序的元数据的单一一致视图。 此模式根据 HDFS 中相应文件目录上的 POSIX 权限检查用户权限。 除了 POSIX 权限模型，HDFS 还提供了访问控制列表，这些列表在位于[http://hadoop.apache.org/docs/r2.4.0/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html#ACLs_Access_Control_Lists](http://hadoop.apache.org/docs/r2.4.0/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html#ACLs_Access_Control_Lists)的 HDFS 上的 ACL 中描述。

考虑到其实现，基于存储的授权模式只提供数据库、表和分区级别的授权，而不是列和行级别的授权。 由于依赖于 HDFS 权限，它缺乏通过 HQL 语句管理授权的灵活性。 要启用基于存储的授权模式，我们可以在`hive-site.xml`文件中设置以下属性：

```sql
<property>
<name>hive.security.authorization.enabled</name>
<value>true</value>
<description>enable or disable the hive client authorization
</description>
</property>

</property>
<name>hive.metastore.pre.event.listeners</name>
<value>org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener</value>
<description>This turns on metastore-side security.</description>
</property>

<property> 
<name>hive.security.authorization.manager</name> <value>org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider</value> 
<description>The class name of the Hive client authorization manager.</description> 
</property> 

<property> 
<name>hive.security.metastore.authorization.manager</name> <value>org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator
</value> 
<description>authenticator manager class name to be used in the metastore for authentication.</description> 
</property> 

<property>
<name>hive.security.metastore.authorization.auth.reads</name>
<value>true</value>
<description>If this is true, metastore authorizer authorizes read actions on database, table</description>
</property>
```

With effect from Hive v0.14.0, storage-based authorization also authorizes read privileges on databases and tables by default through the `hive.security.metastore.authorization.auth.reads` property. For more information, please refer to [https://issues.apache.org/jira/browse/HIVE-8221](https://issues.apache.org/jira/browse/HIVE-8221).

# 基于 SQL 标准的模式

对于列和行级别的细粒度访问控制，我们可以使用从 Hiev0.13.0 开始提供的基于 SQL 标准的模式。 它类似于使用`GRANT`和`REVOKE`语句通过`hiveserver2`配置控制访问的关系数据库授权。 但是，诸如配置单元或 HDFS 命令之类的工具不能通过`hiveserver2`访问数据，因此基于 SQL 标准的模式无法授权它们访问。

因此，建议您结合使用基于存储的模式和基于 SQL 标准的模式，对从各种工具连接的用户进行授权。 要启用基于 SQL 标准的模式授权，我们可以在`hive-site.xml`文件中设置以下属性：

```sql
<property>
<name>hive.security.authorization.enabled</name>
<value>true</value>
<description>enable or disable the hive client authorization </description>
</property>

<property> 
<name>hive.server2.enable.doAs</name> 
<value>false</value> 
<description>Allows Hive queries to be run by the user who submits the query rather than the hive user. Need to turn if off for this SQL standard-base mode</description> 
</property> 

<property> 
<name>hive.users.in.admin.role</name> 
<value>dayongd,administrator</value> 
<description>Comma-separated list of users assigned to the ADMIN role.</description> 
</property> 

<property> 
<name>hive.security.authorization.manager</name>  <value>org.apache.hadoop.hive.ql.security.authorization.plugin.sql</value> </property> 

<property> 
<name>hive.security.authenticator.manager</name> <value>org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory</value> 
</property> 

<property>
<name>hive.security.metastore.authorization.manager</name>
<value>org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider,org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly</value>
<description>It takes a comma separated list, so we can add MetaStoreAuthzAPIAuthorizerEmbedOnly along with StorageBasedAuthorization parameter,if we want to enable that as well</description>
</property>
```

此外，在重启`hiveserver2`之前，我们需要在`hiveserver2-site.xml`中设置以下配置，以使基于 SQL 标准的授权生效：

```sql
<configuration>

<property>
<name>hive.security.authorization.enabled</name>
<value>true</value>
<description></description>
</property>

<property>
<name>hive.security.authorization.manager</name
<value>org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory</value>
</property>

<property>
<name>hive.security.authenticator.manager</name>
<value>org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator</value>
</property>

<property>
<name>hive.metastore.uris</name>
<value>thrift://localhost:9085</value>
<description>Use 9083 for hive1 and 9085 for hive2</description>
</property> 

</configuration>
```

Before restarting `hiveserver2` to enable the preceding setting, do not forget to grant admin roles to the users defined in `hive.users.in.admin.role` using `GRANT admin TO USER <user_name>`.

使用基于 SQL 标准的模式授权，我们可以在两个级别上管理权限：角色或对象。

授予或撤销角色级别的授权的语法如下：

*   `GRANT <ROLE_NAME> TO <PRINCIPLES> [ WITH ADMIN OPTION]`
*   `REVOKE [ADMIN OPTION FOR] <ROLE_NAME> FROM <PRINCIPLES>`

这些参数的用法如下：

*   `<ROLE_NAME>`：这是逗号分隔的角色名称
*   `<PRINCIPLES>`：这是用户或角色
*   `WITH ADMIN OPTION`：这是可选的。 一旦指定，它将确保用户获得将该角色授予其他用户/角色的权限

另一方面，在对象级别授予或撤销授权的语法如下：

*   `GRANT <PRIVILEGE> ON <OBJECT> TO <PRINCIPLES>`
*   `REVOKE <PRIVILEGE> ON <OBJECT> FROM <PRINCIPLES>`

这里使用的参数如下：

*   `<PRIVILEGE>`：可以是`INSERT`、`SELECT`、`UPDATE`、`DELETE`或`ALL`
*   `<PRINCIPLES>`：这可以是用户或角色
*   `<OBJECT>`：这是一个表或视图

有关管理基于 sql 标准授权的 hql 语句的更多示例，请参考位于[https://cwiki.apache.org/confluence/display/Hive/SQL+Standard+Based+Hive+Authorization#SQLStandardBasedHiveAuthorization-Configuration](https://cwiki.apache.org/confluence/display/Hive/SQL+Standard+Based+Hive+Authorization#SQLStandardBasedHiveAuthorization-Configuration)的配置单元维基。

Apache Sentry is a highly modular system for providing centralized, fine-grained, role-based authorization to both data and metadata stored on an Apache Hadoop cluster. It can be integrated with Hive to deliver advanced authorization controls. For more information about Sentry, please refer to [https://sentry.apache.org/](https://sentry.apache.org/). Sentry is usually distributed in the Cloudera CDH package. Another similar project is Apache Ranger ([https://ranger.apache.org/](https://ranger.apache.org/)), which is usually distributed in the Hortonworks HDP package.

# 掩码和加密

对于敏感和受法律保护的数据，如**个人身份信息**(**PII**)或**个人机密信息**(**PCI**)，需要在文件系统中以加密或屏蔽格式存储数据。 从蜂窝 v0.13.0 开始，其数据安全功能在数据散列、数据掩蔽、安全和数据加解密功能等领域已经成熟。

# 数据散列函数

在支持屏蔽数据之前，自 Hieve v1.3.0 以来，内置散列函数一直是一种选择。 散列函数读取输入字符串并生成固定大小的字母数字输出字符串。 由于输出通常是到输入字符串的唯一映射(冲突的可能性很小)，因此散列值经常用于保护列，列是连接或比较数据的唯一标识符。 内置函数，如`md5(...)`、`sha1(...)`和`sha2(...)`，可用于 HQL 中的数据散列：

```sql
> SELECT 
> name, 
> md5(name) as md5_name, -- 128 bit
> sha1(name) as sha1_name, -- 160 bit
> sha2(name, 256) as sha2_name -- 256 bit
> FROM employee;
+---------+----------------------------------+
| name    | md5_name                         |
+---------+----------------------------------+
| Michael | 3e06fa3927cbdf4e9d93ba4541acce86 |
| Will    | 2b80f09163f60ce1774b438e605eb1f9 |
| Shelley | e47e592945f28b3c3891ee9d27ec6b61 |
| Lucy    | 80eb0e612760f756547b660c4c71ba7d |
+---------+----------------------------------+
+------------------------------------------+
| sha1_name                                |
+------------------------------------------+
| f8c38b2167c0ab6d7c720e47c2139428d77d8b6a |
| 3e3e5802bd4cad8e29e144b515307d8204a3202a |
| 2d4cab849437156354d24c9564958e6581711d08 |
| c5c8f32bdf9998e0f692231f4f969085c8dc225b |
+------------------------------------------+
+------------------------------------------------------------------+
| sha2_name                                                        |
+------------------------------------------------------------------+
| f089eaef57aba315bc0e1455985c0c8e40c247f073ce1f4c5a1f8ffde8773176 |
| 6cef4ccc1019d6cee6b9cad39d49cabf808ba2e0665d5832b70c44c09c2dfae0 |
| 1e8b342dde7c90cfbc9634c777b6b59388b6a4bd14274adffbfaeed4b329b26e |
| a3fa95a3b95d421c316f1a9b12c88edcc47896705976764d2652425de98f0c4f |
+------------------------------------------------------------------+
4 rows selected (0.344 seconds)
```

# 数据屏蔽功能

从配置单元 v2.1.0 开始，数据掩码函数作为内置 UDF 在 SQL 中可用。 对于信用卡号码、银行账号和密码等用户敏感数据，经常需要屏蔽数据。 与散列函数不同的是，SQL 中的 MASK 函数可以指定对部分数据进行屏蔽，这使得您在想要保持部分数据不被屏蔽以便更好地理解时更加灵活。 以下是使用 HQL 中的各种掩码函数的示例：

```sql
> SELECT
 -- big letter to U, small letter to l, number to #
> mask("Card-0123-4567-8910", "U", "l", "#") as m0,
 -- mask first n (4) values where X|x for big/small letter, n for number
> mask_first_n("Card-0123-4567-8910", 4) as m1,
 -- mask last n (4) values
> mask_last_n("Card-0123-4567-8910", 4) as m2,
 -- mask everthing except first n(4) values
> mask_show_first_n("Card-0123-4567-8910", 4) as m3,
 -- mask everthing except last n(4) values
> mask_show_last_n("Card-0123-4567-8910", 4) as m4,
 -- return a hash value - sha 256 hex
> mask_hash('Card-0123-4567-8910') as m5
> ;
+-----------------------+-----------------------+------------------------+
| m0                    | m1                    | m2                     |
+-----------------------+-----------------------+------------------------+
| Ulll-####-####-####   | Xxxx-0123-4567-8910   | Card-0123-4567-nnnn    |
+-----------------------+-----------------------+------------------------+
+-------------------+-------------------+--------------------------------+
| m3                | m4                | m5                             |
+-------------------+-------------------+--------------------------------+
|Card-nnnn-nnnn-nnnn|Xxxx-nnnn-nnnn-8910|f0679e470f380ce5183ba403ec0e7e64|
+-------------------+-------------------+--------------------------------+
1 row selected (0.146 seconds)
```

# 数据加密功能

从 hive v1.3.0 开始，提供了`aes_encrypt(input string/binary, key string/binary)`和`aes_decrypt(input binary, key string/binary)`自定义函数来支持使用 AAES(高级加密标准：E[http://en.wikipedia.org/wiki/Advanced_Encryption_Standard](http://en.wikipedia.org/wiki/Advanced_Encryption_Standard))算法进行数据加密和解密，该算法是由比利时密码学家琼·达门和文森特·里曼开发的一种对称的 128 位块数据加密技术。

以下是使用这些函数的示例：

```sql
-- 1st para. is value to encryped/decryped
-- 2nd para. is 128 bit (16 Byte) keys
> SELECT
> name,
> aes_encrypt(name,'1234567890123456') as encrypted,
> aes_decrypt(
> aes_encrypt(name,'1234567890123456'),
> '1234567890123456') as decrypted
> FROM employee;
+---------+-------------------------+-----------+
| name    | encrypted               | decrypted |
+---------+-------------------------+-----------+
| Michael | ��.b��#����-��I    | Micheal   |
| Will    | "�""��r {cgR�%���    | Will      |
| Shelley | ��W@�Dm�[-�?�        | Shelley   |
| Lucy    | ��/i���x���L�q~     | Lucy      |
+---------+------------------------+------------+
4 rows selected (0.24 seconds)
```

# 其他方法

如前所述，我们可以使用 Apache Ranger 或 Sentry 进行列级访问和控制，以实现更精细的安全性。 此外，还有补丁可用于直接在表创建语句上指定列级编码，例如 hive6329c([https://issues.apache.org/jira/browse/HIVE-6329](https://issues.apache.org/jira/browse/HIVE-6329))和 hive7934([https://issues.apache.org/jira/browse/HIVE-7934](https://issues.apache.org/jira/browse/HIVE-7934))。 在存储级别，配置单元还可以利用 HDFS 加密技术([https://issues.apache.org/jira/browse/HDFS-6134](https://issues.apache.org/jira/browse/HDFS-6134))，它提供对 HDFS 上数据的透明加密和解密。 如果我们想在 HDFS 中加密整个数据集，它将满足我们的要求。

# 简略的 / 概括的 / 简易判罪的 / 简易的

在本章中，我们介绍了配置单元安全领域的身份验证、授权、掩码和加密。 我们介绍了`metastore`服务器和`hiveserver2`服务器中的身份验证。 然后，我们讨论了默认模式授权、基于存储的模式授权和基于 SQL 标准的模式授权。 在本章的最后，我们讨论了在配置单元中应用数据掩码和安全性的各种方法。 读完本章后，您应该能够使用不同的身份验证、授权和数据掩码或安全方法来解决安全问题。

在下一章中，我们将讨论如何将蜂窝与大数据生态系统中的其他工具配合使用。