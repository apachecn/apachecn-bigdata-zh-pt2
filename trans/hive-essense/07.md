# 性能注意事项

虽然蜂巢是为处理大数据而建造的，但我们仍然不能忽视性能的重要性。 大多数情况下，更好的查询可以依靠智能查询优化器来查找最佳执行策略以及默认设置和最佳实践。 但是，有经验的用户应该更多地了解性能调优的理论和实践，特别是在处理对性能敏感的项目或环境时。

在本章中，我们将开始使用 HQL 中提供的实用程序来查找导致性能低下的潜在问题。 然后，我们介绍在设计、文件格式、压缩、存储、查询和作业方面的性能注意事项的最佳做法。 在本章中，我们将介绍以下主题：

*   性能实用程序
*   设计优化
*   数据优化
*   作业优化

# 性能实用程序

HQL 提供`EXPLAIN`和`ANALYZE`语句，可用作检查和识别查询性能的实用程序。 此外，蜂窝日志应包含足够详细的信息，用于性能调查和故障排除。

# 解释语句

配置单元提供`EXPLAIN`语句以返回查询执行计划，而不运行查询。 如果我们担心查询的性能，我们可以使用它来分析查询。 `EXPLAIN`语句帮助我们查看两个或多个出于相同目的的查询之间的差异。 它的语法如下：

```sql
EXPLAIN [FORMATTED|EXTENDED|DEPENDENCY|AUTHORIZATION] hql_query
```

可以使用以下关键字：

*   `FORMATTED`：这提供了查询计划的格式化 JSON 版本。
*   `EXTENDED`：这为计划中的操作员提供了附加信息，如文件路径名。
*   `DEPENDENCY`：它提供 JSON 格式的输出，其中包含查询所依赖的表和分区的列表。 从配置单元 v0.10.0 开始提供
*   `AUTHORIZATION`：列出需要授权的所有实体，包括运行查询的输入和输出，以及授权失败(如果有)。 它从配置单元 v0.14.0 开始提供。

典型的查询计划包含以下三个部分。 稍后我们还将查看一个示例：

*   **抽象语法树**(**AST**)：HIVE 使用名为 ANTLR 的解析器生成器(参见[HQL](http://www.antlr.org/))自动生成 http://www.antlr.org/的树语法
*   **阶段依赖项**：列出用于运行查询的所有依赖项和阶段数
*   **阶段计划**：它包含用于运行作业的重要信息，如操作员和排序顺序

以下是典型查询计划的外观。 从下面的示例中，我们可以看到`AST`部分显示为 Map/Reduce 运算符树。 在`STAGE DEPENDENCIES`部分中，`Stage-0`和`Stage-1`都是独立的根阶段。 在`STAGE PLANS`部分中，`Stage-1`有一个 MAP 和 REDUE，它们分别是`Map Operator Tree`和`Reduce Operator Tree`所指的。 在每个`Map/Reduce Operator Tree`部分中，列出了与查询关键字相对应的所有运算符，以及表达式和聚合。 `Stage-0`阶段没有映射和还原。 它只是一个`Fetch`操作：

```sql
> EXPLAIN SELECT gender_age.gender, count(*) 
> FROM employee_partitioned WHERE year=2018 
> GROUP BY gender_age.gender LIMIT 2;
```

```sql

+----------------------------------------------------------------------+
| Explain                                                              |
+----------------------------------------------------------------------+
| STAGE DEPENDENCIES:                                                  |
| Stage-1 is a root stage                                              |
| Stage-0 depends on stages: Stage-1                                   |
|                                                                      |
| STAGE PLANS:                                                         |
| Stage: Stage-1                                                       |
| Map Reduce                                                           |
| Map Operator Tree:                                                   |
| TableScan                                                            |
| alias: employee_partitioned                                          |
| Pruned Column Paths: gender_age.gender                               |
| Statistics:                                                          |
| Num rows: 4 Data size: 223 Basic stats: COMPLETE Column stats: NONE  |
| Select Operator                                                      |
| expressions: gender_age.gender (type: string)                        |
| outputColumnNames: _col0                                             |
| Statistics:                                                          |
| Num rows: 4 Data size: 223 Basic stats: COMPLETE Column stats: NONE  |
| Group By Operator                                                    |
| aggregations: count()                                                |
| keys: _col0 (type: string)                                           |
| mode: hash                                                           |
| outputColumnNames: _col0, _col1                                      |
| Statistics:                                                          |
| Num rows: 4 Data size: 223 Basic stats: COMPLETE Column stats: NONE  |
| Reduce Output Operator                                               |
| key expressions: _col0 (type: string)                                |
| sort order: +                                                        |
| Map-reduce partition columns: _col0 (type: string)                   |
| Statistics:                                                          |
| Num rows: 4 Data size: 223 Basic stats: COMPLETE Column stats: NONE  |
| TopN Hash Memory Usage: 0.1                                          |
| value expressions: _col1 (type: bigint)                              |
| Reduce Operator Tree:                                                |
| Group By Operator                                                    |
| aggregations: count(VALUE._col0)                                     |
| keys: KEY._col0 (type: string)                                       |
| mode: mergepartial                                                   |
| outputColumnNames: _col0, _col1                                      |
| Statistics:                                                          |
| Num rows: 2 Data size: 111 Basic stats: COMPLETE Column stats: NONE  |
| Limit                                                                |
| Number of rows: 2                                                    |
| Statistics:                                                          |
| Num rows: 2 Data size: 110 Basic stats: COMPLETE Column stats: NONE  |
| File Output Operator                                                 |
| compressed: false                                                    |
| Statistics:                                                          |
| Num rows: 2 Data size: 110 Basic stats: COMPLETE Column stats: NONE  |
| table:                                                               |
| input format:                                                        |
| org.apache.hadoop.mapred.SequenceFileInputFormat                     |
| output format:                                                       |
| org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat            |
| serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe            |
|                                                                      |
| Stage: Stage-0                                                       |
| Fetch Operator                                                       |
| limit: 2                                                             |
| Processor Tree:                                                      |
| ListSink                                                             |
+----------------------------------------------------------------------+
53 rows selected (0.232 seconds)
```

Ambari 配置单元视图和色调配置单元编辑器在运行查询时都有内置的可视化查询解释。 Ambari 配置单元视图显示前面的查询，如下所示：

![](assets/82e06eae-1a81-4668-a483-65024d11127c.png)

Ambari Hive view visual explaination

# 分析语句

配置单元统计信息是描述更多详细信息的数据集合，例如数据库中对象的行数、文件数和原始数据大小。 统计数据是数据的元数据，收集并存储在`metastore`数据库中。 HIVE 支持表、分区和列级别的统计信息。 这些统计数据用作基于配置单元**成本优化器**(**CBO**)的输入，该优化器用于根据完成查询所需的系统资源选择成本最低的查询计划。 统计信息在配置单元 v3.2.0 到 JIRA HIVE-11160([https://issues.apache.org/jira/browse/HIVE-11160](https://issues.apache.org/jira/browse/HIVE-11160))中部分自动收集，或通过表、分区和列的`ANALYZE`语句手动收集，如下例所示：

1.  收集现有表的统计信息。如果指定了`NOSCAN`选项，该命令将忽略文件扫描而只收集文件数量及其大小，从而运行速度更快：

```sql
      > ANALYZE TABLE employee COMPUTE STATISTICS;
      No rows affected (27.979 seconds)

 > ANALYZE TABLE employee COMPUTE STATISTICS NOSCAN;
      No rows affected (25.979 seconds)
```

2.  收集特定或所有现有分区的统计信息：

```sql
      -- Applies for specific partition
      > ANALYZE TABLE employee_partitioned 
      > PARTITION(year=2018, month=12) COMPUTE STATISTICS;
      No rows affected (45.054 seconds)

-- Applies for all partitions
     > ANALYZE TABLE employee_partitioned 
 > PARTITION(year, month) COMPUTE STATISTICS;
      No rows affected (45.054 seconds)
```

3.  收集现有表的列的统计信息：

```sql
      > ANALYZE TABLE employee_id COMPUTE STATISTICS FOR COLUMNS 
 employee_id; 
 No rows affected (41.074 seconds)
```

We can enable automatic gathering of statistics by specifying `SET hive.stats.autogather=true`. For new tables or partitions that are populated through the `INSERT OVERWRITE/INTO` statement (rather than the `LOAD` statement), statistics are automatically collected in the `metastore`. 

一旦构建并收集了统计信息，我们就可以使用`DESCRIBE EXTENDED/FORMATTED`语句检查统计信息。 从表/分区输出中，我们可以找到参数内部的统计信息，例如`parameters:{numFiles=1, COLUMN_STATS_ACCURATE=true, transient_lastDdlTime=1417726247, numRows=4, totalSize=227, rawDataSize=223})`。 以下是检查表中的统计信息的示例：

```sql
-- Check statistics in a table
> DESCRIBE EXTENDED employee_partitioned PARTITION(year=2018, month=12);

-- Check statistics in a partition
> DESCRIBE EXTENDED employee;
...
parameters:{numFiles=1, COLUMN_STATS_ACCURATE=true, transient_lastDdlTime=1417726247, numRows=4, totalSize=227, rawDataSize=223}). 
-- Check statistics in a column
> DESCRIBE FORMATTED employee.name;
+--------+---------+---+---+---------+--------------+
|col_name|data_type|min|max|num_nulls|distinct_count| ...
+--------+---------+---+---+---------+--------------+
| name   | string  |   |   | 0       | 5            | ...
+--------+---------+---+---+---------+--------------+
+-----------+-----------+
|avg_col_len|max_col_len| ...
+-----------+-----------+
| 5.6       | 7         | ...
+-----------+-----------+
3 rows selected (0.116 seconds)
```

# 记录 / 砍伐 / 达到 / 伐木

日志提供了解查询/作业如何运行的详细信息。 通过检查日志详细信息，我们可以识别运行时问题和可能导致性能不佳的问题。 有两种类型的日志可用，系统日志和作业日志。

系统日志包含配置单元运行状态和问题。 它在`{HIVE_HOME}/conf/hive-log4j.properties`中配置。 在该文件中可以找到以下三行日志属性：

```sql
hive.root.logger=WARN,DRFA     *## set logger level*
hive.log.dir=/tmp/${user.name} *## set log file path*
hive.log.file=hive.log         *## set log file name*
```

要修改记录器级别，我们可以修改前面应用于所有用户的属性文件，也可以设置仅应用于当前用户会话的配置单元命令行配置，如`$hive --hiveconf hive.root.logger=DEBUG,console`。

作业日志包含作业信息，通常由 Yarn 管理。 要检查作业日志，请使用`yarn logs -applicationId <application_id>`。

# 设计优化

设计优化包括几种设计、数据格式和作业优化策略，以提高性能。 以下各节将详细介绍这一点。

# 分区表设计

配置单元分区是提高大型表查询性能的最有效方法之一。 带有分区筛选的查询将只从指定的分区(子目录)加载数据，因此它的执行速度比按非分区字段过滤的普通查询快得多。 分区键的选择始终是影响性能的重要因素。 它应该始终是一个低基数属性，以避免太多子目录开销。 以下是一些通常用作分区键的属性：

*   **按日期和时间分区**：当数据与日期/时间列(如`load_date`、`business_date`、`run_date`等)相关联时，使用日期和时间(如年、月和日(偶数小时))作为分区键
*   **按位置分区**：当数据与位置相关时，使用国家/地区、地区、州和城市作为分区键
*   **按业务逻辑分区**：当业务逻辑可以均匀地分隔数据时，使用部门、销售区域、应用程序、客户等作为分区键

# 斗台设计

与分区类似，存储桶表在 HDFS 中将数据组织到单独的文件中。 分组可以加快存储桶上的数据采样速度。 如果联接键也是存储桶列，则存储块还可以提高连接性能，因为存储块可以确保密钥出现在某个存储桶中。 选择更好的存储桶列可以使存储桶表连接执行得更好。 选择存储桶列的最佳实践是根据数据集背后的业务逻辑标识最有可能在筛选器或联接条件中使用的列。 有关更多详细信息，请参阅本章后面的*作业优化*部分。

# 指标设计

使用索引是在关系数据库中进行性能调优的一种非常常见的最佳实践。 从 HIVE v0.7.0 开始，HIVE 支持在表/分区上创建索引。 配置单元中的索引为某些操作(如`WHERE`、`GROUP BY`和`JOIN`)提供了基于键的数据视图和更好的数据访问。 使用索引总是比全表扫描更便宜的替代方案。 在 HQL 中创建索引的命令非常简单，如下所示：

```sql
> CREATE INDEX idx_id_employee_id
> ON TABLE employee_id (employee_id)
> AS 'COMPACT'
> WITH DEFERRED REBUILD;
No rows affected (1.149 seconds)
```

除了这个用于存储索引列值及其块 ID 对的`COMPACT`索引之外，从 v0.8.0 开始，HQL 还支持对方差较小的列值使用两个`BITMAP`索引，如下例所示：

```sql
> CREATE INDEX idx_gender_employee_id
> ON TABLE employee_id (gender_age)
> AS 'BITMAP'
> WITH DEFERRED REBUILD;
No rows affected (0.251 seconds)
```

本例中的`WITH DEFERRED REBUILD`选项阻止立即构建索引。 要构建索引，我们可以发出如下示例所示的命令：`ALTER...REBUILD`。 当基表中的数据更改时，必须再次使用同一命令使索引保持最新。 这是一个原子操作。 如果在表上重建的索引以前已索引失败，则索引的状态保持不变。 请参阅此示例以构建索引：

```sql
> ALTER INDEX idx_id_employee_id ON employee_id REBUILD;
```

```sql
 No rows affected (111.413 seconds) 
> ALTER INDEX idx_gender_employee_id ON employee_id REBUILD;
No rows affected (82.23 seconds)
```

一旦建立了索引，就会为名称为**`<database_name>__<table_name>_<index_name>__`**格式的每个索引创建一个新的索引表：

```sql
> SHOW TABLES '*idx*';
+-----------+---------------------------------------------+-----------+
|TABLE_SCHEM|                 TABLE_NAME                  | TABLE_TYPE|
+-----------+---------------------------------------------+-----------+
|default    |default__employee_id_idx_id_employee_id__    |INDEX_TABLE|
|default    |default__employee_id_idx_gender_employee_id__|INDEX_TABLE|
+-----------+---------------------------------------------+-----------+
```

索引表包含索引列、`_bucketname`(HDFS 上的典型文件 URI)和`_offsets`(每行的偏移量)。 然后，当我们查询索引表中的索引列时，可以引用该索引表，如下所示：

```sql
> DESC default__employee_id_idx_id_employee_id__;
+--------------+----------------+----------+
|   col_name   |   data_type    | comment  |
+--------------+----------------+----------+
| employee_id  | int            |          |
| _bucketname  | string         |          |
| _offsets     | array<bigint>  |          |
+--------------+----------------+----------+
3 rows selected (0.135 seconds)

> SELECT * FROM default__employee_id_idx_id_employee_id__;
+--------------+------------------------------------------------------+
| employee_id  | _bucketname                               | _offsets |
+--------------+------------------------------------------------------+
| 100          | .../warehouse/employee_id/employee_id.txt | [0]      |
| 101          | .../warehouse/employee_id/employee_id.txt | [66]     |
| 102          | .../warehouse/employee_id/employee_id.txt | [123]    |
| ...          |                  ...             ...      | ...      |
+--------------+-------------------------------------------+----------+
25 rows selected (0.219 seconds)
```

要删除索引，我们只能使用如下所示的`DROP INDEX index_name ON table_name`语句。 我们不能使用`DROP TABLE`语句删除索引：

```sql
> DROP INDEX idx_gender_employee_id ON employee_id;
No rows affected (0.247 seconds)
```

# 使用倾斜/临时表

除了常规的内部/外部或分区表之外，我们还应该考虑使用倾斜或临时表，以获得更好的设计和性能。

从配置单元 v0.10.0 开始，HQL 支持创建一个特殊的表来组织偏斜数据。-偏斜表可以通过自动将这些偏斜值拆分到单独的文件或目录中来提高性能。 因此，减少了文件或分区文件夹的总数。 此外，查询可以快速高效地包含或忽略此数据。 以下是用于创建倾斜表格的示例：

```sql
> CREATE TABLE sample_skewed_table (
> dept_no int, 
> dept_name string
> ) 
> SKEWED BY (dept_no) ON (1000, 2000); -- Specify value skewed
No rows affected (3.122 seconds)

> DESC FORMATTED sample_skewed_table;
+-----------------+------------------+---------+
| col_name        | data_type        | comment |
+-----------------+------------------+---------+
| ...             | ...              |         |
| Skewed Columns: | [dept_no]        | NULL    |
| Skewed Values:  | [[1000], [2000]] | NULL    |
| ...             | ...              |         |
+-----------------+------------------+---------+
33 rows selected (0.247 seconds)
```

另一方面，在数据递归处理期间使用 HQL 中的临时表保留中间数据将省去重新构建公共或共享结果集的工作。 此外，临时表还可以利用存储策略设置来使用 SSD 或内存进行数据存储，这也会带来更好的性能。

# 数据优化

数据文件优化涵盖了数据文件在文件格式、压缩和存储方面的性能改进。

# 档案格式

配置单元支持`TEXTFILE`、`SEQUENCEFILE`、`AVRO`、`RCFILE`、`ORC`和`PARQUET`文件格式。 有两个 HQL 语句用于指定文件格式，如下所示：

*   `CREATE TABLE ... STORE AS <file_format>`：创建表时指定文件格式
*   `ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT <file_format>`：修改现有表格中的文件格式(仅限定义

一旦创建了以文本格式存储的表，我们就可以直接将文本数据加载到其中。 要将文本数据加载到具有其他文件格式的表格中，我们可以首先将数据加载到以文本形式存储的表格中，在该表格中，我们使用`INSERT OVERWRITE/INTO TABLE ... SELECT`按钮从表格中选择数据，然后将数据插入到具有其他文件格式的表格中。

To change the default file format for table creation, we can set the `hive.default.fileformat = <file_format>` property for all tables or `hive.default.fileformat.managed = <file_format>` only for internal/managed tables.

作为面向行的文件存储格式的`TEXT`、`SEQUENCE`和`AVRO`文件不是最佳解决方案，因为即使只请求一列，查询也必须读取整行。 另一方面，针对这一问题，采用行列混合存储文件格式，如`RCFILE`、`ORC`、`PARQUET`等，HQL 支持的文件格式详情如下：

*   `TEXTFILE`：这是创建表的默认文件格式。 对于此格式，数据以明文形式存储。 文本文件自然是可拆分的，并且能够并行处理。 还可以使用 GZip、LZO 和 Snappy 等算法对其进行压缩。 但是，大多数压缩文件不能拆分以进行并行处理。 因此，他们只使用一个具有单个映射器的作业来缓慢地处理数据。 使用压缩文本文件的最佳实践是确保文件不太大，并且接近几个 HDFS 块大小。
*   `SEQUENCEFILE`：这是键/值对的二进制存储格式。 序列文件的好处是它比文本文件更紧凑，并且非常适合 MapReduce 输出格式。 序列文件可以压缩到记录级或块级，其中块级具有较好的压缩比。 要启用块级压缩，我们需要使用以下设置：`set hive.exec.compress.output=true;`和`set io.seqfile.compression.type=BLOCK;`。
*   `AVRO`：这也是二进制格式。 不仅如此，它还是一个序列化和反序列化框架。 *Avro*提供了一个数据模式，该模式描述数据结构并处理模式更改，如添加、重命名和删除列。 该模式与数据一起存储，以供进一步处理。 考虑到`AVRO`在处理模式演变方面的优势，建议在映射源数据时使用它，源数据可能会随时间发生模式更改。
*   `RCFILE`：这是**记录列文件**的缩写。 它是一个平面文件，由二进制密钥/值对组成，与序列文件有许多相似之处。 `RCFile`将数据水平拆分成行组。 一个或多个组存储在 HDFS 文件中。 然后，`RCFile`以列格式保存行组数据，方法是先保存所有行的第一列，然后保存所有行的第二列，依此类推。 这种格式是可拆分的，允许配置单元跳过数据中不相关的部分，从而更快、更便宜地获得结果。
*   `ORC`：这是优化行列的缩写。 它从配置单元 v0.11.0 开始提供。 可以将`ORC`格式视为`RCFILE`的改进版本。 默认情况下，它提供 256 MB 的较大块大小(`RCFILE`为 4 MB，`SEQUENCEFILE`为 1 MB)，针对 HDFS 上的大型顺序读取进行了优化，以获得更高的吞吐量和更少的文件，以减少 NameNode 中的过载。 与依赖`metastore`了解数据类型的`RCFILE`不同，`ORC`文件通过使用特定的编码器来了解数据类型，因此它可以根据不同的类型优化压缩。 它还存储有关列的基本统计信息，如`MIN`、`MAX`、`SUM`和`COUNT`，以及可用于跳过无关紧要的行块的轻量级索引。
*   `PARQUET`：这是另一种设计与`ORC`类似的行列式文件格式。 更重要的是，与主要由蜂巢、猪和星火支持的`ORC`相比，Parquet 对生态系统中的大多数项目都有更广泛的支持。 `PARQUET`利用 Google 的 DREMEL 设计中的最佳实践(参见[http://research.google.com/pubs/pub36632.html](http://research.google.com/pubs/pub36632.html))来支持数据的嵌套结构。 `PARQUET`从配置单元 v0.10.0 开始受插件支持，在 v0.13.0 之后获得原生支持。

根据使用的技术堆栈，如果配置单元是定义或处理数据的主要工具，则建议使用`ORC`格式。 如果您在生态系统中使用多种工具，`PARQUET`在适应性方面是更好的选择。

**Hadoop Archive File** (**HAR**) is another type of file format to pack HDFS files into archives. This is an option (not a good option) for storing a large number of small-sized files in HDFS, as storing a large number of small-sized files directly in HDFS is not very efficient. However, `HAR` has other limitations, such as an immutable archive process, not being splittable, and compatibility issues. For more information about `HAR` and archiving, please refer to the Hive Wiki at [https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Archiving](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Archiving).

# 压缩 / 压紧 / 压挤 / 压榨，压迫

通过适当压缩中间和最终输出数据，配置单元中的压缩技术可以显著减少映射器和减少器之间传输的数据量。 因此，查询将具有更好的性能。 要压缩在多个 MapReduce 作业之间生成的中间文件，我们需要在命令行会话或`hive-site.xml`文件中设置以下属性(默认情况下为`false`)：

```sql
> SET hive.exec.compress.intermediate=true
```

然后，我们需要决定配置哪个压缩编解码器。 下表列出了通常支持的各种编解码器：

| **压缩** | **编解码器** | **扩展** | **可拆分** |
| 显示 | `org.apache.hadoop.io.compress.DefaultCodec` | `.deflate` | （化学元素）氮 |
| Gzip | `org.apache.hadoop.io.compress.GzipCodec` | `.gz` | （化学元素）氮 |
| Bzip2 | `org.apache.hadoop.io.compress.BZip2Codec` | `.gz` | 英语字母表中第二十五个字母 / Y 字形 / Y 项 |
| LZO | `com.apache.compression.lzo.LzopCodec` | `.lzo` | （化学元素）氮 |
| LZ4 | `org.apache.hadoop.io.compress.Lz4Codec` | `.lz4` | （化学元素）氮 |
| 厉声说话的 / 恶声恶气的 / 短小精悍的 / 时髦的 | `org.apache.hadoop.io.compress.SnappyCodec` | `.snappy` | （化学元素）氮 |

DEVATE(`.deflate`)是一种默认编解码器，具有均衡的压缩比和 CPU 成本。 Gzip 的压缩比非常高，其 CPU 成本也很高。 Bzip2 是可拆分的，但考虑到其巨大的 CPU 成本，就像 Gzip 一样，它对于压缩来说太慢了。 LZO 文件本身不能拆分，但我们可以对其进行预处理(使用`com.hadoop.compression.lzo.LzoIndexer`)以创建确定文件拆分的索引。 当谈到 CPU 成本和压缩比的平衡时，LZ4 或 Snappy 比 Deflate 做得更好，但 Snappy 更受欢迎。 由于大多数压缩文件是不可拆分的，因此不建议压缩单个大文件。 最佳实践是生成几个 HDFS 块大小的压缩文件，以便每个文件花费更少的处理时间。 压缩编解码器可以在`mapred-site.xml`、`hive-site.xml`或命令行会话中指定，如下所示：

```sql
> SET hive.intermediate.compression.codec=
org.apache.hadoop.io.compress.SnappyCodec
```

中间压缩只会为需要多个 MapReduce 作业的特定作业节省磁盘空间。 为了进一步节省磁盘空间，可以压缩实际的配置单元输出文件。 当`hive.exec.compress.output`属性设置为`true`时，配置单元将使用由`mapreduce.output.fileoutputformat.compress.codec`属性配置的编解码器来压缩 HDFS 中的数据，如下所示。 这些属性可以在`hive-site.xml`或命令行会话中设置：

```sql
> SET hive.exec.compress.output=true
> SET mapreduce.output.fileoutputformat.compress.codec=
org.apache.hadoop.io.compress.SnappyCodec
```

# 存储优化

频繁使用或扫描的数据可以识别为热门数据。 通常，热数据上的查询性能对整体性能至关重要。 为热数据增加 HDFS 中的数据复制系数(请参见以下示例)可能会增加作业在本地命中数据的可能性，并提高整体性能。 但是，这是对存储的权衡：

```sql
$ hdfs dfs -setrep -R -w 4 /user/hive/warehouse/employee
Replication 4 set: /user/hive/warehouse/employee/000000_0
```

另一方面，过多的文件或冗余可能会使 NameNode 的内存耗尽，特别是大量小于 HDFS 块大小的小文件。 Hadoop 本身已经有一些解决方案，可以通过以下方式处理许多小文件问题：

*   **Hadoop Archive/**`HAR`：这些是前面介绍的打包小文件的工具包。
*   `SEQUENCEFILE`**格式**：这是一种可用于将小文件压缩成大文件的格式。
*   `CombineFileInputFormat`：一种`InputFormat`类型，用于在映射和缩减处理之前合并小文件。 它是配置单元的默认`InputFormat`(参见[https://issues.apache.org/jira/browse/HIVE-2245](https://issues.apache.org/jira/browse/HIVE-2245))。
*   **HDFS 联合**：支持多个名称节点管理更多文件。

如果我们安装了 Hadoop 生态系统中的其他工具，我们还可以利用它们，例如：

*   HBase 具有更小的数据块大小和更好的文件格式，可处理较小的文件存储和访问问题
*   Flume NG 可用作将小文件合并为大文件的管道
*   开发并计划了一个文件合并程序，用于在 HDFS 中或在将文件加载到 HDFS 之前合并小文件

对于配置单元，我们可以使用以下配置来合并查询结果文件，避免重新创建小文件：

*   `hive.merge.mapfiles`：这会在仅地图作业结束时合并小文件。 默认情况下，它是`true`。
*   `hive.merge.mapredfiles`：这会在 MapReduce 作业结束时合并小文件。 将其设置为*TRUE，*，因为默认值为`false`。
*   `hive.merge.size.per.task`：此选项定义作业结束时合并文件的大小。 默认值为 256,000,000。
*   `hive.merge.smallfiles.avgsize`：这是触发文件合并的阈值。 默认值为 16,000,000。

当作业的平均输出文件大小小于`hive.merge.smallfiles.avgsize`*和*属性指定的值，并且`hive.merge.mapfiles`(对于纯地图作业)和`hive.merge.mapredfiles`(对于 MapReduce 作业)都设置为*TRUE*时，配置单元将启动附加的 MapReduce 作业以将输出文件合并到大文件中。

# 作业优化

作业优化包括在作业运行模式、JVM 重用、作业并行运行和查询连接优化等方面提高性能的经验和技能。

# 本地模式

Hadoop 可以在独立、伪分布式和完全分布式模式下运行。 大多数情况下，我们需要将其配置为在完全分布式模式下运行。 当要处理的数据较小时，由于完全分布式模式的启动时间比作业处理时间更长，因此启动分布式数据处理是一种开销。 从 v0.7.0 开始，配置单元支持使用以下设置自动转换作业以在本地模式下运行：

```sql
> SET hive.exec.mode.local.auto=true; -- default false > SET hive.exec.mode.local.auto.inputbytes.max=50000000;
> SET hive.exec.mode.local.auto.input.files.max=5; -- default 4
```

作业必须满足以下条件才能在本地模式下运行：

*   作业的总输入大小小于`hive.exec.mode.local.auto.inputbytes.max`设置的值
*   地图任务总数小于设置的值`hive.exec.mode.local.auto.input.files.max`
*   所需的 Reduce 任务总数为 1 或 0

# JVM 重用

默认情况下，Hadoop 为每个 map 或 Reduce 作业启动一个新的 JVM，并并行运行 map 或 Reduce 任务。 当 map 或 Reduce 作业是只运行几秒钟的轻量级作业时，JVM 启动过程可能会带来很大的开销。 Hadoop 可以通过共享 JVM 来选择重用 JVM，以串行而不是并行地运行映射器/减少器。 JVM 重用适用于映射或减少同一作业中的任务。 来自不同作业的任务将始终在单独的 JVM 中运行。 要启用重用，我们可以使用以下属性为 JVM 重用设置单个作业的最大任务数。 默认值为 1。如果设置为-1，则没有限制：

```sql
> SET mapreduce.job.jvm.numtasks=5;
```

# 并行执行

蜂窝查询通常被转换成由默认序列执行的多个阶段。 这些阶段并不总是相互依赖的。 相反，它们可以并行运行，以减少总体作业运行时间。 我们可以使用以下设置启用此功能，并设置并行运行的预期作业数：

```sql
> SET hive.exec.parallel=true; -- default false > SET hive.exec.parallel.thread.number=16; -- default 8
```

并行执行将提高集群利用率。 如果集群的利用率已经很高，并行执行在总体性能方面不会有太大帮助。

# 连接优化

我们已经在[章](04.html)，*的数据关联和范围*中讨论了不同类型的配置单元联接的优化。 在这里，我们将简要回顾联接改进的关键设置。

# 公共连接

公共联接也称为 Reduce 侧联接。 它是 HQL 中的一个基本连接，并且在大多数情况下都有效。 对于普通连接，我们需要确保大表位于最右侧或由 HIT 指定，如下所示：

```sql
/*+ STREAMTABLE(stream_table_name) */
```

# 地图连接

当其中一个连接表足够小，可以放入内存时，就会使用映射连接，因此它速度很快，但受到表大小的限制。 从配置单元 v0.7.0 开始，它可以使用以下设置自动转换地图连接：

```sql
> SET hive.auto.convert.join=true; -- default true after v0.11.0 > SET hive.mapjoin.smalltable.filesize=600000000; -- default 25m
> SET hive.auto.convert.join.noconditionaltask=true; -- default value above is true so map join hint is not needed
> SET hive.auto.convert.join.noconditionaltask.size=10000000; -- default value above controls the size of table to fit in memory
```

一旦启用联接和自动转换，配置单元将自动检查较小的表文件大小是否大于`hive.mapjoin.smalltable.filesize`指定的值，然后将联接转换为普通联接。 如果文件大小小于此阈值，它将尝试将公共连接转换为映射连接。 一旦启用了自动转换联接，就不需要在查询中提供映射联接提示。

# 桶图连接

存储桶映射连接是应用于存储桶表的一种特殊类型的映射连接。 要启用存储桶图加入，我们需要启用以下设置：

```sql
> SET hive.auto.convert.join=true; 
> SET hive.optimize.bucketmapjoin=true; -- default false
```

在存储桶映射连接中，所有连接表必须是存储桶表并连接到存储桶列。 此外，较大表格中的存储桶编号必须是较小表格中的存储桶编号的倍数。

# 排序合并存储桶(SMB)联接

SMB 是对具有相同的排序、存储桶和联接条件列的存储桶表执行的联接。 它从两个桶表中读取数据，并在桶表上执行公共连接(映射和 Reduce 触发)。 我们需要启用以下属性才能使用 SMB：

```sql
> SET hive.input.format=
> org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
> SET hive.auto.convert.sortmerge.join=true;
> SET hive.optimize.bucketmapjoin=true;
> SET hive.optimize.bucketmapjoin.sortedmerge=true;
> SET hive.auto.convert.sortmerge.join.noconditionaltask=true;
```

# 排序合并桶图(SMBM)联接

SMBM 联接是一种特殊的桶联接，但仅触发映射端联接。 它可以避免像映射连接那样缓存内存中的所有行。 要执行 SMBM 联接，联接表必须具有相同的存储桶列、排序列和联接条件列。 要启用此类联接，我们需要启用以下设置：

```sql
> SET hive.auto.convert.join=true;
> SET hive.auto.convert.sortmerge.join=true
> SET hive.optimize.bucketmapjoin=true;
> SET hive.optimize.bucketmapjoin.sortedmerge=true;
> SET hive.auto.convert.sortmerge.join.noconditionaltask=true;
> SET hive.auto.convert.sortmerge.join.bigtable.selection.policy=
org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;
```

# 斜连接

当处理分布高度不均匀的数据时，可能会发生数据歪斜，导致少数计算节点必须处理大量计算。 如果发生数据倾斜，以下设置将通知配置单元正确优化：

```sql
> SET hive.optimize.skewjoin=true; --If there is data skew in join, set it to true. Default is false.

> SET hive.skewjoin.key=100000; 
 --This is the default value. If the number of key is bigger than 
 --this, the new keys will send to the other unused reducers.
```

Skewed data could occur with the `GROUP BY` data too. To optimize it, we need `set hive.groupby.skewindata=true` to use the preceding settings to enable skew data optimization in the `GROUP BY` result. Once configured, Hive will first trigger an additional MapReduce job whose map output will randomly distribute to the reducer to avoid data skew.

有关联接优化的更多信息，请参考位于[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization)和[https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization](https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization)的配置单元维基。

# 作业引擎

配置单元支持在不同引擎上运行作业。 发动机的选择也会影响整体性能。 然而，与其他设置相比，这是一个更大的变化。 此外，此更改需要重新启动服务，而不是临时使其在命令行会话中生效。 以下是设置引擎的语法以及每个引擎的详细信息：

```sql
SET hive.execution.engine=<engine>; -- <engine> = mr|tez|spark 
```

*   `mr`：这是默认引擎 MapReduce。 在配置单元 v2.0.0 之后，它已弃用。
*   `tez`：TEZ([http://tez.apache.org/](http://tez.apache.org/))是一个构建在 Yarn 上的应用程序框架，它可以为一般的数据处理任务执行复杂的**有向无环图**和(**DAG**s)。 TEZ 进一步将 MAP 和 Reduce 作业拆分成更小的任务，并以灵活高效的方式将它们组合在一起执行。 TEZ 被认为是 MapReduce 框架灵活而强大的继任者。 TEZ 已经为生产做好了准备，并且大部分时间都被用来取代 MR 引擎。
*   `spark`：Spark 是另一个通用大数据框架。 它的组件 Spark SQL 支持 HQL 的一个子集，并提供类似于 HQL 的语法。 通过使用 Spark 上的配置单元，Spark 可以利用 Spark 的内存计算模型以及 Spark 成熟的基于成本的优化器。 然而，星火之上的蜂巢需要手动配置，在实际生产中仍然缺乏坚实的使用案例。 有关“星火上的蜂巢”的更多细节，请参考([https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started))的维基页面。
*   `mr3`：mr3 是另一个实验引擎([https://mr3.postech.ac.kr/](https://mr3.postech.ac.kr/))。 它类似于 TEZ，但增强了设计、更好的性能和更多的功能。 MR3 记录为可投入生产使用，并支持 TEZ 的所有主要功能，例如基于 Kerberos 的安全性、身份验证和授权、容错和恢复。 但是，它缺乏可靠的生产使用案例和生产部署的最佳实践，也缺乏对 CDH 或 HDP 分发的支持。

**Live Long And Process** (**LLAP**) functionality was added in Hive v2.0.0\. It combines a live long running query service and intelligent in-memory caching to deliver fast queries. Together with a job engine, LLAP provides a hybrid execution model to improve overall Hive performance. LLAP needs to work through Apache Slider ([https://slider.incubator.apache.org/](https://slider.incubator.apache.org/)) and only works with Tez for now. In the future, it will support other engines. The recent HDP has provided LLAP supported thought Tez.

# 优化器

与关系数据库类似，配置单元在提交最终执行之前生成并优化每个查询的逻辑和物理执行计划。 目前在配置单元中有两个主要的优化器来进一步优化总体上的查询性能，向量化和**基于成本的优化**(**CBO**)。

# 矢量化优化

矢量化优化可以同时处理更大批量的数据，而不是一次处理一行，从而显著降低计算开销。 每个批处理由一个列向量组成，该列向量通常是一个基元类型数组。 操作是在整个列向量上执行的，这改进了指令流水线和高速缓存的使用。 文件必须以`ORC`格式存储，才能使用矢量化。 有关矢量化的更多详细信息，请参考蜂巢维基([https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution](https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution))。 要启用矢量化，我们需要使用以下设置：

```sql
> SET hive.vectorized.execution.enabled=true; -- default false
```

# 基于成本的优化

配置单元中的 CBO 由 Apache 方解石([http://calcite.apache.org/](http://calcite.apache.org/))提供支持，这是一个开源的、基于成本的企业级逻辑优化器和查询执行框架。 HIVE CBO 通过检查由`ANALYZE`语句或`metastore`本身收集的查询成本来生成高效的执行计划，最终缩短查询执行时间并降低资源使用率。 要使用 CBO，请设置以下属性：

```sql
> SET hive.cbo.enable=true; -- default true after v0.14.0
> SET hive.compute.query.using.stats=true; -- default false
> SET hive.stats.fetch.column.stats=true; -- default false
> SET hive.stats.fetch.partition.stats=true; -- default true
```

# 简略的 / 概括的 / 简易判罪的 / 简易的

在本章中，我们首先介绍了如何使用`EXPLAIN`和`ANALYZE`语句识别性能瓶颈。 然后，我们谈到了使用表、分区和索引时的性能优化设计。 我们还介绍了数据文件优化，包括文件格式、压缩和存储。 在本章的最后，我们讨论了作业优化、作业引擎和优化器。 读完本章后，您应该能够在配置单元中进行性能故障排除和调优。 在下一章中，我们将讨论配置单元的功能扩展。