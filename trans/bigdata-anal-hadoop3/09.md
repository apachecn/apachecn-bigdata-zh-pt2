# 九、Apache Flink 流处理

在这一章中，我们将研究使用 Apache Flink 的流处理，以及该框架如何在数据到达时立即用于处理数据，以构建令人兴奋的实时应用程序。我们将从数据流应用编程接口开始，看看可以执行的各种操作。

我们将关注以下内容:

*   使用 DataStream 应用编程接口进行数据处理
*   转换
*   聚集
*   窗户
*   物理分区
*   改比例
*   数据接收器
*   事件时间和水印
*   卡夫卡连接器
*   Twitter 连接器
*   Elasticsearch Connector
*   卡珊德拉连接器

# 流执行模型介绍

Flink 是一个用于分布式流处理的开源框架，它:

*   提供准确的结果，即使是无序或延迟到达的数据
*   是有状态的和容错的，并且可以从故障中无缝恢复，同时保持一次应用程序状态
*   大规模执行，在数千个节点上运行，具有非常好的吞吐量和延迟特性

下图是流处理的一般视图:

![](assets/1269453b-2ee2-45d0-bf50-1bf3d0347bf8.png)

Flink 的许多功能(状态管理、处理无序数据、灵活的窗口)对于在无界数据集上计算精确结果至关重要，并且由 Flink 的流执行模型实现:

*   Flink 保证有状态计算只有一次语义。有状态意味着应用程序可以维护一段时间内处理过的数据的聚合或汇总，而 Flink 的检查点机制确保了在发生故障时应用程序状态的一次语义:

![](assets/1ebf3a20-62fb-4bb6-bb65-47993356becb.png)

*   Flink 支持流处理和带有事件时间语义的窗口。事件时间可以轻松计算出事件无序到达和事件可能延迟到达的流的准确结果:

![](assets/74b2f7a9-899a-44a3-8d11-0000450c4b2f.png)

*   除了数据驱动窗口之外，Flink 还支持基于时间、计数或会话的灵活窗口。Windows 可以通过灵活的触发条件进行定制，以支持复杂的流模式。Flink 的窗口技术可以模拟数据生成环境的真实情况:

![](assets/fad745cc-3d75-4d25-b616-a7383f8722bf.png)

*   Flink 的容错是轻量级的，允许系统保持高吞吐率，同时提供一次性一致性保证。Flink 从故障中恢复，零数据丢失，而可靠性和延迟之间的平衡可以忽略不计:

![](assets/aa4ce642-0d14-457c-a40a-04d89724e730.png)

*   Flink 能够实现高吞吐量和低延迟(快速处理大量数据)。
*   Flink 的保存点提供了一种状态版本机制，使更新应用程序或重新处理历史数据成为可能，而不会丢失状态，停机时间也很少。
*   Flink 旨在运行在具有数千个节点的大规模集群上，除了独立的集群模式之外，Flink 还提供了对纱线和 Mesos 的支持。

# 使用 DataStream 应用编程接口进行数据处理

拥有强大的分析功能来处理实时数据至关重要。这对于数据驱动的域更为重要。Flink 使您能够使用其 DataStream 应用编程接口进行实时分析。这个流式数据处理应用编程接口帮助您迎合**物联网** ( **物联网**)应用，并实时或接近实时地存储、处理和分析数据。

在接下来的几节中，让我们检查与数据流应用编程接口相关的每个元素:

*   执行环境
*   数据源
*   转换
*   数据接收器
*   连接器

# 执行环境

要编写一个 Flink 程序，需要一个执行环境。您可以使用现有环境或创建新环境。

根据您的需求，Flink 允许您使用现有的 Flink 环境、创建本地环境或创建远程环境。

根据您的要求，使用`getExecutionEnvironment()`命令完成不同的任务:

*   为了在集成开发环境的本地环境中执行，它会启动一个本地执行环境
*   为了执行 JAR，Flink 集群管理器以分布式方式执行程序
*   要创建您自己的本地或远程环境，您可以使用诸如`createLocalEnvironment()`和`createRemoteEnvironment`等方法(字符串主机、int 端口、字符串和`.jar`文件)

# 数据源

Flink 从不同的来源获得数据。它有许多内置的源函数来无缝地获取数据。Flink 中几个预先实现的数据源功能简化了数据来源。Flink 还允许您在现有函数不足以进行数据来源时编写自定义数据源函数。

这里记录了 DataStream API:[https://ci . Apache . org/project/flink/flink-docs-release-1.4/dev/DataStream _ API . html](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/datastream_api.html)。

以下是 Flink 中一些现有的数据源函数:

*   基于套接字的数据来源
*   基于文件的数据来源

# 基于套接字的

数据流应用编程接口使您能够从套接字读取数据。查看下面这段代码，了解流应用编程接口的简单说明:

```scala
// Data type for words with count
case class WordWithCount(word: String, count: Long)
// get input data by connecting to the socket
val text = senv.socketTextStream("127.0.0.1", 9000, '\n')
// parse the data, group it, window it, and aggregate the counts
val windowCounts = text
 .flatMap { w => w.split("\\s") }
 .map { w => WordWithCount(w, 1) }
 .keyBy("word")
 .timeWindow(Time.seconds(5), Time.seconds(1))
 .sum("count")
// print the results with a single thread, rather than in parallel
windowCounts.print().setParallelism(1)
senv.execute("Socket Window WordCount")
```

前面的代码连接到本地主机上的端口`9000`，接收和处理文本，将字符串拆分成单个单词(用空格分隔)。然后，代码在`5`秒的窗口中统计单词的频率并打印出来。

为了运行这个例子，我们将使用 Flink 的 Scala 外壳:

![](assets/ff933f47-408d-47af-9448-ddafe7f2e522.png)

现在，在任何 Linux 系统上启动一个运行`nc`的本地服务器，如下所示:

![](assets/306bcd44-a116-450b-b18a-1f19686f8e8c.png)

现在，运行 shell 中的代码连接到端口`9000`并监听数据:

![](assets/d1b98795-7f42-4a2e-901c-fae4a7740bb7.png)

您现在可以在 web 控制台中看到作业正在运行:

![](assets/07bbf422-d6f0-4d93-85a6-b8dbd221634b.png)

您可以深入了解这些任务:

![](assets/b89a5abf-f5de-419a-8da0-2115390e9a1f.png)

Figure: Screenshot showing a view of the tasks

如果您开始在`nc`服务器控制台中键入文本，您将开始在`log`文件夹中看到输出。

在我的例子中，我看到一个`taskmanager`的`log`文件:

```scala
tail -f log/flink-sridharalla-taskmanager-1-Moogie.local.out
```

以下是您在跟踪`log`文件时会看到的内容:

![](assets/fc18cea2-958c-4ffc-8e80-5dd47919ee47.png)

现在我们已经看到了运行的示例代码，让我们看看套接字流的 API。

在应用编程接口中指定从套接字读取数据的主机和端口:

```scala
socketTextStream(hostName, port);
```

您也可以指定分隔符:

```scala
socketTextStream(hostName,port,delimiter)
```

您还可以指定应用编程接口必须从套接字获取数据的最大次数:

```scala
socketTextStream(hostName,port,delimiter, maxRetry)
```

# 基于文件

使用 Flink 中基于文件的源函数从文件源中流式传输数据。使用`readTextFile(String path)`从指定文件中流式传输数据。默认情况下，字符串路径具有默认值`TextInputFormat`。这意味着它逐行读取文本和字符串。

如果文件格式不同于文本，请使用以下功能指定格式:

```scala
readFile(FileInputFormat<Out> inputFormat, String path)
```

使用`readFileStream()`功能，Flink 可以在文件流产生时读取文件流:

```scala
readFileStream(String filePath, long intervalMillis, FileMonitoringFunction.WatchType watchType)
```

指定文件路径、轮询文件路径的轮询间隔以及监视类型。手表类型有三种:

*   `FileMonitoringFunction.WatchType.ONLY_NEW_FILES`:用于只处理新文件
*   `FileMonitoringFunction.WatchType.PROCESS_ONLY_APPENDED`:用于只处理文件的附加内容
*   `FileMonitoringFunction.WatchType.REPROCESS_WITH_APPENDED`:不仅用来重新处理文件的追加内容，也用来重新处理文件中之前的内容

如果文件不是文本文件，则可以使用此功能定义文件输入格式:

```scala
readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo)
```

此命令将读取文件任务分为两个子任务:

*   一个子任务只监控基于指定`WatchType`的文件路径
*   第二子任务并行执行实际的文件读取

监控文件路径的子任务是非并行子任务。它根据轮询间隔连续扫描文件路径，报告要处理的文件，分割文件，并将分割分配给相应的下游线程。

# 转换

数据转换将数据流从一种形式转换为另一种形式。输入可以是一个或多个数据流，输出可以是零，也可以是一个或多个数据流。在接下来的部分中，让我们检查不同的转换。

# 地图

这是最简单的转换之一，其中输入是一个数据流，输出也是一个数据流:

**在 Java 中**:

```scala
inputStream.map(new MapFunction<Integer, Integer>() {
@Override
public Integer map(Integer value) throws Exception {
return 5 * value;
}
});
```

**在斯卡拉**:

```scala
inputStream.map { x => x * 5 }
```

# 平面地图

`flatMap`将一条记录作为输入，并给出零条、一条或多条记录的输出:

**在 Java 中**:

```scala
inputStream.flatMap(new FlatMapFunction<String, String>() {
@Override
public void flatMap(String value, Collector<String> out)
throws Exception {
    for(String word: value.split(" ")){
        out.collect(word);
    }
});

```

**在斯卡拉**:

```scala
inputStream.flatMap { str => str.split(" ") }
```

# 过滤器

`filter`功能评估条件，并根据满足的条件给出记录作为输出:

The `filter` function can output zero records also.

**在 Java 中**:

```scala
inputStream.filter(new FilterFunction<Integer>() {
@Override
    public boolean filter(Integer value) throws Exception {
        return value != 1;
    }
});
```

**在斯卡拉**:

```scala
inputStream.filter { _ != 1 }
```

# 键比

`keyBy`根据密钥对流进行逻辑分区。它使用`hash`函数来划分流。它返回`KeyedDataStream`:

**在 Java 中**:

```scala
inputStream.keyBy("someKey");
```

**在斯卡拉**:

```scala
inputStream.keyBy("someKey")
```

# 减少

`reduce`通过将最后一个减少的值减少当前值来推出`KeyedDataStream`。下面的代码做一个`KeyedDataStream`的和减:

**在 Java 中**:

```scala
keyedInputStream. reduce(new ReduceFunction<Integer>() {
@Override
    public Integer reduce(Integer value1, Integer value2)
        throws Exception {
            return value1 + value2;
        }
});
```

**在斯卡拉**:

```scala
keyedInputStream. reduce { _ + _ }
```

# 折叠

`fold`通过将最后一个文件夹的流与当前记录相结合，推出`KeyedDataStream`。它会发回数据流:

**在 Java 中**:

```scala
keyedInputStream keyedStream.fold("Start", new FoldFunction<Integer, String>() {
@Override
    public String fold(String current, Integer value) {
        return current + "=" + value;
    }
});
```

**在斯卡拉**:

```scala
keyedInputStream.fold("Start")((str, i) => { str + "=" + i })
```

前面的函数应用于(1，2，3，4，5)流时，将发出一个流

像这样:`Start=1=2=3=4=5`。

# 聚集

数据流应用编程接口支持各种聚合，如`min`、`max`、`sum`等。这些功能可以在`KeyedDataStream`上应用，以获得滚动聚合:

**在 Java 中**:

```scala
keyedInputStream.sum(0)
keyedInputStream.sum("key")
keyedInputStream.min(0)
keyedInputStream.min("key")
keyedInputStream.max(0)
keyedInputStream.max("key")
keyedInputStream.minBy(0)
keyedInputStream.minBy("key")
keyedInputStream.maxBy(0)
keyedInputStream.maxBy("key")

```

**在斯卡拉**:

```scala
keyedInputStream.sum(0)
keyedInputStream.sum("key")
keyedInputStream.min(0)
keyedInputStream.min("key")
keyedInputStream.max(0)
keyedInputStream.max("key")
keyedInputStream.minBy(0)
keyedInputStream.minBy("key")
keyedInputStream.maxBy(0)
keyedInputStream.maxBy("key")
```

`max`和`maxBy`的区别在于`max`返回一个流中的最大值，而`maxBy`返回一个有最大值的键。这同样适用于`min`和`minBy`。

# 窗户

`window`功能允许按时间或其他条件对现有的`KeyedDataStreams`进行分组。以下转换以`10`秒的时间窗口发出记录组:

**在 Java 中**:

```scala
inputStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(10)));
```

**在斯卡拉**:

```scala
inputStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(10)))
```

Flink 定义了称为窗口的数据切片来处理潜在的无限数据流。

这有助于使用转换处理数据块。要对流进行开窗，请分配一个可以进行分发的密钥和一个描述对开窗流执行哪些转换的函数。

要将流分割成窗口，可以使用预先实现的 Flink 窗口分配器。使用滚动窗口、滑动窗口、全局窗口和会话窗口等选项。

Flink 还允许你通过扩展`WindowAssigner`类来编写自定义窗口分配器。

让我们在以下几节中研究这些分配器是如何工作的。

# 全局窗口

除非由触发器指定，否则全局窗口是永不结束的窗口。一般来说，在这种情况下，每个元素被分配给一个单独的每键全局窗口。如果未指定任何触发器，则不会触发任何计算。

# 翻滚的窗户

翻转窗口是固定长度的窗口，并且不重叠。使用滚动窗口在特定时间计算元素。例如，10 分钟的翻转窗口可用于计算 10 分钟内发生的一组事件。

# 推拉窗

滑动窗口类似于翻滚窗口，只是它们是重叠的。它们是固定长度的窗口，通过用户给定的窗口滑动参数与前面的窗口重叠。

使用这个窗口来计算在特定时间范围内发生的一组事件。

# 会话窗口

当必须根据输入数据决定窗口边界时，会话窗口非常有用。会话窗口允许窗口开始时间和窗口大小的灵活性。

提供会话间隙配置参数，该参数指示在认为会话已关闭之前等待的持续时间。

# windowsll

`windowAll`功能允许对常规数据流进行分组。这通常是非并行数据转换，因为它运行在非分区数据流上:

**在 Java 中**:

```scala
inputStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(10)));
```

**在斯卡拉**:

```scala
inputStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(10)))
```

类似于常规的数据流函数，我们也有窗口数据流函数。唯一不同的是，它们处理的是窗口数据流。因此，窗口缩小的工作方式类似于`reduce`函数，窗口折叠的工作方式类似于`fold`函数，并且还有聚合。

# 联盟

`union`函数执行两个或多个数据流的合并。它并行组合数据流。如果将一个流与其自身组合，它会输出每个记录两次:

**在 Java 中**:

```scala
inputStream. union(inputStream1, inputStream2, ...);
```

**在斯卡拉**:

```scala
inputStream. union(inputStream1, inputStream2, ...)
```

# 窗口连接

通过公共窗口中的一些键连接两个数据流。以下示例显示了两个流在`5`秒窗口中的连接，其中第一个流的第一个属性的连接条件等于另一个流的第二个属性:

**在 Java 中**:

```scala
inputStream. join(inputStream1)
.where(0).equalTo(1)
.window(TumblingEventTimeWindows.of(Time.seconds(5)))
.apply (new JoinFunction () {...});
```

**在斯卡拉**:

```scala
inputStream. join(inputStream1)
.where(0).equalTo(1)
.window(TumblingEventTimeWindows.of(Time.seconds(5)))
.apply { ... }
```

# 使分离

使用此功能，根据标准将流`split`分成两个或多个流。当您获得混合流并且您可能想要单独处理数据时，这尤其有用:

**在 Java 中**:

```scala
SplitStream<Integer> split = inputStream.split(new OutputSelector<Integer>() {
@Override
public Iterable<String> select(Integer value) {
List<String> output = new ArrayList<String>();
if (value % 2 == 0) {
output.add("even");
}
else {
output.add("odd");
}
return output;
}
});

```

**在斯卡拉**:

```scala
val split = inputStream.split( (num: Int) =>(num % 2) match {
    case 0 => List("even")
    case 1 => List("odd")
})
```

# 挑选

使用此功能从分割流中选择特定的流:

**在 Java 中**:

```scala
SplitStream<Integer> split;
DataStream<Integer> even = split.select("even");
DataStream<Integer> odd = split.select("odd");
DataStream<Integer> all = split.select("even","odd");
```

**在斯卡拉**:

```scala
val even = split select "even"
val odd = split select "odd"
val all = split.select("even","odd")
```

# 项目

使用`project`功能从事件流中选择属性子集，并且仅将选定的元素发送到下一个处理流:

**在 Java 中**:

```scala
DataStream<Tuple4<Integer, Double, String, String>> in = // [...]
DataStream<Tuple2<String, String>> out = in.project(3,2);
```

**在斯卡拉**:

```scala
val in : DataStream[(Int,Double,String)] = // [...]
val out = in.project(3,2)
```

前面的函数从给定的记录中选择属性号`2`和`3`。以下是示例输入和输出记录:

```scala
(1,10.0, A, B )=> (B,A)
(2,20.0, C, D )=> (D,C)
```

# 物理分区

使用 Flink，您可以对流数据进行物理分区。您还可以选择提供自定义分区。让我们在以下几节中研究不同类型的分区。

# 自定义分区

如前所述，您可以提供分区器的自定义实现:

**在 Java 中**:

```scala
inputStream.partitionCustom(partitioner, "someKey");
inputStream.partitionCustom(partitioner, 0);
```

**在斯卡拉**:

```scala
inputStream.partitionCustom(partitioner, "someKey")
inputStream.partitionCustom(partitioner, 0)
```

编写自定义分区器时，请确保实现了有效的`hash`函数。

# 随机分区

随机分区以均匀的方式随机划分数据流:

**在 Java 中**:

```scala
inputStream.shuffle();
```

**在斯卡拉**:

```scala
inputStream.shuffle()
```

# 重新平衡分区

这种类型的分区有助于均匀分布数据。它使用循环方法进行分发。当数据有偏差时，这种类型的分区是很好的:

**在 Java 中**:

```scala
inputStream.rebalance();
```

**在斯卡拉**:

```scala
inputStream.rebalance()
```

# 改比例

重新缩放用于跨操作分布数据，对数据子集执行转换，并将它们组合在一起。这种重新平衡仅发生在单个节点上，因此不需要任何跨网络的数据传输:

**在 Java 中**:

```scala
inputStream.rescale();
```

**在斯卡拉**:

```scala
inputStream.rescale()
```

# 广播

广播将所有记录分发到每个分区。这有助于将每个元素分布到所有分区:

**在 Java 中**:

```scala
inputStream.broadcast();
```

**在斯卡拉**:

```scala
inputStream.broadcast()Data Sinks
```

一旦数据转换完成，您必须保存结果。以下是保存结果的一些 Flink 选项:

*   `writeAsText()`:以字符串形式一次写入一行记录。
*   `writeAsCsV()`:将元组写成逗号分隔的值文件。还可以配置行和字段分隔符。
*   `print()` / `printErr()`:将记录写入标准输出。您也可以选择写入标准错误。
*   `writeUsingOutputFormat()`:也可以提供自定义输出格式。定义自定义格式时，扩展`OutputFormat`，负责序列化和反序列化。
*   `writeToSocket()` : Flink 也支持将数据写入特定的套接字。定义`SerializationSchema`进行适当的序列化和格式化。

# 事件时间和水印

Flink 流媒体应用编程接口的灵感来自谷歌数据流模型。这个 API 支持不同的时间概念。以下是您可以在流环境中捕获时间的三个常见位置:

*   **事件时间**:事件时间是指事件在其产生设备上发生的时间。例如，在物联网项目中，它可以是传感器捕捉读数的时间。一般来说，这些事件时间在进入 Flink 之前需要嵌入到记录中。在时间处理过程中，这些时间戳被提取出来并考虑用于开窗。事件时间处理可用于无序事件。
*   **处理时间**:处理时间是执行数据流处理的机器时间。处理时间窗口只考虑事件被处理的时间戳。处理时间是流处理的最简单方式，因为它不需要处理机器和生产机器之间的任何同步。在分布式异步环境处理中，时间不提供确定性，因为它取决于记录在系统中流动的速度。
*   **摄入时间**:摄入时间是特定事件进入 Flink 的时间。所有基于时间的操作都引用这个时间戳。摄取时间是一个比处理更昂贵的操作，但会产生可预测的结果。摄取时间程序不能处理任何无序事件，因为它只在事件进入 Flink 系统后才分配时间戳。

以下示例显示了如何设置事件时间和水印。在摄取时间和处理时间的情况下，只需分配时间特征，水印生成就会自动完成。下面是这个的代码片段:

**在 Java 中**:

```scala
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);
//or
env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);
```

**在斯卡拉**:

```scala
val env = StreamExecutionEnvironment.getExecutionEnvironment
env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime)
//or
env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime)
```

对于事件时间流程序，指定分配水印和时间戳的方式。分配水印和时间戳有两种方式:

*   直接从数据源属性
*   使用时间戳受理人

要使用事件时间流，请按如下方式分配时间特性:

**在 Java 中**:

```scala
final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime;
```

**在斯卡拉**:

```scala
val env = StreamExecutionEnvironment.getExecutionEnvironment
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
```

在源中存储记录时，最好总是存储事件时间。Flink 还支持一些预定义的时间戳提取器和水印生成器。

# 连接器

Apache Flink 支持各种连接器，允许跨各种技术进行数据读/写。

# 卡夫卡连接器

Kafka 是一个发布-订阅分布式消息队列系统，允许用户发布消息到某个主题。然后将这些内容分发给该主题的订阅者。Flink 提供了将卡夫卡消费者定义为 Flink 流中的数据源的选项。要使用 Flink Kafka 连接器，必须使用特定的 JAR 文件。

使用以下 Maven 依赖项来使用连接器。例如，对于卡夫卡 0.9 版本，在`pom.xml`中添加以下依赖项:

```scala
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-kafka-0.9_2.11/artifactId>
    <version>1.1.4</version>
</dependency>
```

现在，让我们看看如何使用卡夫卡消费者作为卡夫卡的来源:

**在 Java 中**:

```scala
Properties properties = new Properties();
properties.setProperty("bootstrap.servers", "localhost:9092");
properties.setProperty("group.id", "test");
DataStream<String> input = env.addSource(new
FlinkKafkaConsumer09<String>("mytopic", new SimpleStringSchema(), properties));
```

**在斯卡拉**:

```scala
val properties = new Properties();
properties.setProperty("bootstrap.servers", "localhost:9092");
// only required for Kafka 0.8
properties.setProperty("zookeeper.connect", "localhost:2181");
properties.setProperty("group.id", "test");
stream = env
.addSource(new FlinkKafkaConsumer09[String]("mytopic", new
SimpleStringSchema(), properties))
.print
```

在前面的代码中，我们首先设置了 Kafka 主机以及 zookeeper 主机和端口的属性。然后，我们指定了主题名称，在本例中为`mytopic`。因此，如果任何消息发布到`mytopic`主题，它们将由 Flink 流处理。

如果以不同的格式获取数据，还可以为反序列化指定自定义模式。默认情况下，Flink 支持字符串和 JSON 反序列化程序。要启用容错，请在 Flink 中启用检查点。弗林克定期拍摄该州的快照。如果出现故障，它会恢复到最后一个检查点并重新开始处理。你也可以把卡夫卡制作人定义为一个水槽。这将数据写入了卡夫卡的主题。要将数据写入卡夫卡的主题:

**在 Java 中**:

```scala
stream.addSink(new FlinkKafkaProducer09[String]("localhost:9092", "mytopic", new SimpleStringSchema()))
```

**在斯卡拉**:

```scala
stream.addSink(new FlinkKafkaProducer09<String>("localhost:9092", "mytopic", new SimpleStringSchema()));
```

# Twitter 连接器

随着社交媒体和社交网站日益强大，能够从推特获取数据并进行处理变得至关重要。Twitter 数据可以用来对各种产品、服务、应用等做情感分析。

Flink 提供了 Twitter 连接器作为一个数据源。要使用连接器，请使用您的 Twitter 帐户创建一个 Twitter 应用程序，并生成连接器要使用的身份验证密钥。

Twitter 连接器可以使用 Java 或 Scala API。生成令牌后，您可以编写一个程序从 Twitter 获取数据，如下所示:

1.  首先，添加一个 Maven 依赖项:

```scala
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-connector-twitter_2.11/artifactId>
<version>1.1.4</version>
</dependency>
```

2.  接下来，添加 Twitter 作为数据源:

**在 Java 中**:

```scala
Properties props = new Properties();
props.setProperty(TwitterSource.CONSUMER_KEY, "");
props.setProperty(TwitterSource.CONSUMER_SECRET, "");
props.setProperty(TwitterSource.TOKEN, "");
props.setProperty(TwitterSource.TOKEN_SECRET, "");
DataStream<String> streamSource = env.addSource(new TwitterSource(props));
```

**在斯卡拉**:

```scala
val props = new Properties();
props.setProperty(TwitterSource.CONSUMER_KEY, "");
props.setProperty(TwitterSource.CONSUMER_SECRET, "");
props.setProperty(TwitterSource.TOKEN, "");
props.setProperty(TwitterSource.TOKEN_SECRET, "");
DataStream<String> streamSource = env.addSource(new TwitterSource(props));
```

在前面的代码中，我们首先为获得的令牌设置属性，然后添加`TwitterSource`。如果给定的信息是正确的，开始从推特获取数据。`TwitterSource`以 JSON 字符串格式发出数据。推特 JSON 示例如下所示:

```scala
{
...
"text": ""Loyalty 3.0: How to Revolutionize Customer &amp; Employee
Engagement with Big Data &amp; #Gamification" can be ordered here:
http://t.co/1XhqyaNjuR",
"geo": null,
"retweeted": false,
"in_reply_to_screen_name": null,
"possibly_sensitive": false,
"truncated": false,
"lang": "en",
"hashtags": [{
"text": "Gamification",
"indices": [90,
103]
}],
},
"in_reply_to_status_id_str": null,
"id": 330094515484508160
...
}
```

`TwitterSource`提供各种`StatusesSampleEndpoint`，返回一组随机推文。如果需要添加一些过滤器，又不想使用默认端点，可以实现`TwitterSource.EndpointInitializer`界面。

一旦你从推特上获取数据，你就可以处理、存储或分析数据。

# 兔子 MQ 连接器

RabbitMQ 是一个广泛使用的分布式高性能消息队列系统。它被用作高吞吐量操作的消息传递系统。它允许您创建分布式消息队列，并在队列中包含发布者和订阅者。更多关于 RabbitMQ 的信息，请访问[https://www.rabbitmq.com/](https://www.rabbitmq.com/)。

Flink 支持从 RabbitMQ 获取和发布数据。它提供了一个连接器，可以作为数据流的数据源。

要使 RabbitMQ 连接器正常工作，您必须提供以下信息:

*   **rabbtmq**:主机、端口、用户凭证等配置。
*   **队列**:您希望订阅的 RabbitMQ 队列名称。
*   **关联 ID**:这是一个 RabbitMQ 特性，用于在分布式世界中通过唯一的 ID 关联请求和响应。Flink RabbitMQ 连接器提供了一个接口，根据您是否使用它，将其设置为`true`或`false`。
*   **反序列化模式**:rabbtmq 以序列化的方式存储和传输数据，避免网络流量。因此，当收到消息时，订户知道如何反序列化消息。Flink 连接器为我们提供了一些默认的反序列化程序，比如字符串反序列化程序。

RabbitMQ 源为我们提供了以下关于流交付的选项:

*   **恰好一次**:使用 Rabbtmq 相关标识和 Flink 检查点机制处理 Rabbtmq 事务
*   **至少一次**:当启用了 Flink 检查点但没有设置 RabbitMQ 相关标识时

RabbitMQ 自动提交模式没有强有力的交付保证。

现在让我们编写一个代码来让这个连接器工作。像其他连接器一样，向代码中添加一个 Maven 依赖项:

```scala
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-connector-rabbitmq_2.11/artifactId>
<version>1.1.4</version>
</dependency>
```

下面的代码片段展示了如何在 Java 中使用 RabbitMQ 连接器:

```scala
//Configurations
RMQConnectionConfig connectionConfig = new RMQConnectionConfig.Builder()
.setHost(<host>).setPort(<port>).setUserName(..)
.setPassword(..).setVirtualHost("/").build();

//Get Data Stream without correlation ids
DataStream<String> streamWO = env.addSource(new
RMQSource<String>(connectionConfig, "my-queue", new SimpleStringSchema()))
.print

//Get Data Stream with correlation ids
DataStream<String> streamW = env.addSource(new
RMQSource<String>(connectionConfig, "my-queue", true, new
SimpleStringSchema()))
.print
```

同样，在 Scala 中，代码可以编写如下:

```scala
val connectionConfig = new RMQConnectionConfig.Builder()
.setHost(<host>).setPort(<port>).setUserName(..)
.setPassword(..).setVirtualHost("/").build()
streamsWOIds = env.addSource(new RMQSource[String](connectionConfig, " my-queue", new SimpleStringSchema))
.print
streamsWIds = env.addSource(new RMQSource[String](connectionConfig, "my-queue", true, new SimpleStringSchema))
.print
```

您也可以使用 RabbitMQ 连接器作为 Flink 接收器。

要将进程发送回不同的 RabbitMQ 队列，请提供三个重要配置:

*   rabbitmq 配置
*   队列名称–将处理后的数据发送回哪里
*   序列化模式–Rabbtmq 将数据转换为字节的模式

下面是用 Java 编写的示例代码，展示了如何将这个连接器用作 Flink 接收器:

```scala
RMQConnectionConfig connectionConfig = new RMQConnectionConfig.Builder()
.setHost(<host>).setPort(<port>).setUserName(..)
.setPassword(..).setVirtualHost("/").build();
stream.addSink(new RMQSink<String>(connectionConfig, "target-queue", new StringToByteSerializer()));
```

同样的事情也可以在 Scala 中完成:

```scala
val connectionConfig = new RMQConnectionConfig.Builder()
.setHost(<host>).setPort(<port>).setUserName(..)
.setPassword(..).setVirtualHost("/").build()
stream.addSink(new RMQSink[String](connectionConfig, "target-queue", new StringToByteSerializer
```

# Elasticsearch Connector

Elasticsearch 是一个分布式、低延迟的全文搜索引擎系统，它允许您对自己选择的文档进行索引，然后对文档集进行全文搜索。要了解更多关于弹性搜索的信息，请参见[https://www.elastic.co](https://www.elastic.co)。

在许多情况下，您可能希望使用 Flink 处理数据，然后将其存储在弹性搜索中。为此，Flink 支持弹性搜索连接器。到目前为止，Elasticsearch 已经发布了两个主要版本。弗林克支持他们两个。对于 Elasticsearch 1.x，需要添加以下 Maven 依赖项:

```scala
<dependency>
\<groupId>org.apache.flink</groupId>
<artifactId>flink-connector-elasticsearch_2.11</artifactId>
<version>1.1.4</version>
</dependency>
```

Flink 连接器提供了一个将数据写入弹性搜索的接收器。它使用两种方法连接到弹性搜索:

*   **嵌入式节点模式**:在嵌入式节点模式下，接收器使用 BulkProcessor 将文档发送到 ElasticSearch。您可以配置在将文档发送到弹性搜索之前要缓冲多少请求。以下是代码片段:

```scala
DataStream<String> input = ...;
Map<String, String> config = Maps.newHashMap();
config.put("bulk.flush.max.actions", "1");
config.put("cluster.name", "cluster-name");
input.addSink(new ElasticsearchSink<>(config, new
IndexRequestBuilder<String>() {
@Override
public IndexRequest createIndexRequest(String element, RuntimeContext ctx) {
    Map<String, Object> json = new HashMap<>();
    json.put("data", element);
    return Requests.indexRequest()
    .index("my-index")
    .type("my-type")
    .source(json);
}
}));
```

在前面的代码片段中，我们创建了一个哈希映射，其中包含一些配置，例如集群名称以及在发送请求之前要缓冲多少文档。然后，我们将接收器添加到流中，指定要存储的索引、类型和文档。同样，Scala 中的代码如下:

```scala
val input: DataStream[String] = ...
val config = new util.HashMap[String, String]
config.put("bulk.flush.max.actions", "1")
config.put("cluster.name", "cluster-name")
text.addSink(new ElasticsearchSink(config, new IndexRequestBuilder[String]
{
    override def createIndexRequest(element: String, ctx: RuntimeContext):
    IndexRequest = {
        val json = new util.HashMap[String, AnyRef]
        json.put("data", element)
        Requests.indexRequest.index("my-index").`type`("my-type").source(json)
    }
}))
```

*   **传输客户端模式**:弹性搜索允许通过端口`9300`上的传输客户端进行连接。Flink 支持通过其连接器使用这些连接。在配置中指定集群中存在的所有弹性搜索节点。以下是 Java 中的代码片段:

```scala
DataStream<String> input = ...;
Map<String, String> config = Maps.newHashMap();
config.put("bulk.flush.max.actions", "1");
config.put("cluster.name", "cluster-name");
List<TransportAddress> transports = new ArrayList<String>();
transports.add(new InetSocketTransportAddress("es-node-1", 9300));
transports.add(new InetSocketTransportAddress("es-node-2", 9300));
transports.add(new InetSocketTransportAddress("es-node-3", 9300));
input.addSink(new ElasticsearchSink<>(config, transports, new
IndexRequestBuilder<String>() {
@Override
public IndexRequest createIndexRequest(String element, RuntimeContext ctx) {
Map<String, Object> json = new HashMap<>();
json.put("data", element);
return Requests.indexRequest()
.index("my-index")
.type("my-type")
.source(json);
}
}));
```

在这里，我们还提供了关于集群名称、节点、端口、批量发送的最大请求数等详细信息。Scala 中类似的代码可以编写如下:

```scala
val input: DataStream[String] = ...
val config = new util.HashMap[String, String]
config.put("bulk.flush.max.actions", "1")
config.put("cluster.name", "cluster-name")
val transports = new ArrayList[String]
transports.add(new InetSocketTransportAddress("es-node-1", 9300))
transports.add(new InetSocketTransportAddress("es-node-2", 9300))
transports.add(new InetSocketTransportAddress("es-node-3", 9300))
text.addSink(new ElasticsearchSink(config, transports, new
IndexRequestBuilder[String] {
override def createIndexRequest(element: String, ctx: RuntimeContext):
IndexRequest = {
val json = new util.HashMap[String, AnyRef]
json.put("data", element)
Requests.indexRequest.index("my-index").`type`("my-type").source(json)
}
}))
```

# 卡珊德拉连接器

Cassandra 是一个分布式、低延迟的 NoSQL 数据库。这是一个基于键值的数据库。许多高吞吐量应用程序使用 Cassandra 作为它们的主数据库。Cassandra 采用分布式集群模式，没有主从架构。任何节点都可以支持读写。更多关于卡珊德拉的信息，请访问[http://cassandra.apache.org](http://cassandra.apache.org)。

Apache Flink 提供了一个连接器，可以将数据写入 Cassandra。在许多应用程序中，人们可能希望在 Cassandra 中存储来自 Flink 的流数据。

像其他连接器一样，为了获得这一点，我们需要将其作为 Maven 依赖项添加:

```scala
<dependency>
<groupId>org.apache.flink</groupId>
<artifactId>flink-connector-cassandra_2.11</artifactId>
<version>1.1.4</version>
</dependency>
```

添加依赖项后，添加 Cassandra 接收器及其配置，如下所示:

**在 Java 中**:

```scala
CassandraSink.addSink(input)
.setQuery("INSERT INTO cep.events (id, message) values (?, ?);")
.setClusterBuilder(new ClusterBuilder() {
@Override
public Cluster buildCluster(Cluster.Builder builder) {
return builder.addContactPoint("127.0.0.1").build();
}
})
.build()
```

**在斯卡拉**:

前面的代码将数据流写入名为**事件**的表中。该表需要一个事件标识和一条消息:

```scala
CassandraSink.addSink(input)
.setQuery("INSERT INTO cep.events (id, message) values (?, ?);")
.setClusterBuilder(new ClusterBuilder() {
@Override
public Cluster buildCluster(Cluster.Builder builder) {
return builder.addContactPoint("127.0.0.1").build();
}
)
.build();
```

# 摘要

在这一章中，我们了解了 Flink 最强大的 API——DataStream API；数据源、转换和接收器如何协同工作；以及关于各种技术连接器，比如 Elasticsearch、Cassandra、Kafka、RabbitMQ 等等。在本章中，我们还讨论了使用 Apache Flink 的流处理。

在下一章中，我们将转换话题，看看可视化数据最令人兴奋的领域之一。