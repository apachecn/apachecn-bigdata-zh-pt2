# 六、Apache Spark 批处理分析

在本章中，您将了解 Apache Spark 以及如何将其用于基于批处理模型的大数据分析。Spark SQL 是 Spark Core 之上的一个组件，可用于查询结构化数据。它正在成为事实上的工具，取代 Hive 成为 Hadoop 上批处理分析的选择。

此外，您将学习如何使用 Spark 来分析结构化数据(非结构化数据，如包含任意文本的文档，或必须转换为结构化形式的其他格式)。我们将在这里看到数据框架/数据集是如何成为基石的，以及 SparkSQL 的 API 如何使查询结构化数据变得简单而健壮。

我们还将介绍数据集，并了解数据集、数据框架和关系数据库之间的区别。简而言之，本章将涵盖以下主题:

*   迷你图 SQL 和数据框
*   数据框架和 SQL 应用编程接口
*   数据框模式
*   数据集和编码器
*   加载和保存数据
*   聚集
*   连接

# 迷你图 SQL 和数据框

在 Apache Spark 之前，Apache Hive 是任何人想要对大量数据运行类似于 SQL 的查询时的首选技术。Apache Hive 本质上是将一个 SQL 查询翻译成 MapReduce，就像逻辑自动使对大数据执行多种分析变得非常容易，而无需实际学习用 Java 和 Scala 编写复杂的代码。

随着 Apache Spark 的出现，我们如何在大数据规模上执行分析发生了范式转变。Spark SQL 在 Apache Spark 的分布式计算能力之上提供了一个类似 SQL 的层，使用起来相当简单。事实上，Spark SQL 可以用作在线分析处理数据库。Spark SQL 的工作原理是将类似 SQL 的语句解析成**抽象语法树** ( **AST** )，随后将该计划转换为逻辑计划，然后将逻辑计划优化为可执行的物理计划，如下图所示:

![](assets/f82a0936-cde9-436d-a76e-0471d37f51bc.png)

最终的执行使用底层的数据框架应用编程接口，通过简单地使用类似于 SQL 的接口，而不是学习所有的内部，任何人都可以非常容易地使用数据框架应用编程接口。由于本书深入探讨了各种应用编程接口的技术细节，我们将主要介绍数据框架应用编程接口，在一些地方展示了火花 SQL 应用编程接口，以对比使用这些应用编程接口的不同方式。因此，数据框架应用编程接口是火花 SQL 下面的底层。在本章中，我们将向您展示如何使用各种技术创建数据框，包括 SQL 查询和对数据框执行操作。

数据框架是对**弹性分布式数据集** ( **RDD** )的抽象，处理使用催化剂优化器优化的更高级功能，并且通过钨计划也是高性能的。

自成立以来，钨项目一直是火花执行引擎的最大变化。它的主要焦点在于提高 Spark 应用程序的 CPU 和内存效率。该项目包括三项举措:

*   内存管理和二进制处理
*   缓存感知计算
*   代码生成

For more information, you can check out [https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html).

您可以将数据集视为 RDD 上的一个高效表，它具有高度优化的数据二进制表示。二进制表示是使用编码器实现的，编码器将各种对象序列化为二进制结构，性能比 RDD 表示好得多。因为数据框在内部使用 RDD，所以数据框/数据集也像 RDD 一样分布，因此也是一个分布式数据集。显然，这也意味着数据集是不可变的。

以下是数据的二进制表示的说明:

![](assets/bc4b3781-6d28-4518-adb8-7aaa8aa4b450.png)

数据集是在 Spark 1.6 中添加的，并提供了在数据框之上进行强类型化的优势。事实上，由于 Spark 2.0，数据框只是数据集的别名。

[http://spark.apache.org/sql/](http://spark.apache.org/sql/) defines the DataFrame type as a *Dataset[Row]*, which means that most of the APIs will work well with both dataset and *DataFrame.type DataFrame = Dataset[Row]*.

数据框在概念上类似于关系数据库中的表。因此，数据帧包含多行数据，每行由几列组成。我们需要记住的第一件事是，就像关系数据库一样，数据帧也是不可变的。数据帧不可变的这一特性意味着每次转换或操作都会创建一个新的数据帧。

让我们从更多地研究数据帧以及它们与关系数据库有什么不同开始。如前所述，RDDs 代表了 Apache Spark 中数据操作的低级 API。数据框架是在关系数据库之上创建的，以抽象关系数据库的低级内部工作方式，并公开更易于使用的高级应用编程接口，并提供大量现成的功能。DataFrame 是按照 Python pandas 包、R 语言、Julia 语言等类似概念创建的。

正如我们之前提到的，数据框架将 SQL 代码和特定于域的语言表达式转换成优化的执行计划，在 Spark Core APIs 之上运行，以便 SQL 语句执行各种各样的操作。数据框支持许多不同类型的输入数据源和许多类型的操作。这包括所有类型的 SQL 操作，如连接、分组依据、聚合和窗口函数。

Spark SQL 也非常类似于 Hive 查询语言，由于 Spark 为 Apache Hive 提供了一个自然的适配器，所以一直在 Apache Hive 工作的用户可以轻松地将自己的知识转移并应用到 Spark SQL 中，从而最大限度地减少转换时间。如前所述，数据帧本质上依赖于表的概念。

该表的操作方式与 Apache Hive 的工作方式非常相似。事实上，Apache Spark 中对表的许多操作类似于 Apache Hive 处理表和对表进行操作的方式。一旦您有了一个作为数据框的表，数据框就可以注册为一个表，并且您可以使用 Spark SQL 语句代替数据框 API 来操作数据。

数据框架取决于催化剂优化器和钨性能的提高，所以让我们简单地研究一下催化剂优化器是如何工作的。catalyst 优化器根据输入的 SQL 创建一个解析的逻辑计划，然后通过查看 SQL 语句中使用的各种属性和列来分析逻辑计划。一旦分析的逻辑计划被创建，catalyst 优化器通过组合几个操作并重新安排逻辑以获得更好的性能来进一步尝试优化计划。

In order to understand the catalyst optimizer, think about it as a common sense logic optimizer which can reorder operations such as filters and transformations, sometimes grouping several operations into one so as to minimize the amount of data that is shuffled across the worker nodes. For example, the catalyst optimizer may decide to broadcast the smaller datasets when performing joint operations between different datasets. Use explain to look at the execution plan of any DataFrame. The catalyst optimizer also computes statistics of the DataFrames columns and partitions improving the speed of execution.

例如，如果数据分区上有转换和过滤器，那么我们过滤数据和应用转换的顺序对操作的整体性能非常重要。作为所有优化的结果，生成优化的逻辑计划，然后将其转换为物理计划。

显然，几个物理计划可以执行相同的 SQL 语句并生成相同的结果。成本优化逻辑基于成本优化和估计来确定和挑选好的物理计划。与之前的版本(如 Spark 1.6 或更早版本)相比，Spark 2.x 提供了显著的性能提升，钨性能的提升是其背后的另一个关键因素。

钨实现了对内存管理和其他性能改进的全面检修。最重要的内存管理改进使用对象的二进制编码，并在堆外和堆内内存中引用它们。因此，通过使用二进制编码机制对所有对象进行编码，钨允许使用办公室堆内存。二进制编码对象占用的内存少得多。

项目钨也提高洗牌性能。数据通常通过`DataFrameReader`加载到数据框中，数据通过`DataFrameWriter`从数据框中保存。

# 数据框架应用编程接口和 SQL 应用编程接口

数据框可以通过几种方式创建；其中一些如下:

*   执行 SQL 查询，加载外部数据，如 Parquet、JSON、CSV、Text、Hive、JDBC 等
*   将关系数据库转换为数据帧
*   加载一个 CSV 文件

我们将在这里看一下`statesPopulation.csv`，然后我们将它作为数据帧加载。

CSV 包含 2010 年至 2016 年美国各州人口的以下格式:

| **状态** | **年** | **人口** |
| 亚拉巴马州 | Two thousand and ten | 47,85,492 |
| 阿拉斯加 | Two thousand and ten | Seven hundred and fourteen thousand and thirty-one |
| 亚利桑那州 | Two thousand and ten | 64,08,312 |
| 阿肯色州 | Two thousand and ten | Two million nine hundred and twenty-one thousand nine hundred and ninety-five |
| 加利福尼亚 | Two thousand and ten | Thirty-seven million three hundred and thirty-two thousand six hundred and eighty-five |

由于这个 CSV 有一个头，我们可以使用它来快速加载到带有隐式模式检测的数据帧中:

```
scala> val statesDF = spark.read.option("header",
"true").option("inferschema", "true").option("sep",
",").csv("statesPopulation.csv")
statesDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 1
more field]
```

一旦我们加载了数据帧，就可以检查它的模式:

```
scala> statesDF.printSchema
root
|-- State: string (nullable = true)
|-- Year: integer (nullable = true)
|-- Population: integer (nullable = true)
```

`option("header", "true").option("inferschema", "true").option("sep", ",")` tells Spark that the CSV has a header; a comma separator is used to separate the fields/columns and also that schema can be inferred implicitly.

DataFrame 的工作原理是解析逻辑计划、分析逻辑计划、优化计划，然后最终执行物理执行计划。

使用数据框上的解释显示执行计划:

```
scala> statesDF.explain(true)
== Parsed Logical Plan ==
Relation[State#0,Year#1,Population#2] csv
== Analyzed Logical Plan ==
State: string, Year: int, Population: int
Relation[State#0,Year#1,Population#2] csv
== Optimized Logical Plan ==
Relation[State#0,Year#1,Population#2] csv
== Physical Plan ==
*FileScan csv [State#0,Year#1,Population#2] Batched: false, Format: CSV,
Location: InMemoryFileIndex[file:/Users/salla/states.csv],
PartitionFilters: [], PushedFilters: [], ReadSchema:
struct<State:string,Year:int,Population:int>
```

数据框也可以注册为表名(如下所示)，这样就可以像关系数据库一样键入 SQL 语句:

```
scala> statesDF.createOrReplaceTempView("states")
```

一旦我们将数据框作为结构化数据框或表，我们就可以运行命令对数据进行操作:

```
scala> statesDF.show(5)
scala> spark.sql("select * from states limit 5").show
+----------+----+----------+
| State|Year|Population|
+----------+----+----------+
| Alabama|2010| 4785492|
| Alaska|2010| 714031|
| Arizona|2010| 6408312|
| Arkansas|2010| 2921995|
|California|2010| 37332685|
+----------+----+----------+
```

如果您在前面的代码中看到，我们已经编写了一个类似于 SQL 的语句，并使用`spark.sql` API 执行了它。

Note that the Spark SQL is simply converted to the DataFrame API for execution and the SQL is only a DSL for ease of use.

使用数据框上的`sort`操作，可以按任意列对数据框中的行进行排序。我们看到使用`Population`列进行降序排序的效果如下。这些行由`Population`按降序排列:

```
scala> statesDF.sort(col("Population").desc).show(5)
scala> spark.sql("select * from states order by Population desc limit
5").show
+----------+----+----------+
| State|Year|Population|
 +----------+----+----------+
|California|2016| 39250017|
|California|2015| 38993940|
|California|2014| 38680810|
|California|2013| 38335203|
|California|2012| 38011074|
+----------+----+----------+
```

使用`groupBy`我们可以按任意列对数据帧进行分组。以下是通过`State`对行进行分组，然后对每个`State`累加`Population`计数的代码:

```
scala> statesDF.groupBy("State").sum("Population").show(5)
scala> spark.sql("select State, sum(Population) 
from states group by State
limit 5").show
+---------+---------------+
| State|sum(Population)|
+---------+---------------+
| Utah| 20333580|
| Hawaii| 9810173|
|Minnesota| 37914011|
| Ohio| 81020539|
| Arkansas| 20703849|
+---------+---------------+
```

使用`agg`操作，您可以对数据框的列执行许多不同的操作，例如查找列的`min`、`max`和`avg`。您还可以同时执行该操作并重命名该列，以适合您的用例:

```
scala>
statesDF.groupBy("State").agg(sum("Population").alias("Total")).show(5)
scala> spark.sql("select State, sum(Population) as Total from states group
by State limit 5").show
+---------+--------+
| State| Total|
+---------+--------+
| Utah|20333580|
| Hawaii| 9810173|
|Minnesota|37914011|
| Ohio|81020539|
| Arkansas|20703849|
+---------+--------+
```

自然，逻辑越复杂，执行计划也就越复杂。让我们看看`groupBy`和`agg` API 调用的前一个操作的计划，以便更好地理解幕后到底发生了什么。以下是显示`group by`条款执行计划和每个`State`人口总和的代码:

```
scala>
statesDF.groupBy("State").agg(sum("Population").alias("Total")).explain(true)
== Parsed Logical Plan ==
'Aggregate [State#0], [State#0, sum('Population) AS Total#31886]
+- Relation[State#0,Year#1,Population#2] csv
== Analyzed Logical Plan ==
State: string, Total: bigint
Aggregate [State#0], [State#0, sum(cast(Population#2 as bigint)) AS
Total#31886L]
+- Relation[State#0,Year#1,Population#2] csv
== Optimized Logical Plan ==
Aggregate [State#0], [State#0, sum(cast(Population#2 as bigint)) AS
Total#31886L]
+- Project [State#0, Population#2]
+- Relation[State#0,Year#1,Population#2] csv
== Physical Plan ==
*HashAggregate(keys=[State#0], functions=[sum(cast(Population#2 as
bigint))], output=[State#0, Total#31886L])
+- Exchange hashpartitioning(State#0, 200)
+- *HashAggregate(keys=[State#0], functions=[partial_sum(cast(Population#2
as bigint))], output=[State#0, sum#31892L])
+- *FileScan csv [State#0,Population#2] Batched: false, Format: CSV,
Location: InMemoryFileIndex[file:/Users/salla/states.csv],
PartitionFilters: [], PushedFilters: [], ReadSchema:
struct<State:string,Population:int>
```

数据框操作可以很好地链接在一起，这样执行就可以利用成本优化(钨性能改进和催化剂优化器一起工作)。我们还可以在一条语句中将操作链接在一起，如下所示，其中我们不仅按`State`列对数据进行分组，然后对`Population`值求和，还按求和列对数据帧进行排序:

```
scala>
statesDF.groupBy("State").agg(sum("Population").alias("Total")).sort(col("Total").desc).show(5)
scala> spark.sql("select State, sum(Population) as Total from states group
by State order by Total desc limit 5").show
+----------+---------+
| State| Total|
+----------+---------+
|California|268280590
| Texas|185672865|
| Florida|137618322|
| New York|137409471|
| Illinois| 89960023|
+----------+---------+
```

前面的链式操作由多个转换和动作组成，
可以使用下图可视化:

![](assets/5b53a25d-b856-4dca-bde6-7338f4954033.png)

也可以同时创建多个聚合，如下所示:

```
scala> statesDF.groupBy("State").agg(
min("Population").alias("minTotal"),
max("Population").alias("maxTotal"),
avg("Population").alias("avgTotal"))
.sort(col("minTotal").desc).show(5)
scala> spark.sql("select State, min(Population) as minTotal,
max(Population) as maxTotal, avg(Population) as avgTotal from states group
by State order by minTotal desc limit 5").show
+----------+--------+--------+--------------------+
| State|minTotal|maxTotal| avgTotal|
+----------+--------+--------+--------------------+
|California|37332685|39250017|3.8325798571428575E7|
| Texas|25244310|27862596| 2.6524695E7|
| New York|19402640|19747183| 1.962992442857143E7|
| Florida|18849098|20612439|1.9659760285714287E7|
| Illinois|12801539|12879505|1.2851431857142856E7|
+----------+--------+--------+--------------------+
```

# 中心

为了创建更适合执行多个汇总和聚合的不同视图，转换表的最佳方法之一是**旋转**。我们可以通过取一个列的值并使每个值成为一个实际的列来实现这一点。

让我们借助一个例子更好地理解这一点。我们将按`Year`旋转数据帧的行，并检查结果。我们现在获得的结果描述了来自`Year`列的值，每个值都形成了一个新列。这样做的最终结果是，我们可以使用`Year`创建的年度列进行汇总和汇总，而不仅仅是查看年度列:

```
scala> statesDF.groupBy("State").pivot("Year").sum("Population").show(5)
+---------+--------+--------+--------+--------+--------+--------+--------+
| State| 2010| 2011| 2012| 2013| 2014| 2015| 2016|
+---------+--------+--------+--------+--------+--------+--------+--------+
| Utah| 2775326| 2816124| 2855782| 2902663| 2941836| 2990632| 3051217|
| Hawaii| 1363945| 1377864| 1391820| 1406481| 1416349| 1425157| 1428557|
|Minnesota| 5311147| 5348562| 5380285| 5418521| 5453109| 5482435| 5519952|
| Ohio|11540983|11544824|11550839|11570022|11594408|11605090|11614373|
| Arkansas| 2921995| 2939493| 2950685| 2958663| 2966912| 2977853| 2988248|
+---------+--------+--------+--------+--------+--------+--------+--------+
```

# 过滤

数据框也支持筛选，可以通过筛选数据框行来生成新的数据框。`Filter`实现了一个非常重要的数据转换，将数据框架缩小到我们的用例。让我们看一下数据帧过滤的执行计划，只考虑`California`的状态:

```
scala> statesDF.filter("State == 'California'").explain(true)
== Parsed Logical Plan ==
'Filter ('State = California)
+- Relation[State#0,Year#1,Population#2] csv
== Analyzed Logical Plan ==
State: string, Year: int, Population: int
Filter (State#0 = California)
+- Relation[State#0,Year#1,Population#2] csv
== Optimized Logical Plan ==
Filter (isnotnull(State#0) && (State#0 = California))
+- Relation[State#0,Year#1,Population#2] csv
== Physical Plan ==
*Project [State#0, Year#1, Population#2]
+- *Filter (isnotnull(State#0) && (State#0 = California))
+- *FileScan csv [State#0,Year#1,Population#2] Batched: false, Format:
CSV, Location: InMemoryFileIndex[file:/Users/salla/states.csv],
PartitionFilters: [], PushedFilters: [IsNotNull(State),
EqualTo(State,California)], ReadSchema:
struct<State:string,Year:int,Population:int>
```

现在我们已经看到了执行计划，现在让我们执行`filter`命令如下:

```
scala> statesDF.filter("State == 'California'").show
+----------+----+----------+
| State|Year|Population|
+----------+----+----------+
|California|2010| 37332685|
|California|2011| 37676861|
|California|2012| 38011074|
|California|2013| 38335203|
|California|2014| 38680810|
|California|2015| 38993940|
|California|2016| 39250017|
+----------+----+----------+
```

# 用户定义的函数

**用户定义函数** ( **UDFs** )定义了新的基于列的函数，扩展了 Spark SQL 的功能。当 Spark 中的内置函数无法处理我们的需求时，创建 UDF 会有所帮助。

`udf()` internally calls a case class `UserDefinedFunction` which in turn calls `ScalaUDF` internally.

让我们来看一个简单地将`State`列值转换为大写的 UDF 的例子。首先，我们在 Scala 中创建我们需要的函数，如以下代码片段所示:

```
import org.apache.spark.sql.functions._
scala> val toUpper: String => String = _.toUpperCase
toUpper: String => String = <function1>
```

然后我们必须将创建的函数封装在`udf`中，以创建 UDF:

```
scala> val toUpperUDF = udf(toUpper)
toUpperUDF: org.apache.spark.sql.expressions.UserDefinedFunction =
UserDefinedFunction(<function1>,StringType,Some(List(StringType)))
```

现在我们已经创建了`udf`，我们可以使用它将`State`列转换为大写:

```
scala> statesDF.withColumn("StateUpperCase",
toUpperUDF(col("State"))).show(5)
+----------+----+----------+--------------+
| State|Year|Population|StateUpperCase|
+----------+----+----------+--------------+
| Alabama|2010| 4785492| ALABAMA|
| Alaska|2010| 714031| ALASKA|
| Arizona|2010| 6408312| ARIZONA|
| Arkansas|2010| 2921995| ARKANSAS|
|California|2010| 37332685| CALIFORNIA|
+----------+----+----------+--------------+
```

# 模式-数据结构

模式是对数据结构的描述，可以是隐式的，也可以是显式的。将现有关系数据库转换为数据集有两种主要方法，因为数据框架在内部基于 RDDs 它们如下:

*   用反射来推断 RDD 的图式
*   通过一个编程接口，在这个接口的帮助下，您可以获取一个现有的 RDD 并呈现一个模式，从而将 RDD 转换为一个具有模式的数据集

# 隐式模式

让我们看一个例子，将一个**逗号分隔值** ( **CSV** )文件加载到一个数据帧中。每当文本文件包含标题时，读取应用编程接口就可以通过读取标题行来推断模式。我们还可以选择指定用于拆分文本文件行的分隔符。

我们从标题行读取`csv`推断模式，并使用逗号(`,`)作为分隔符。我们还展示了使用`schema`命令和`printSchema`命令来验证输入文件的模式:

```
scala> val statesDF = spark.read.option("header", "true")
 .option("inferschema", "true")
 .option("sep", ",")
 .csv("statesPopulation.csv")
statesDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 1
more field]
scala> statesDF.schema
res92: org.apache.spark.sql.types.StructType = StructType(
StructField(State,StringType,true),
StructField(Year,IntegerType,true),
StructField(Population,IntegerType,true))
scala> statesDF.printSchema
root
|-- State: string (nullable = true)
|-- Year: integer (nullable = true)
|-- Population: integer (nullable = true)
```

# 显式模式

使用`StructType`描述模式，T0 是`StructField`对象的集合。

`StructType` and `StructField` belong to the 
`org.apache.spark.sql.types` package. `DataTypes` such as `IntegerType` and `StringType` also belong to the `org.apache.spark.sql.types` package.

使用这些导入，我们可以定义一个自定义的显式模式。

首先，导入必要的类:

```
scala> import org.apache.spark.sql.types.{StructType, IntegerType,
StringType}
import org.apache.spark.sql.types.{StructType, IntegerType, StringType}
```

定义带有两个列/字段和一个后跟字符串的整数的模式:

```
scala> val schema = new StructType().add("i", IntegerType).add("s",
StringType)
schema: org.apache.spark.sql.types.StructType =
StructType(StructField(i,IntegerType,true), StructField(s,StringType,true))
```

很容易打印刚刚创建的`schema`:

```
scala> schema.printTreeString
root
|-- i: integer (nullable = true)
|-- s: string (nullable = true)
```

还有一个打印 JSON 的选项，如下，使用`prettyJson`功能:

```
scala> schema.prettyJson
res85: String =
{
"type" : "struct",
"fields" : [ {
"name" : "i",
"type" : "integer",
"nullable" : true,
"metadata" : { }
}, {
"name" : "s",
"type" : "string",
"nullable" : true,
"metadata" : { }
} ]
}
```

Spark SQL 的所有数据类型都位于包`org.apache.spark.sql.types`中。

您可以通过以下方式访问它们:

```
import org.apache.spark.sql.types._
```

# 编码器

Spark 2.x 支持为复杂数据类型定义模式的不同方式。首先，让我们看一个简单的例子。`Encoders`必须使用导入语句导入，以便您使用`Encoders`:

```
import org.apache.spark.sql.Encoders
```

让我们看一个简单的例子，将元组定义为数据集 API 中使用的数据类型:

```
scala> Encoders.product[(Integer, String)].schema.printTreeString
root
|-- _1: integer (nullable = true)
|-- _2: string (nullable = true)
```

前面的代码看起来总是很复杂，所以我们也可以为我们的需求定义一个`case class`，然后使用它。

我们可以用两个字段定义一个案例`class Record`，一个`Integer`和一个`String`:

```
scala> case class Record(i: Integer, s: String)
defined class Record
```

使用`Encoders`我们可以很容易地在`case class`之上创建一个模式，从而允许我们轻松地使用各种应用编程接口:

```
scala> Encoders.product[Record].schema.printTreeString
root
|-- i: integer (nullable = true)
|-- s: string (nullable = true)
```

Spark SQL 的所有数据类型都位于包`org.apache.spark.sql.types`中。

您可以通过以下方式访问它们:

```
import org.apache.spark.sql.types._
```

您应该在代码中使用`DataTypes`对象来创建复杂的`Spark SQL`类型，如数组或映射，如下所示:

```
scala> import org.apache.spark.sql.types.DataTypes
import org.apache.spark.sql.types.DataTypes
scala> val arrayType = DataTypes.createArrayType(IntegerType)
arrayType: org.apache.spark.sql.types.ArrayType =
ArrayType(IntegerType,true)
```

以下是 SparkSQL APIs 支持的数据类型:

| **数据类型** | **Scala 中的值类型** | **API 访问
或创建数据类型** |
| `ByteType` | `Byte` | `ByteType` |
| `ShortType` | `Short` | `ShortType` |
| `IntegerType` | `Int` | `IntegerType` |
| `LongType` | `Long` | `LongType` |
| `FloatType` | `Float` | `FloatType` |
| `DoubleType` | `Double` | `DoubleType` |
| `DecimalType ` | `java.math.BigDecimal` | `DecimalType` |
| `StringType` | `String ` | `StringType` |
| `BinaryType` | `Array[Byte] ` | `BinaryType` |
| `BooleanType` | `Boolean` | `BooleanType` |
| `TimestampType` | `java.sql.Timestamp` | `TimestampType` |
| `DateType` | `java.sql.Date ` | `DateType` |
| `ArrayType` | `scala.collection.Seq` | `ArrayType(elementType, [containsNull])` |
| `MapType` | `scala.collection.Map` | `MapType(keyType, valueType,`
`[valueContainsNull])Note`:默认`valueContainsNull`为`true`。 |
| `StructType` | `org.apache.spark.sql.Row` | `StructType(fields).Note`:菲尔兹是`StructFields`的`Seq`。此外，不允许两个字段同名。 |

# 正在加载数据集

Spark SQL 可以通过`DataFrameReader`界面从文件、Hive 表、JDBC 数据库等外部存储系统读取数据。

API 调用的格式为`spark.read.inputtype`:

*   镶木地板
*   战斗支援车
*   蜂巢表
*   JDBC
*   妖魔
*   文本
*   JSON

让我们看几个将 CSV 文件读入数据帧的简单例子:

```
scala> val statesPopulationDF = spark.read.option("header",
"true").option("inferschema", "true").option("sep",
",").csv("statesPopulation.csv")
statesPopulationDF: org.apache.spark.sql.DataFrame = [State: string, Year:
int ... 1 more field]
scala> val statesTaxRatesDF = spark.read.option("header",
"true").option("inferschema", "true").option("sep",
",").csv("statesTaxRates.csv")
statesTaxRatesDF: org.apache.spark.sql.DataFrame = [State: string, TaxRate:
double]
```

# 保存数据集

Spark SQL 可以通过`DataFrameWriter`界面将数据保存到文件、Hive 表和 JDBC 数据库等外部存储系统中。

API 调用的格式为`dataframe.write.outputtype`:

*   镶木地板
*   妖魔
*   文本
*   蜂巢表
*   JSON
*   战斗支援车
*   JDBC

让我们看几个将数据帧写入或保存到 CSV 文件的例子:

```
scala> statesPopulationDF.write.option("header",
"true").csv("statesPopulation_dup.csv")
scala> statesTaxRatesDF.write.option("header",
"true").csv("statesTaxRates_dup.csv")
```

# 聚集

聚合是根据条件将数据收集在一起并对数据进行分析的方法。聚合对于理解各种大小的数据非常重要，因为仅仅拥有原始数据记录对于大多数用例来说并不那么有用。

Imagine a table containing one temperature measurement per day for every city in the world for five years.

例如，如果您看到下表，然后看到相同数据的聚合视图，那么很明显，仅仅原始记录并不能帮助您理解数据。下表显示了原始数据:

| **城市** | **日期** | **温度** |
| 波士顿 | 12/23/2016 | Thirty-two |
| 纽约 | 12/24/2016 | Thirty-six |
| 波士顿 | 12/24/2016 | Thirty |
| 费城 | 12/25/2016 | Thirty-four |
| 波士顿 | 12/25/2016 | Twenty-eight |

下图是每个城市的平均温度:

| **城市** | **平均**T2**气温** |
| 波士顿 | 30 - (32 + 30 + 28)/3 |
| 纽约 | Thirty-six |
| 费城 | Thirty-four |

# 聚合函数

可以在`org.apache.spark.sql.functions`包中找到的函数的帮助下执行聚合。除此之外，还可以创建自定义聚合函数，也称为**用户自定义聚合函数** ( **UDAF** )。

Each grouping operation returns a `RelationalGroupedDataset` on which aggregations can be specified.

我们将加载示例数据来说明本节中所有不同类型的聚合函数:

```
val statesPopulationDF = spark.read.option("header", "true").
 option("inferschema", "true").
 option("sep", ",").csv("statesPopulation.csv")
```

# 数数

Count 是最基本的聚合函数，它只计算指定列的行数。`countDistinct`是`count`的延伸；它还消除了重复。

`count` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```
def count(columnName: String): TypedColumn[Any, Long]
 Aggregate function: returns the number of items in a group.
def count(e: Column): Column
 Aggregate function: returns the number of items in a group.
def countDistinct(columnName: String, columnNames: String*): Column
 Aggregate function: returns the number of distinct items in a group.
def countDistinct(expr: Column, exprs: Column*): Column
 Aggregate function: returns the number of distinct items in a group.
```

让我们看一下在数据框中调用`count`和`countDistinct`来打印行数的一些例子:

```
import org.apache.spark.sql.functions._
scala> statesPopulationDF.select(col("*")).agg(count("State")).show
scala> statesPopulationDF.select(count("State")).show
+------------+
|count(State)|
+------------+
| 350|
+------------+
scala> statesPopulationDF.select(col("*")).agg(countDistinct("State")).show
scala> statesPopulationDF.select(countDistinct("State")).show
+---------------------+
|count(DISTINCT State)|
+---------------------+
| 50|
```

# 第一

获取`RelationalGroupedDataset`中的第一条记录。

`first` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```
def first(columnName: String): Column
 Aggregate function: returns the first value of a column in a group.
def first(e: Column): Column
 Aggregate function: returns the first value in a group.
def first(columnName: String, ignoreNulls: Boolean): Column
 Aggregate function: returns the first value of a column in a group.
def first(e: Column, ignoreNulls: Boolean): Column 
 Aggregate function: returns the first value in a group.
```

让我们看一下在数据帧上调用 first 来输出第一行的例子:

```
import org.apache.spark.sql.functions._
 scala> statesPopulationDF.select(first("State")).show
+-------------------+
|first(State, false)|
+-------------------+
| Alabama|
+-------------------+
```

# 最后的

获取`RelationalGroupedDataset`中的最后一条记录。

`last` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```
def last(columnName: String): Column
 Aggregate function: returns the last value of the column in a group.
def last(e: Column): Column
 Aggregate function: returns the last value in a group.
def last(columnName: String, ignoreNulls: Boolean): Column
 Aggregate function: returns the last value of the column in a group.
def last(e: Column, ignoreNulls: Boolean): Column
 Aggregate function: returns the last value in a group.
```

让我们看一下调用 DataFrame 上的 last 来输出最后一行的例子:

```
import org.apache.spark.sql.functions._
 scala> statesPopulationDF.select(last("State")).show
 +------------------+
 |last(State, false)|
 +------------------+
 | Wyoming|
 +------------------+
```

# 近似 _ 计数 _ 独特

如果您需要不同记录的近似计数，近似不同计数是一种更快的方法，而不是执行通常需要大量洗牌和其他操作的精确计数。

`approx_count_distinct` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```
def approx_count_distinct(columnName: String, rsd: Double): Column
 Aggregate function: returns the approximate number of distinct items in a
 group.
def approx_count_distinct(e: Column, rsd: Double): Column
 Aggregate function: returns the approximate number of distinct items in a group.
def approx_count_distinct(columnName: String): Column
 Aggregate function: returns the approximate number of distinct items in a group.
def approx_count_distinct(e: Column): Column
 Aggregate function: returns the approximate number of distinct items in a group.
```

让我们看一下在数据帧上调用`approx_count_distinct`来打印数据帧的大概计数的例子:

```
import org.apache.spark.sql.functions._
 scala>
 statesPopulationDF.select(col("*")).agg(approx_count_distinct("State")).show
 +----------------------------+
 |approx_count_distinct(State)|
 +----------------------------+
 | 48|
 +----------------------------+
 scala> statesPopulationDF.select(approx_count_distinct("State", 0.2)).show
 +----------------------------+
 |approx_count_distinct(State)|
 +----------------------------+
 | 49|
 +----------------------------+
```

# 部

`min`是数据框中某一列的最小列值。`min`的一个例子就是如果要查一个城市的最低气温。

`min` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```
def min(columnName: String): Column
 Aggregate function: returns the minimum value of the column in a group.
def min(e: Column): Column
 Aggregate function: returns the minimum value of the expression in a group.
```

让我们看一下在数据帧上调用`min`来打印最小值`Population`的例子:

```
import org.apache.spark.sql.functions._
 scala> statesPopulationDF.select(min("Population")).show
 +---------------+
 |min(Population)|
 +---------------+
 | 564513|
+---------------+
```

# 最大

`max`是数据框中某一列的最大列值。这方面的一个例子是，如果你想检查一个城市的最高温度。

`max` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```
def max(columnName: String): Column
 Aggregate function: returns the maximum value of the column in a group.
def max(e: Column): Column
 Aggregate function: returns the maximum value of the expression in a group.
```

让我们看一下在数据框上调用`max`打印最大值`Population`的例子:

```
import org.apache.spark.sql.functions._
 scala> statesPopulationDF.select(max("Population")).show
+---------------+
 |max(Population)|
 +---------------+
 | 39250017|
 +---------------+
```

# 平均值

这些值的平均值是通过将这些值相加并除以值的数量来计算的。

The average of *1*, *2*, *3* is *(1 + 2 + 3) / 3 = 6/3 = z*.

`avg` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```
def avg(columnName: String): Column
 Aggregate function: returns the average of the values in a group.
def avg(e: Column): Column
 Aggregate function: returns the average of the values in a group.
```

让我们看一下在数据框上调用`avg`来打印平均人口的例子:

```
import org.apache.spark.sql.functions._
 scala> statesPopulationDF.select(avg("Population")).show
 +-----------------+
 | avg(Population)|
 +-----------------+
 |6253399.371428572|
 +----------------+
```

# 总和

计算列值的总和。可选地，`sumDistinct`只能用于累加不同的值。

`sum` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```
def sum(columnName: String): Column
 Aggregate function: returns the sum of all values in the given column.
def sum(e: Column): Column
 Aggregate function: returns the sum of all values in the expression.
def sumDistinct(columnName: String): Column
 Aggregate function: returns the sum of distinct values in the expression
def sumDistinct(e: Column): Column
 Aggregate function: returns the sum of distinct values in the expression.
```

让我们看一下在数据帧上调用 sum 来打印 sum(total)`Population`的例子:

```
import org.apache.spark.sql.functions._
scala> statesPopulationDF.select(sum("Population")).show
 +---------------+
 |sum(Population)|
 +---------------+
 | 2188689780|
 +---------------+
```

# 峭度

`kurtosis`是一种量化分布形状差异的方法，在平均值和方差方面看起来非常相似，但实际上是不同的。

`kurtosis` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```
def kurtosis(columnName: String): Column
 Aggregate function: returns the kurtosis of the values in a group.
def kurtosis(e: Column): Column
 Aggregate function: returns the kurtosis of the values in a group.
```

让我们看一个在`Population`列的数据框上调用`kurtosis`的例子:

```
import org.apache.spark.sql.functions._
scala> statesPopulationDF.select(kurtosis("Population")).show
 +--------------------+
 |kurtosis(Population)|
 +--------------------+
 | 7.727421920829375|
 +--------------------+
```

# 歪斜

`skewness`测量数据中的值围绕平均值或平均值的不对称性。

`skewness` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```
def skewness(columnName: String): Column
 Aggregate function: returns the skewness of the values in a group.
def skewness(e: Column): Column
 Aggregate function: returns the skewness of the values in a group.
```

让我们看一下在`Population`列的数据框上调用`skewness`的例子:

```
import org.apache.spark.sql.functions._
 scala> statesPopulationDF.select(skewness("Population")).show
 +--------------------+
 |skewness(Population)|
 +--------------------+
 | 2.5675329049100024|
 +--------------------+
```

# 差异

方差是每个值与平均值的平方差的平均值。

`var` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```
def var_pop(columnName: String): Column
 Aggregate function: returns the population variance of the values in a group.
def var_pop(e: Column): Column
 Aggregate function: returns the population variance of the values in a group.
def var_samp(columnName: String): Column
 Aggregate function: returns the unbiased variance of the values in a group.
def var_samp(e: Column): Column
 Aggregate function: returns the unbiased variance of the values in a group.
```

现在，让我们看看在测量`Population`方差的数据帧上调用`var_pop`的例子:

```
import org.apache.spark.sql.functions._
scala> statesPopulationDF.select(var_pop("Population")).show
 +--------------------+
 | var_pop(Population)|
 +--------------------+
 |4.948359064356177E13|
 +--------------------+
```

# 标准偏差

标准差是方差的平方根(见上一节)。

`stddev` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```
def stddev(columnName: String): Column
 Aggregate function: alias for stddev_samp.
def stddev(e: Column): Column
 Aggregate function: alias for stddev_samp.
def stddev_pop(columnName: String): Column
 Aggregate function: returns the population standard deviation of the
 expression in a group.
def stddev_pop(e: Column): Column
 Aggregate function: returns the population standard deviation of the
 expression in a group.
def stddev_samp(columnName: String): Column
 Aggregate function: returns the sample standard deviation of the expression in a group.
def stddev_samp(e: Column): Column
Aggregate function: returns the sample standard deviation of the expression in a group.
```

我们来看一个在数据框上调用`stddev`的例子，打印`Population`的标准
偏差:

```
import org.apache.spark.sql.functions._
scala> statesPopulationDF.select(stddev("Population")).show
 +-----------------------+
 |stddev_samp(Population)|
 +-----------------------+
 | 7044528.191173398|
 +-----------------------+
```

# 协方差

协方差是两个随机变量联合可变性的度量。

`covar` API 有如下几种实现方式。具体使用的应用编程接口取决于具体的用例:

```
def covar_pop(columnName1: String, columnName2: String): Column
 Aggregate function: returns the population covariance for two columns.
def covar_pop(column1: Column, column2: Column): Column
 Aggregate function: returns the population covariance for two columns.
def covar_samp(columnName1: String, columnName2: String): Column
 Aggregate function: returns the sample covariance for two columns.
def covar_samp(column1: Column, column2: Column): Column
 Aggregate function: returns the sample covariance for two columns.
```

让我们看一个在数据帧上调用`covar_pop`来计算`Year`和`Population`列之间协方差的例子:

```
import org.apache.spark.sql.functions._
scala> statesPopulationDF.select(covar_pop("Year", "Population")).show
 +---------------------------+
 |covar_pop(Year, Population)|
 +---------------------------+
 | 183977.56000006935|
 +---------------------------+
```

# 群组依据

数据分析中常见的任务是将数据分成不同的类别，然后对结果数据组进行计算。

让我们在数据框上运行`groupBy`功能，打印每个`State`的聚合计数:

```
scala> statesPopulationDF.groupBy("State").count.show(5)
 +---------+-----+
| State|count|
 +---------+-----+
 | Utah| 7|
 | Hawaii| 7|
 |Minnesota| 7|
 | Ohio| 7|
 | Arkansas| 7|
 +---------+-----+
```

您也可以`groupBy`然后应用之前看到的任何聚合函数，如`min`、`max`、`avg`、`stddev`等:

```
import org.apache.spark.sql.functions._
 scala> statesPopulationDF.groupBy("State").agg(min("Population"),
 avg("Population")).show(5)
+---------+---------------+--------------------+
 | State|min(Population)| avg(Population)|
 +---------+---------------+--------------------+
 | Utah| 2775326| 2904797.1428571427|
 | Hawaii| 1363945| 1401453.2857142857|
 |Minnesota| 5311147| 5416287.285714285|
 | Ohio| 11540983|1.1574362714285715E7|
 | Arkansas| 2921995| 2957692.714285714|
 +---------+---------------+--------------------+
```

# 到达

Rollup 是用于执行分层或嵌套计算的多维聚合。例如，如果我们想要显示每个`State`和`Year`组以及每个`State`的记录数量(汇总所有年份以给出每个`State`的总计，而不考虑`Year`，我们可以如下使用`rollup`:

```
scala> statesPopulationDF.rollup("State", "Year").count.show(5)
 +------------+----+-----+
 | State|Year|count|
 +------------+----+-----+
 |South Dakota|2010| 1|
 | New York|2012| 1|
 | California|2014| 1|
 | Wyoming|2014| 1|
 | Hawaii|null| 7|
 +------------+----+-----+
```

# 立方

`Cube`是一个多维聚合，用于执行分层或嵌套计算，就像`rollup`一样，不同的是`cube`对所有维度执行相同的操作。例如，如果我们想要显示每个`State`和`Year`组以及每个`State`的记录数量(汇总一整年以给出每个`State`的总计，与`Year`无关)，我们可以使用`cube`如下:

```
scala> statesPopulationDF.cube("State", "Year").count.show(5)
 +------------+----+-----+
 | State|Year|count|
 +------------+----+-----+
 |South Dakota|2010| 1|
 | New York|2012| 1|
 | null|2014| 50|
 | Wyoming|2014| 1|
 | Hawaii|null| 7|
 +------------+----+-----+
```

# 窗口功能

窗口函数允许您在一个数据窗口上执行聚合，而不是在整个数据或某些筛选数据上执行聚合。此类窗口函数的用例有:

*   累计总和
*   同一个键的上一个值的增量
*   加权移动平均线

您可以通过执行简单的计算，指定一个窗口查看三行 *T-1* 、 *T* 和 *T+1* 。您还可以在最近/最近的 10 个值上指定一个窗口:

![](assets/8d678b82-b4dd-455c-9a29-8575ed5c444e.png)

`Window`规范的 API 需要三个属性，`partitionBy()`、`orderBy()`和`rowsBetween()`。`partitionBy`按照`partitionBy()`的规定将数据分块到分区/组中。`orderBy()`用于对每个数据分区内的数据进行排序。

`rowsBetween()`指定执行计算的窗框或滑动窗口的跨度。

要试用 Windows 功能，需要某些软件包。可以使用导入指令导入必要的包，如下所示:

```
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.col
import org.apache.spark.sql.functions.max
```

现在我们准备写一些代码来学习`Window`函数。让我们为按`Population`排序和按`State`分区的分区创建一个窗口规范。此外，指定我们要将当前行之前的所有行视为`Window`的一部分:

```
val windowSpec = Window
 .partitionBy("State")
 .orderBy(col("Population").desc)
 .rowsBetween(Window.unboundedPreceding, Window.currentRow)
```

计算超过`Window`规格的等级。只要在指定的`Window`范围内，结果将是添加到每行的等级(行号)。在这个例子中，我们选择按`State`进行划分，然后按降序对每个`State`的行进行排序。因此，每个`State`行都分配有自己的等级编号:

```
import org.apache.spark.sql.functions._
 scala> statesPopulationDF.select(col("State"), col("Year"),
 max("Population").over(windowSpec), rank().over(windowSpec)).sort("State",
 "Year").show(10)
 +-------+----+-------------------------------------------------------------
 -----------------------------------------------------------------+---------
 ---------------------------------------------------------------------------
 ---------------------------------+
| State|Year|max(Population) OVER (PARTITION BY State ORDER BY Population
 DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|RANK()
 OVER (PARTITION BY State ORDER BY Population DESC NULLS LAST ROWS BETWEEN
 UNBOUNDED PRECEDING AND CURRENT ROW)|
 +-------+----+-------------------------------------------------------------
 -----------------------------------------------------------------+---------
 ---------------------------------------------------------------------------
 ---------------------------------+
|Alabama|2010| 4863300| 6|
 |Alabama|2011| 4863300| 7|
 |Alabama|2012| 4863300| 5|
 |Alabama|2013| 4863300| 4|
 |Alabama|2014| 4863300| 3|
```

# 奈尔斯

`ntiles`是一种流行的窗口聚合，通常用于将输入数据集划分为 *n 个*部分。

例如，如果我们想将`statesPopulationDF`划分为`State`(窗口说明如前所示)，按人口排序，然后分成两部分，我们可以在`windowspec`上使用`ntile`:

```
import org.apache.spark.sql.functions._
scala> statesPopulationDF.select(col("State"), col("Year"),
 ntile(2).over(windowSpec), rank().over(windowSpec)).sort("State",
 "Year").show(10)
+-------+----+-------------------------------------------------------------
 ----------------------------------------------------------+----------------
 ---------------------------------------------------------------------------
 --------------------------+
| State|Year|ntile(2) OVER (PARTITION BY State ORDER BY Population DESC
 NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|RANK() OVER
 (PARTITION BY State ORDER BY Population DESC NULLS LAST ROWS BETWEEN
 UNBOUNDED PRECEDING AND CURRENT ROW)|
 +-------+----+-------------------------------------------------------------
 ----------------------------------------------------------+----------------
 ---------------------------------------------------------------------------
 --------------------------+
 |Alabama|2010| 2| 6|
 |Alabama|2011| 2| 7|
 |Alabama|2012| 2| 5|
 |Alabama|2013| 1| 4|
 |Alabama|2014| 1| 3|
 |Alabama|2015| 1| 2|
 |Alabama|2016| 1| 1|
 | Alaska|2010| 2| 7|
 | Alaska|2011| 2| 6|
 | Alaska|2012| 2| 5|
 +-------+----+-------------------------------------------------------------
 ----------------------------------------------------------+----------------
 --------------------------------------------------------------
```

如前所示，我们使用`Window`功能和`ntile()`一起将每个`State`的行分成两个相等的部分。

A popular use of this function is to compute decile used in data science models.

# 连接

在传统数据库中，连接用于将一个事务表与另一个查找表连接起来，以生成更完整的视图。例如，如果您有一个按客户标识排序的在线交易表和另一个包含客户城市和客户标识的表，则可以使用联接生成按城市排序的交易报告。

**交易表**:本表有三栏，**客户编号**、**采购项目**，客户支付项目金额:

| **客户名称** | **采购项目** | **支付的价格** |
| one | 耳机 | Twenty-five |
| Two | 看 | Twenty |
| three | 键盘 | Twenty |
| one | 老鼠 | Ten |
| four | 电缆 | Ten |
| three | 耳机 | Thirty |

**客户信息表**:该表有两列**客户信息**和客户居住的**城市**:

| **客户编号** | **城市** |
| one | 波士顿 |
| Two | 纽约 |
| three | 费城 |
| four | 波士顿 |

将交易表与客户信息表连接起来将生成如下视图:

| **客户编号** | **采购项目** | **支付的价格** | **城市** |
| one | 双耳式耳机 | Twenty-five | 波士顿 |
| Two | 看 | One hundred | 纽约 |
| three | 键盘 | Twenty | 费城 |
| one | 老鼠 | Ten | 波士顿 |
| four | 电缆 | Ten | 波士顿 |
| three | 耳机 | Thirty | 费城 |

现在，我们可以使用这个连接的视图来生成按`City`排序的`Total`销售价格报告:

| **城市** | **#项** | **销售总价** |
| 波士顿 | three | Forty-five |
| 费城 | Two | Fifty |
| 纽约 | one | One hundred |

连接是火花 SQL 的一个重要功能，因为它们使您能够将两个数据集结合在一起，如前所述。当然，Spark 不仅仅意味着生成一些报告，还用于以 Peta 字节规模处理数据，以处理实时流用例、机器学习算法或简单分析。为了实现这些目标，Spark 提供了所需的 API 函数。

两个数据集之间的典型连接使用左右数据集的一个或多个键进行，然后将键集上的条件表达式计算为布尔表达式。如果布尔表达式的结果返回`true`，则连接成功，否则连接的数据帧将不包含相应的连接。`join`应用编程接口有六种不同的实现:

```
join(right: Dataset[_]): DataFrame
 Condition-less inner join
 join(right: Dataset[_], usingColumn: String): DataFrame
 Inner join with a single column
 join(right: Dataset[_], usingColumns: Seq[String]): DataFrame
 Inner join with multiple columns
 join(right: Dataset[_], usingColumns: Seq[String], joinType: String):
 DataFrame
Join with multiple columns and a join type (inner, outer,....)
 join(right: Dataset[_], joinExprs: Column): DataFrame
 Inner Join using a join expression
join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame
 Join using a Join expression and a join type (inner, outer, ...)
```

我们将使用其中一个 API 来了解如何使用`join`API；但是，您可以根据用例选择使用其他 API:

```
def join(right: Dataset[_], joinExprs: Column, joinType: String):
 DataFrame
Join with another DataFrame using the given join expression
 right: Right side of the join.
joinExprs: Join expression.
 joinType : Type of join to perform. Default is inner join
// Scala:
 import org.apache.spark.sql.functions._
 import spark.implicits._
 df1.join(df2, $"df1Key" === $"df2Key", "outer")
```

请注意，在接下来的几节中将详细介绍连接。

# 连接的内部工作方式

Join 通过使用多个执行器对数据帧的分区进行操作来工作。但是，实际操作和后续性能取决于连接的类型和要连接的数据集的性质。在下一节中，我们将研究不同类型的连接。

# 随机加入

两个大数据集之间的连接包括无序连接，其中左右数据集的分区分布在执行器中。混洗是昂贵的，分析逻辑以确保分区和混洗的分布是最佳的是很重要的。

以下是 shuffle join 内部工作原理的说明:

![](assets/8cebbd91-123c-463b-ba38-c70003247350.png)

# 广播加入

一个大数据集和一个小数据集之间的连接是通过将小数据集广播给从左数据集开始存在分区的所有执行器来实现的，这种连接称为**广播连接**。

以下是广播连接如何在内部工作的说明:

![](assets/0a1e4ac5-c3a1-4059-b340-fba572d1437f.png)

# 连接类型

下表列出了不同类型的联接。这一点很重要，因为连接两个数据集时所做的选择对输出和性能都有很大影响:

| 连接类型 | **描述** |
| 内部的 | 内部连接从左到右比较每一行，并且仅当左右数据集都具有非`NULL`值时，才组合匹配的行对。 |
| 外侧，全外侧，全外侧 | 完全外部联接给出了左侧和右侧表中的所有行。如果我们想保留两个表中的所有行，我们使用完全外部连接。当其中一个表匹配时，完全外部联接返回所有行 |
| 左反 | 左反连接只给出基于左侧表的那些行，这些行不在右侧表中。 |
| 左，左外侧 | 左外连接给出了左中的所有行加上左和右的公共行(内连接)。如果不正确，填写`NULL`。 |
| 左半 | 左半连接根据右侧的存在只给出左侧的行。不包括右侧的值。 |
| 右，右外侧 | 右外连接给出了右中的所有行加上左和右的公共行(内连接)。如果不在左边，填写`NULL`。 |

我们将通过使用示例数据集来研究不同的连接类型是如何工作的:

```
scala> val statesPopulationDF = spark.read.option("header",
 "true").option("inferschema", "true").option("sep",
 ",").csv("statesPopulation.csv")
 statesPopulationDF: org.apache.spark.sql.DataFrame = [State: string, Year:
 int ... 1 more field]
scala> val statesTaxRatesDF = spark.read.option("header",
 "true").option("inferschema", "true").option("sep",
 ",").csv("statesTaxRates.csv")
 statesTaxRatesDF: org.apache.spark.sql.DataFrame = [State: string, TaxRate:
 double]
scala> statesPopulationDF.count
 res21: Long = 357
scala> statesTaxRatesDF.count
 res32: Long = 47
%sql
 statesPopulationDF.createOrReplaceTempView("statesPopulationDF")
 statesTaxRatesDF.createOrReplaceTempView("statesTaxRatesDF")
```

# 内部连接

当两个数据集中的`State`都不为`NULL`时，内部连接会产生来自`statesPopulationDF`和`statesTaxRatesDF`的行:

![](assets/25f757bc-df91-4b5d-8f3c-4ce8e6626bf1.png)

通过`State`列连接两个数据集，如下所示:

```
val joinDF = statesPopulationDF.join(statesTaxRatesDF,
 statesPopulationDF("State") === statesTaxRatesDF("State"), "inner")
%sql
 val joinDF = spark.sql("SELECT * FROM statesPopulationDF INNER JOIN
 statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")
scala> joinDF.count
 res22: Long = 329
scala> joinDF.show
 +--------------------+----+----------+--------------------+-------+
 | State|Year|Population| State|TaxRate|
+--------------------+----+----------+--------------------+-------+
 | Alabama|2010| 4785492| Alabama| 4.0|
 | Arizona|2010| 6408312| Arizona| 5.6|
 | Arkansas|2010| 2921995| Arkansas| 6.5|
 | California|2010| 37332685| California| 7.5|
 | Colorado|2010| 5048644| Colorado| 2.9|
 | Connecticut|2010| 3579899| Connecticut| 6.35|
```

你可以在`joinDF`上运行`explain()`查看执行计划:

```
scala> joinDF.explain
 == Physical Plan ==
*BroadcastHashJoin [State#570], [State#577], Inner, BuildRight
 :- *Project [State#570, Year#571, Population#572]
 : +- *Filter isnotnull(State#570)
 : +- *FileScan csv [State#570,Year#571,Population#572] Batched: false,
Format: CSV, Location: InMemoryFileIndex[file:/Users/salla/spark-2.1.0-binhadoop2.7/
 statesPopulation.csv], PartitionFilters: [], PushedFilters:
 [IsNotNull(State)], ReadSchema:
 struct<State:string,Year:int,Population:int>
 +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string,
 true]))
 +- *Project [State#577, TaxRate#578]
 +- *Filter isnotnull(State#577)
 +- *FileScan csv [State#577,TaxRate#578] Batched: false, Format: CSV,
Location: InMemoryFileIndex[file:/Users/salla/spark-2.1.0-binhadoop2.7/
 statesTaxRates.csv], PartitionFilters: [], PushedFilters:[IsNotNull(State)], ReadSchema: struct<State:string,TaxRate:double>
```

# 左外连接

左外连接导致从`statesPopulationDF`开始的所有行，包括在`statesPopulationDF`和`statesTaxRatesDF`中常见的任何行:

![](assets/e018fcd4-86f3-49fe-9f24-b47542b2882e.png)

通过`State`列连接两个数据集，如下所示:

```
val joinDF = statesPopulationDF.join(statesTaxRatesDF,
 statesPopulationDF("State") === statesTaxRatesDF("State"), "leftouter")
%sql
 val joinDF = spark.sql("SELECT * FROM statesPopulationDF LEFT OUTER JOIN
statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")
 scala> joinDF.count
 res22: Long = 357
 scala> joinDF.show(5)
 +----------+----+----------+----------+-------+
 | State|Year|Population| State|TaxRate|
 +----------+----+----------+----------+-------+
 | Alabama|2010| 4785492| Alabama| 4.0|
 | Alaska|2010| 714031| null| null|
 | Arizona|2010| 6408312| Arizona| 5.6|
 | Arkansas|2010| 2921995| Arkansas| 6.5|
 |California|2010| 37332685|California| 7.5|
 +----------+----+----------+----------+-------+
```

# 右外连接

右外连接导致从`statesTaxRatesDF`开始的所有行，包括在`statesPopulationDF`和`statesTaxRatesDF`中常见的行:

![](assets/7bb4021d-a5a2-4e3e-b621-c866448844db.png)

通过`State`列连接两个数据集，如下所示:

```
val joinDF = statesPopulationDF.join(statesTaxRatesDF,
 statesPopulationDF("State") === statesTaxRatesDF("State"), "rightouter")
%sql
 val joinDF = spark.sql("SELECT * FROM statesPopulationDF RIGHT OUTER JOIN
 statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")
scala> joinDF.count
 res22: Long = 323
scala> joinDF.show
 +--------------------+----+----------+--------------------+-------+
 | State|Year|Population| State|TaxRate|
 +--------------------+----+----------+--------------------+-------+
 | Colorado|2011| 5118360| Colorado| 2.9|
 | Colorado|2010| 5048644| Colorado| 2.9|
 | null|null| null|Connecticut| 6.35|
 | Florida|2016| 20612439| Florida| 6.0|
 | Florida|2015| 20244914| Florida| 6.0|
 | Florida|2014| 19888741| Florida| 6.0|
```

# 外部连接

外部连接导致从`statesPopulationDF`到`statesTaxRatesDF`的所有行:

![](assets/8d82c5c0-e8a0-4888-810d-ef3768a82d8c.png)

通过`State`列连接两个数据集，如下所示:

```
val joinDF = statesPopulationDF.join(statesTaxRatesDF,
 statesPopulationDF("State") === statesTaxRatesDF("State"), "fullouter")
%sql
 val joinDF = spark.sql("SELECT * FROM statesPopulationDF FULL OUTER JOIN
 statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")
scala> joinDF.count
 res22: Long = 351
scala> joinDF.show
 +--------------------+----+----------+--------------------+-------+
 | State|Year|Population| State|TaxRate|
 +--------------------+----+----------+--------------------+-------+
 | Delaware|2010| 899816| null| null|
 | Delaware|2011| 907924| null| null|
 | West Virginia|2010| 1854230| West Virginia| 6.0|
 | West Virginia|2011| 1854972| West Virginia| 6.0|
 | Missouri|2010| 5996118| Missouri| 4.225|
 | null|null| null| Connecticut| 6.35|
```

# 左反连接

当且仅当`statesTaxRatesDF`中没有对应的行时，左反连接仅导致从`statesPopulationDF`开始的行:

![](assets/c79e5fd7-f482-4289-aba9-7afa705faef3.png)

通过`State`列连接两个数据集，如下所示:

```
val joinDF = statesPopulationDF.join(statesTaxRatesDF,
 statesPopulationDF("State") === statesTaxRatesDF("State"), "leftanti")
 %sql
 val joinDF = spark.sql("SELECT * FROM statesPopulationDF LEFT ANTI JOIN
 statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")
scala> joinDF.count
res22: Long = 28
 scala> joinDF.show(5)
 +--------+----+----------+
 | State|Year|Population|
 +--------+----+----------+
 | Alaska|2010| 714031|
 |Delaware|2010| 899816|
 | Montana|2010| 990641|
 | Oregon|2010| 3838048|
 | Alaska|2011| 722713|
 +--------+----+----------+
```

# 左半连接

当且仅当在`statesTaxRatesDF`中有对应的行时，左半连接导致仅从`statesPopulationDF`开始的行:

![](assets/c58407d6-2379-4b9e-a29f-f4c0f3a402a0.png)

通过`State`列连接两个数据集，如下所示:

```
val joinDF = statesPopulationDF.join(statesTaxRatesDF,
 statesPopulationDF("State") === statesTaxRatesDF("State"), "leftsemi")
 %sql

```

```
val joinDF = spark.sql("SELECT * FROM statesPopulationDF LEFT SEMI JOIN
 statesTaxRatesDF ON statesPopulationDF.State = statesTaxRatesDF.State")
```

```
scala> joinDF.count
```

```
res22: Long = 322
 scala> joinDF.show(5)
 +----------+----+----------+
 | State|Year|Population|
 +----------+----+----------+
 | Alabama|2010| 4785492|
 | Arizona|2010| 6408312|
 | Arkansas|2010| 2921995|
 |California|2010| 37332685|
 | Colorado|2010| 5048644|
 +----------+----+----------+
```

# 交叉连接

交叉连接将左边的每一行与右边的每一行进行匹配，生成笛卡尔叉积:

![](assets/ca00f7bd-ff1c-4a0c-9ddd-41b451ed4124.png)

通过`State`列连接两个数据集，如下所示:

```
scala> val joinDF=statesPopulationDF.crossJoin(statesTaxRatesDF)
 joinDF: org.apache.spark.sql.DataFrame = [State: string, Year: int ... 3
 more fields]
%sql
val joinDF = spark.sql("SELECT * FROM statesPopulationDF CROSS JOIN
 statesTaxRatesDF")
 scala> joinDF.count
res46: Long = 16450
 scala> joinDF.show(10)
 +-------+----+----------+-----------+-------+
 | State|Year|Population| State|TaxRate|
 +-------+----+----------+-----------+-------+
 |Alabama|2010| 4785492| Alabama| 4.0|
 |Alabama|2010| 4785492| Arizona| 5.6|
 |Alabama|2010| 4785492| Arkansas| 6.5|
 |Alabama|2010| 4785492| California| 7.5|
 |Alabama|2010| 4785492| Colorado| 2.9|
 |Alabama|2010| 4785492|Connecticut| 6.35|
 |Alabama|2010| 4785492| Florida| 6.0|
 |Alabama|2010| 4785492| Georgia| 4.0|
 |Alabama|2010| 4785492| Hawaii| 4.0|
 |Alabama|2010| 4785492| Idaho| 6.0|
 +-------+----+----------+-----------+-------+
```

You can also use join with cross `joinType` instead of calling the cross join API: `statesPopulationDF.join(statesTaxRatesDF, statesPopulationDF("State").isNotNull, "cross").count`.

# 加入对性能的影响

选择的连接类型直接影响连接的性能。这是因为联接需要在执行器之间进行数据洗牌来执行任务，因此在使用联接时需要考虑不同的联接，甚至联接的顺序。下面是一个在编写连接代码时可以用作参考的表:

| 连接类型 | **性能注意事项和提示** |
| 内部的 | 内部联接要求左右表具有相同的列。如果您在左侧或右侧有重复或多个键的副本，连接将很快变成某种笛卡尔连接，比正确设计以最小化多个键花费更长的时间来完成。 |
| 跨过 | 交叉连接将左边的每一行与右边的每一行进行匹配，生成笛卡尔叉积。由于这是性能最差的连接，因此应谨慎使用，仅在特定用例中使用。 |
| 外侧，全外侧，全外侧 | 完全外部联接给出联接子句左侧和右侧表中的所有(匹配和不匹配)行。当我们想要保留两个表中的所有行时，我们使用完全外部连接。当其中一个表匹配时，完全外部联接返回所有行。如果在几乎没有共同点的表上使用，可能会导致非常大的结果，从而降低性能。 |
| 左反 | 左反连接只给出基于左侧表的那些行，这些行不在右侧表中。当我们希望只保留左表中的行而不保留右表中的行时，请使用此选项。非常好的性能，因为只充分考虑了一个表，并且只检查了另一个表的连接条件。 |
| 左，左外侧 | 左外连接给出了左侧表中的所有行，以及两个表共有的行(内连接)。如果在几乎没有共同点的表上使用，会导致非常大的结果，从而降低性能。 |
| 左半 | 当且仅当左侧表中的行存在于右侧表中时，左侧半联接才给出左侧表中的行。当且仅当行在右表中找到时，使用此选项从左表中获取行。这与上面看到的 leftanti join 相反。不包括右侧值。非常好的性能，因为只充分考虑了一个表，并且只检查了另一个表的连接条件。 |
| 右，右外侧 | 右外连接给出了右侧表中的所有行以及左侧和右侧的公共行(内连接)。使用它可以获得右表中的所有行以及左表和右表中的行。如果不在左边，填写`NULL`。性能类似于本表前面提到的左外连接。 |

# 摘要

在本章中，我们讨论了数据框的起源，以及 Spark SQL 如何在数据框之上提供 SQL 接口。数据帧的强大之处在于，与最初基于 RDD 的计算相比，执行时间减少了。拥有这样一个功能强大的层和一个简单的类似于 SQL 的接口使它变得更加强大。我们还研究了创建和操作数据帧的各种 API，并深入挖掘了聚合的复杂特性，包括`groupBy`、`Window`、`rollup`和`cubes`。最后，我们还研究了连接数据集的概念以及各种可能的连接类型，如内部、外部、交叉等。

我们将通过 Apache Spark 在[第 7 章](07.html)、*实时分析中探索令人兴奋的实时数据处理和分析世界。*