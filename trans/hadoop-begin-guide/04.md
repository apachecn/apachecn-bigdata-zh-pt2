# 第四章开发 MapReduce 程序

*既然我们已经探索了 MapReduce 技术，我们将在本章研究如何使用它。 具体地说，我们将使用一个更丰富的数据集，并研究如何使用 MapReduce 提供的工具来分析它。*

在本章中，我们将介绍以下主题：

*   Hadoop 流媒体及其应用
*   不明飞行物目击数据集
*   使用流作为开发/调试工具
*   在单个作业中使用多个映射器
*   在群集中高效共享实用程序文件和数据
*   报告作业和任务状态以及对调试有用的日志信息

在本章中，目标是介绍有关如何分析新数据集的具体工具和想法。 我们将从查看如何使用脚本编程语言来帮助 MapReduce 进行原型开发和初始分析开始。 尽管在上一章中学习 Java API 并立即迁移到不同的语言似乎有些奇怪，但我们的目标是让您了解解决问题的不同方法。 正如许多作业使用 Java API 以外的其他方式实现没有什么意义一样，在其他情况下，使用另一种方法是最合适的。 把这些技术看作是你工具带的新补充，有了这些经验，你就会更容易地知道哪种技术最适合给定的场景。

# 在 Hadoop 中使用 Java 以外的语言

我们在前面已经提到过，MapReduce 程序不必用 Java 编写。 大多数程序都是用 Java 编写的，但是您可能想要或需要用另一种语言编写地图和减少任务，这有几个原因。 也许您有现有的代码可以利用，或者需要使用第三方二进制文件-原因是多种多样且合理的。

Hadoop 提供了许多机制来帮助非 Java 开发，其中最主要的是**Hadoop 管道**，它为 Hadoop 提供了本机 C++接口，以及**Hadoop Streaming**，它允许任何使用标准输入和输出的程序用于映射和缩减任务。 在本章中，我们将大量使用 Hadoop 流。

## Hadoop 流的工作原理

使用MapReduce Java API，map 和 Reduce 任务都提供了包含任务功能的方法的实现。 这些方法将输入作为方法参数接收到任务，然后通过`Context`对象输出结果。 这是一个清晰且类型安全的接口，但根据定义是特定于 Java 的。

Hadoop 流媒体采用了一种不同的方法。 使用流，您可以编写一个映射任务，该任务从标准输入读取其输入，一次一行，并将其结果的输出提供给标准输出。 然后，Reduce 任务也执行同样的操作，同样只对其数据流使用标准输入和输出。

从标准输入和输出读取和写入的任何程序都可以在流中使用，比如编译的二进制文件、UnixShell 脚本或用 Ruby 或 Python 等动态语言编写的程序。

## 为什么要使用 Hadoop 流

流媒体最大的优势是，与使用 Java 相比，它可以让你更快地尝试想法并进行迭代。 您只需编写脚本并将它们作为参数传递到流 JAR 文件，而不是编译/JAR/提交循环。 特别是在对新数据集进行初步分析或尝试新想法时，这可以显著加快开发速度。

关于动态语言和静态语言的经典争论平衡了快速开发与运行时性能和类型检查的好处。 使用流媒体时，这些动态缺点也适用。 因此，我们倾向于使用流式处理进行前期分析，使用 Java 实现将在生产集群上执行的作业。

在本章中，我们将使用 Ruby 作为流媒体示例，但这是个人喜好。 如果您更喜欢 shell 脚本或另一种语言(如 Python)，那么可以利用这个机会将这里使用的脚本转换为您选择的语言。

# 操作时间-使用流实现字数计数

让我们再次鞭打 wordcount 的死马，并通过执行以下步骤使用流实现它：

1.  将以下文件保存到`wcmapper.rb`：

    ```
    #/bin/env ruby

    while line = gets
        words = line.split("\t")
        words.each{ |word| puts word.strip+"\t1"}}
    end
    ```

2.  通过执行以下命令使文件成为可执行文件：

    ```
    $ chmod +x wcmapper.rb

    ```

3.  将以下文件保存到`wcreducer.rb`：

    ```
    #!/usr/bin/env ruby

    current = nil
    count = 0

    while line = gets
        word, counter = line.split("\t")

        if word == current
            count = count+1
        else
            puts current+"\t"+count.to_s if current
            current = word
            count = 1
        end
    end
    puts current+"\t"+count.to_s
    ```

4.  通过执行以下命令使文件成为可执行文件：

    ```
    $ chmod +x wcreducer.rb

    ```

5.  使用上一章中的数据文件将脚本作为流作业执行：

    ```
    $ hadoop jar hadoop/contrib/streaming/hadoop-streaming-1.0.3.jar 
    -file wcmapper.rb -mapper wcmapper.rb -file wcreducer.rb 
    -reducer wcreducer.rb -input test.txt -output output
    packageJobJar: [wcmapper.rb, wcreducer.rb, /tmp/hadoop-hadoop/hadoop-unjar1531650352198893161/] [] /tmp/streamjob937274081293220534.jar tmpDir=null
    12/02/05 12:43:53 INFO mapred.FileInputFormat: Total input paths to process : 1
    12/02/05 12:43:53 INFO streaming.StreamJob: getLocalDirs(): [/var/hadoop/mapred/local]
    12/02/05 12:43:53 INFO streaming.StreamJob: Running job: job_201202051234_0005
    …
    12/02/05 12:44:01 INFO streaming.StreamJob:  map 100%  reduce 0%
    12/02/05 12:44:13 INFO streaming.StreamJob:  map 100%  reduce 100%
    12/02/05 12:44:16 INFO streaming.StreamJob: Job complete: job_201202051234_0005
    12/02/05 12:44:16 INFO streaming.StreamJob: Output: wcoutput

    ```

6.  检查结果文件：

    ```
    $ hadoop fs -cat output/part-00000

    ```

## *刚刚发生了什么？*

忽略 Ruby 的细节。 如果你不懂这种语言，在这里就不重要了。

首先，我们创建了将成为我们的映射器的脚本。 它使用`gets`函数从标准输入中读取一行，将其拆分成单词，然后使用`puts`函数将单词和值`1`写入标准输出。 然后，我们将该文件设为可执行文件。

由于我们将在下一节描述的原因，我们的减速器稍微复杂一些。 但是，它执行我们预期的工作，它统计每个单词的出现次数，从标准输入读取，并将输出作为最终值提供给标准输出。 同样，我们确保将该文件设置为可执行文件。

请注意，在这两种情况下，我们都隐式使用前面章节中讨论的 Hadoop 输入和输出格式。 它是处理源文件的`TextInputFormat`属性，并将每一行一次提供给映射脚本。 相反，`TextOutputFormat`属性将确保 Reduce 任务的输出也被正确写入为文本数据。 如果需要，我们当然可以修改这些内容。

接下来，我们通过上一节中显示的相当繁琐的命令行将流作业提交给 Hadoop。 每个文件需要指定两次的原因是，任何在每个节点上不可用的文件都必须由 Hadoop 打包并跨群集传送，这需要通过`-file`选项指定。 然后，我们还需要告诉 Hadoop 哪个脚本执行映射器和减速器角色。

最后，我们查看了作业的输出，它应该与前面基于 Java 的 WordCount 实现相同

## 使用流式处理时作业的差异

流式字数映射器看起来比 Java 版本简单得多，但是缩减器似乎有更多的逻辑。 为什么？ 原因是当我们使用流时，Hadoop 和我们的任务之间的隐含契约会发生变化。

在 Java 中，我们知道我们的`map()`方法将为每个输入键/值对调用一次，而我们的`reduce()`方法将为每个键及其值集调用。

对于流，我们不再有`map`或`reduce`方法的概念，而是编写了处理接收的数据流的脚本。 这改变了我们编写减速器的方式。 在 Java 中，每个键的值分组由 Hadoop 执行；每次调用`reduce`方法都将接收一个键及其所有值。 在流式传输中，`reduce`任务的每个实例每次都被赋予一个单独的未收集的值。

Hadoop 流确实会对键进行排序，例如，如果映射器发出以下数据：

```
First     1
Word      1
Word      1
A         1
First     1
```

流减少器将按以下顺序接收此数据：

```
A         1
First     1
First     1
Word      1
Word      1
```

Hadoop 仍然收集每个键的值，并确保每个键只传递给一个减法器。 换句话说，Reducer 获取多个键的所有值，并将它们分组在一起；但是，它们不会打包到 Reducer 的单独执行中，即每个键一个值，这与 Java API 不同。

这应该解释了 Ruby Reducer 中使用的机制；它首先为当前单词设置空默认值；然后在读取每一行之后，它确定这是否是Current 键的另一个值，如果是，则递增计数。 如果不是，则前一个键将没有更多的值，其最终输出将被发送到标准输出，并重新开始对新单词的计数。

在阅读了前面几章中关于 Hadoop 为我们做了这么多事情有多么棒之后，这看起来可能要复杂得多，但在您编写了几个流媒体缩减器之后，它实际上并没有乍看起来那么糟糕。 还要记住，Hadoop 仍然管理对各个映射任务的拆分分配，以及将给定键的值发送到同一减法器的必要协调。 可以通过配置设置修改此行为，以更改映射器和减少器的数量，就像 Java API 一样。

# 分析大型数据集

有了用 Java 和流编写 MapReduce 作业的能力，我们现在将探索一个比我们以前看到的更重要的数据集。 在接下来的小节中，我们将尝试展示如何进行此类分析，以及 Hadoop 允许您对大型数据集提出的问题类型。

## 获取 UFO 目击数据集

我们将使用超过 60,000 次 UFO 目击事件的公共领域数据集。 这是由信息黑猩猩在[http://www.infochimps.com/datasets/60000-documented-ufo-sightings-with-text-descriptions-and-metada](http://www.infochimps.com/datasets/60000-documented-ufo-sightings-with-text-descriptions-and-metada)托管的。

你需要注册一个免费的信息黑猩猩账户才能下载一份数据。

该数据包括一系列 UFO 目击记录，其字段如下：

1.  **目击日期**：此字段给出目击不明飞行物的日期。
2.  **记录日期**：此字段给出目击报告的日期，通常与目击日期不同。
3.  **位置**：此字段提供目击发生的位置。
4.  **形状**：此字段提供 UFO 形状的简要摘要，例如，菱形、灯光、圆柱体。
5.  **持续时间**：此字段给出视线持续的持续时间。
6.  **描述**：此字段提供目击的自由文本详细信息。

下载后，您会发现数据有几种格式。 我们将使用`.tsv`(制表符分隔值)版本。

## 体验数据集

当面对一个新的数据集时，通常很难感觉到所涉及数据的性质、广度和质量。 有几个问题，它们的答案将影响您处理后续分析的方式，特别是：

*   数据集有多大？
*   这些记录有多完整？
*   记录与预期格式的匹配程度如何？

第一个是一个简单的规模问题；我们谈论的是数百、数千、数百万还是更多的记录？ 第二个问题是问记录有多完整。 如果您希望每条记录都有 10 个字段(如果这是结构化或半结构化数据)，那么有多少关键字段填充了数据？ 最后一个问题在这一点上展开，记录与您对格式和表示的期望匹配程度如何？

# 行动时间-汇总 UFO 数据

现在我们有了个数据，让我们初步总结一下它的大小以及有多少条记录可能不完整：

1.  将 HDFS 上的 UFO**制表符分隔值**(**TSV**)文件另存为`ufo.tsv`，将以下文件保存到`summarymapper.rb`：

    ```
    #!/usr/bin/env ruby

    while line = gets
        puts "total\t1"
        parts = line.split("\t")
        puts "badline\t1" if parts.size != 6
        puts "sighted\t1" if !parts[0].empty?
        puts "recorded\t1" if !parts[1].empty?
        puts "location\t1" if !parts[2].empty?
        puts "shape\t1" if !parts[3].empty?
        puts "duration\t1" if !parts[4].empty?
        puts "description\t1" if !parts[5].empty?
    end
    ```

2.  通过执行以下命令使文件成为可执行文件：

    ```
    $ chmod +x summarymapper.rb

    ```

3.  使用流式处理执行作业，如下所示：

    ```
    $ hadoop jar hadoop/contrib/streaming/hadoop-streaming-1.0.3.jar 
    -file summarymapper.rb -mapper summarymapper.rb -file wcreducer.rb -reducer wcreducer.rb -input ufo.tsv -output ufosummary

    ```

4.  取数汇总数据：

    ```
    $ hadoop fs -cat ufosummary/part-0000

    ```

## *刚刚发生了什么？*

请记住，我们的 UFO 目击应该有六个字段，如前所述。 它们如下所列：

*   目击日期
*   报告目击事件的日期
*   目击地点
*   对象的形状
*   目击的持续时间
*   活动的免费文本说明

映射器检查文件并计算记录总数，此外还识别可能不完整的记录。

我们只需记录在处理文件时遇到了多少不同的记录，就可以生成总计数。 我们通过标记不包含恰好六个字段或至少有一个字段为空值的记录来识别潜在的不完整记录。

因此，映射器的实现读取每一行，并在遍历文件时执行三项操作：

*   它提供要在处理的记录总数中递增的令牌的输出
*   它在制表符边界上拆分记录，并记录没有产生六个字段值的任何线的出现情况
*   对于它报告的六个预期字段中的每一个，当显示的值不是空字符串时，即该字段中有数据，尽管这实际上并不说明该数据的质量

我们有意编写此映射器以生成`(token,``count)`形式的输出。 这样做可以让我们使用前面实现中的现有字数减减器作为此作业的减字器。 当然还有更高效的实现，但由于这项工作不太可能频繁执行，所以这种便利是值得的。

在撰写本文时，这项工作的结果如下：

```
badline324
description61372
duration58961
location61377
recorded61377
shape58855
sighted61377
total61377
```

我们从这些数字中看到，我们有 61,300 项记录。 所有这些都提供了“目击日期”、“报告日期”和“位置”字段的值。 大约 58,000-59,000 条记录具有形状值和持续时间值，并且几乎所有记录都有描述。

在制表符上拆分时，发现有 372 行没有恰好有 6 个字段。 但是，由于只有 5 条记录没有描述价值，这表明坏记录通常有太多的选项卡，而不是太少。 当然，我们可以更改我们的地图绘制程序来收集有关这一事实的详细信息。 这很可能是由于自由文本描述中使用了制表符，因此目前我们将进行分析，预计大多数记录都会正确放置所有六个字段的值，但不会对每条记录中的其他制表符做出任何假设。

### 检查 UFO 形状

在这些报告中的所有字段中，形状是我们最感兴趣的，因为它可以提供一些有趣的数据分组方式，具体取决于我们在该字段中拥有的信息类型。

# 行动时间-汇总形状数据

正如我们之前提供的总体 UFO 数据集的汇总一样，现在让我们对为 UFO 形状提供的数据进行更有针对性的汇总：

1.  将以下内容保存到`shapemapper.rb`：

    ```
    #!/usr/bin/env ruby

    while line = gets  
        parts = line.split("\t")    
        if parts.size == 6        
            shape = parts[3].strip     
            puts shape+"\t1" if !shape.empty?   
        end     
    end     
    ```

2.  使文件成为可执行文件：

    ```
    $ chmod +x shapemapper.rb

    ```

3.  使用 Wordcount Reducer：

    ```
    $ hadoop jar hadoop/contrib/streaming/hadoop-streaming-1.0.3.jarr --file shapemapper.rb -mapper shapemapper.rb -file wcreducer.rb -reducer wcreducer.rb -input ufo.tsv -output shapes

    ```

    再次执行作业
4.  检索形状信息：

    ```
    $ hadoop fs -cat shapes/part-00000 

    ```

## *刚刚发生了什么？*

我们这里的地图绘制程序非常简单。 它将每条记录分解成其组成字段，丢弃恰好没有 6 个字段的任何记录，并给出一个计数器作为任何非空形状值的输出。

出于我们这里的目的，我们很乐意忽略任何与我们期望的规范不完全匹配的记录。 也许唯一的 UFO 目击记录将一劳永逸地证明这一点，但即便如此，它也不太可能对我们的分析产生太大影响。 在决定如此轻易地丢弃某些记录之前，请考虑一下单个记录的潜在价值。 如果您主要在关注趋势的大型聚合上工作，那么单个记录可能无关紧要。 但在单个值可能对分析产生实质性影响或必须考虑的情况下，尝试更保守地解析和恢复而不是丢弃可能是最好的方法。 我们将在[第 6 章](06.html "Chapter 6. When Things Break")，*中更多地讨论这种权衡*。

在将地图程序设置为可执行文件并运行我们生成的作业的常规程序之后，报告了显示 29 种不同 UFO 形状的数据。 以下是出于空间原因以紧凑形式列出的一些示例输出：

```
changed1 changing1533 chevron758 cigar1774
circle5250 cone265 crescent2 cross177
cylinder981 delta8 diamond909 disk4798
dome1 egg661 fireball3437 flare1
flash988 formation1775 hexagon1 light12140
other4574 oval2859 pyramid1 rectangle957
round2 sphere3614 teardrop592 triangle6036
unknown4459
```

正如我们所看到的，在目击频率上有很大的差异。 有些像金字塔这样的形状只出现一次，而光占所有报告形状的五分之一以上。 考虑到许多不明飞行物目击事件都发生在夜间，人们可能会争辩说，对光的描述并不是特别有用或具体，当结合其他和未知的值时，我们会看到，在我们报告的 58000 个形状中，大约有 21000 可能实际上没有任何用处。 因为我们不会跑出去做额外的研究，所以这不是很重要，但重要的是开始从这些方面考虑你的数据。 即使是这些类型的汇总分析也可以开始洞察数据的性质，并指出可能的分析质量。 例如，在报告形状的情况下，我们已经发现，在我们的 61000 次目击中，只有 58000 次报告了形状，其中 21000 次的价值令人怀疑。 我们已经确定，我们的 61000 个样本集只提供了 37000 个我们可能能够使用的形状报告。 如果您的分析基于最小数量的样本，请始终确保预先进行此类总结，以确定数据集是否真的能满足您的需求。

# 行动时间-目击持续时间与 UFO 形状的关联

让我们对此形状数据做一点更详细的分析。 我们想知道目击持续时间与报告的形状之间是否有关联。 也许雪茄形状的不明飞行物停留的时间比其他的要长，或者队形出现的时间总是准确的。

1.  将以下内容保存到`shapetimemapper.rb`：

    ```
    #!/usr/bin/env ruby

    pattern = Regexp.new /\d* ?((min)|(sec))/

    while line = gets
    parts = line.split("\t")
    if parts.size == 6
    shape = parts[3].strip
    duration = parts[4].strip.downcase
    if !shape.empty? && !duration.empty?
    match = pattern.match(duration)
    time = /\d*/.match(match[0])[0]
    unit = match[1]
    time = Integer(time)
    time = time * 60 if unit == "min"
    puts shape+"\t"+time.to_s
    end
    end
    end
    ```

2.  通过执行以下命令使文件成为可执行文件：

    ```
    $ chmod +x shapetimemapper.rb

    ```

3.  将以下内容保存到`shapetimereducer.rb`：

    ```
    #!/usr/bin/env ruby

    current = nil
    min = 0
    max = 0
    mean = 0
    total = 0
    count = 0

    while line = gets
    word, time = line.split("\t")
    time = Integer(time)

    if word == current
    count = count+1
    total = total+time
    min = time if time < min
    max = time if time > max
    else
    puts current+"\t"+min.to_s+" "+max.to_s+" "+(total/count).to_s if current
    current = word
    count = 1
    total = time
    min = time
    max = time
    end
    end
    puts current+"\t"+min.to_s+" "+max.to_s+" "+(total/count).to_s
    ```

4.  通过执行以下命令使文件可执行：

    ```
    $ chmod +x shapetimereducer.rb

    ```

5.  运行作业：

    ```
    $ hadoop jar hadoop/contrib/streaminghHadoop-streaming-1.0.3.jar -file shapetimemapper.rb -mapper shapetimemapper.rb -file shapetimereducer.rb -reducer shapetimereducer.rb -input ufo.tsv -output shapetime

    ```

6.  检索结果：

    ```
    $ hadoop fs -cat shapetime/part-00000

    ```

## *刚刚发生了什么？*

由于持续时间字段的性质，这里的映射器比前面的示例稍微复杂一些。 快速查看一些示例记录，我们发现值如下所示：

```
15 seconds
2 minutes
2 min
2minutes
5-10 seconds
```

换句话说，存在范围和绝对值的混合，时间单位的不同格式和不一致的术语。 同样，为简单起见，我们决定对数据进行有限的解释；如果有，我们将取绝对值，如果没有，则取范围的上半部分。 我们假设字符串`min`或`sec`将出现在时间单位中，并将所有计时转换为秒。 使用一些正则表达式魔术，我们将持续时间字段解压成这些部分并进行转换。 再次注意，我们只是丢弃任何不能按预期工作的记录，这可能并不总是合适的。

减法器遵循与我们前面的示例相同的模式，从缺省键开始，然后读取值，直到遇到新的键。 在本例中，我们希望捕获每个形状的最小值、最大值和平均值，因此使用大量变量来跟踪所需的数据。

请记住，流缩减器需要处理分组到其关联键中的一系列值，并且必须识别新行何时具有更改的键，从而指示已处理的前一个键的最后一个值。 相反，Java Reducer 会更简单，因为它在每次执行中只处理单个键的值。

在使这两个文件成为可执行文件之后，我们运行该作业并获得以下结果，其中我们删除了所有少于 10 次的形状，并再次出于空间原因使输出更加紧凑。 每个形状的数字分别是最小值、最大值和平均值：

```
changing0 5400 670 chevron0 3600 333
cigar0 5400 370 circle0 7200 423
cone0 4500 498 cross2 3600 460
cylinder0 5760 380 diamond0 7800 519
disk0 5400 449 egg0 5400 383
fireball0 5400 236 flash0 7200 303
formation0 5400 434 light0 9000 462
other0 5400 418 oval0 5400 405
rectangle0 4200 352 sphere0 14400 396
teardrop0 2700 335 triangle0 18000 375
unknown0 6000 470
```

令人惊讶的是，在所有形状类型中，平均目击持续时间的变化相对较小；大多数形状的平均值在 350 秒到 430 秒之间。 有趣的是，我们还看到火球的平均持续时间最短，可变物体的平均持续时间最长，这两者都有一定的直觉意义。 根据定义，火球不会是一个持久的现象，一个多变的物体需要很长的时间才能被注意到它的变化。

### 在 Hadoop 外部使用流式脚本

最后一个示例及其更复杂的映射器和减少器很好地说明了流如何以另一种方式帮助 MapReduce 开发；您可以在 Hadoop 之外执行脚本。

在 MapReduce 开发期间，拥有用于测试代码的生产数据样本通常是很好的做法。 但是，当这是在 HDFS 上，并且您正在编写 Java map 和 Reduce 任务时，可能很难调试问题或改进复杂的逻辑。 使用从命令行读取输入的 map 和 Reduce 任务，您可以直接针对一些数据运行它们，以获得对结果的快速反馈。 如果您有一个提供 Hadoop 集成的开发环境，或者在独立模式下使用 Hadoop，那么问题就会最小化；只需记住，流确实让您能够在 Hadoop 之外尝试脚本；它可能有一天会有用。

在开发这些脚本时，作者注意到他的 UFO 数据文件中的最后一组记录的数据比文件开头的记录具有更好的结构化方式。 因此，要在映射器上进行快速测试，只需：

```
$ tail ufo.tsv | shapetimemapper.rb
```

这一原则可以应用于整个工作流，以执行 map 和 Reduce 脚本。

# 操作时间-从命令行执行形状/时间分析

如何进行这种本地命令行分析可能不是很明显，所以让我们来看一个例子。

使用本地文件系统上的 UFO 数据文件，执行以下命令：

```
$ cat ufo.tsv | shapetimemapper.rb | sort| shapetimereducer.rb

```

## *刚刚发生了什么？*

使用单一的 unix 命令行，我们生成了与之前完整的 MapReduce 作业相同的输出。 如果您查看一下命令行的功能，就会发现这是有意义的。

首先，将输入文件(一次一行)发送到映射器。 它的输出通过 Unix 排序实用程序传递，并且这个排序后的输出一次传递一行给减法器。 当然，这是我们的一般 MapReduce 作业工作流的一个非常简化的表示。

那么，一个显而易见的问题是，如果我们可以在命令行进行等价的分析，那么我们为什么还要费心使用 Hadoop 呢？ 答案当然是我们的老朋友，Scale。 这种简单的方法对于 UFO 目击这样的文件很有效，虽然不是微不足道的，但只有 71MB 大小。 将其放在上下文中，我们可以在单个现代磁盘驱动器上保存此数据集的数千个副本。

那么，如果数据集的大小改为 71 GB，甚至是 71TB，又会怎样呢？ 在后一种情况下，至少我们必须将数据分布在多个主机上，然后决定如何拆分数据、合并部分答案，并在此过程中处理不可避免的故障。 换句话说，我们需要类似 Hadoop 的东西。

但是，不要忽视使用这样的命令行工具，在 MapReduce 开发过程中应该很好地使用这些方法。

## Java 形状和位置分析

让我们返回到Java MapReduce API，并考虑对报告中的形状和位置数据进行一些分析。

但是，在开始编写代码之前，让我们先考虑一下我们是如何处理该数据集的每个字段的分析的。 以前的映射器有一个共同的模式：

*   丢弃确定为损坏的记录
*   处理有效记录以提取感兴趣的字段
*   输出我们关心的记录数据的表示形式

现在，如果我们要编写 Java 映射器来分析位置，然后可能还要分析目击和报告时间列，我们将遵循类似的模式。 那么，我们可以避免任何随之而来的代码重复吗？

通过使用`org.apache.hadoop.mapred.lib.ChainMapper`，答案是肯定的。 此类提供了一种顺序执行多个映射器的方法，最终映射器的输出将传递给减少器。 `ChainMapper`不仅适用于这种类型的数据清理；在分析特定作业时，在应用 Reducer 之前执行多个映射类型任务并不少见。

此方法的一个示例是编写一个验证映射器，该映射器可供所有未来的现场分析作业使用。 该映射器将丢弃被认为已损坏的行，只将有效行传递给实际的业务逻辑映射器，该映射器现在可以专注于分析数据，而不是担心粗略级别的验证。

这里的另一种方法是在丢弃无效记录的自定义`InputFormat`类中进行验证；哪种方法最有意义取决于您的特定情况。

链中的每个映射器都在单个 JVM 中执行，因此不必担心使用多个映射器会增加我们的文件系统 I/O 负载。

# 行动时间-使用 ChainMapper 进行现场验证/分析

让我们使用这个原则并使用`ChainMapper`类来帮助我们在工作中提供一些记录验证：

1.  将以下类创建为`UFORecordValidationMapper.java`：

    ```
    import java.io.IOException;

    import org.apache.hadoop.io.* ;
    import org.apache.hadoop.mapred.* ;
    import org.apache.hadoop.mapred.lib.* ;

    public class UFORecordValidationMapper extends MapReduceBase
    implements Mapper<LongWritable, Text, LongWritable, Text>
    {

        public void map(LongWritable key, Text value,
            OutputCollector<LongWritable, Text> output,
            Reporter reporter) throws IOException
    {
    String line = value.toString();
            if (validate(line))
                output.collect(key, value);
        }

            private boolean validate(String str)
            {
                String[] parts = str.split("\t") ;

                if (parts.length != 6)
                return false ;

                return true ;
            }
        }
    ```

2.  将创建为`UFOLocation.java`：

    ```
    import java.io.IOException;
    import java.util.Iterator ;
    import java.util.regex.* ;

    import org.apache.hadoop.conf.* ;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.* ;
    import org.apache.hadoop.mapred.* ;
    import org.apache.hadoop.mapred.lib.* ;

    public class UFOLocation
    {

        public static class MapClass extends MapReduceBase
    implements Mapper<LongWritable, Text, Text, LongWritable>
    {

    private final static LongWritable one = new LongWritable(1);
    private static Pattern locationPattern = Pattern.compile(
    "[a-zA-Z]{2}[^a-zA-Z]*$") ;

    public void map(LongWritable key, Text value,
    OutputCollector<Text, LongWritable> output,
    Reporter reporter) throws IOException
    {
    String line = value.toString();
            String[] fields = line.split("\t") ;
            String location = fields[2].trim() ;
            if (location.length() >= 2)
            {

                Matcher matcher = locationPattern.matcher(location) ;
                if (matcher.find() )
                {
                    int start = matcher.start() ;
                    String state = location.substring(start,start+2);

                    output.collect(new Text(state.toUpperCase()), 
                           One);
                }
            }
        }
    }

    public static void main(String[] args) throws Exception
    {
        Configuration config = new Configuration() ;
    JobConf conf = new JobConf(config, UFOLocation.class);
    conf.setJobName("UFOLocation");

    conf.setOutputKeyClass(Text.class);
    conf.setOutputValueClass(LongWritable.class);

    JobConf mapconf1 = new JobConf(false) ;
    ChainMapper.addMapper( conf, UFORecordValidationMapper.class,                  
    LongWritable.class, Text.class, LongWritable.class, 
    Text.class, true, mapconf1) ;

    JobConf mapconf2 = new JobConf(false) ;
    ChainMapper.addMapper( conf, MapClass.class, 
    LongWritable.class, Text.class, 
    Text.class, LongWritable.class, true, mapconf2) ;
    conf.setMapperClass(ChainMapper.class);
    conf.setCombinerClass(LongSumReducer.class);
    conf.setReducerClass(LongSumReducer.class);

    FileInputFormat.setInputPaths(conf,args[0]) ;
    FileOutputFormat.setOutputPath(conf, new Path(args[1])) ;

    JobClient.runJob(conf);
    }
    }
    ```

3.  编译两个文件：

    ```
    $ javac UFORecordValidationMapper.java UFOLocation.java

    ```

4.  将类文件打包并将作业提交给 Hadoop：

    ```
    $ Hadoop jar ufo.jar UFOLocation ufo.tsv output

    ```

5.  将输出文件复制到本地文件系统并进行检查：

    ```
    $ Hadoop fs -get output/part-00000 locations.txt
    $ more locations.txt
    ```

## *刚刚发生了什么？*

这里发生了相当多的事情，所以让我们一次看一段。

第一个映射器是我们的简单验证映射器。 该类遵循与标准 MapReduce API 相同的接口，`map`方法只是返回实用程序验证方法的结果。 我们将其拆分到一个单独的方法中，以突出映射器的功能，但是检查很容易就在 main`map`方法本身中进行。 为简单起见，我们坚持以前的验证策略，即查找字段的数量，并丢弃没有恰好分成六个制表符分隔字段的行。

请注意，不幸的是，`ChainMapper`类是最后要迁移到上下文对象 API 的组件之一，从 Hadoop1.0 开始，它只能与较旧的 API 一起使用。 它仍然是一个有效的概念和有用的工具，但在 Hadoop2.0 之前，它将最终迁移到`org.apache.hadoop.mapreduce.lib.chain`包中，它目前的使用需要旧的方法。

另一个文件包含 Main 方法中的另一个映射器实现和更新的驱动程序。 地图绘制程序在 UFO 目击报告中的位置字段末尾查找由两个字母组成的序列。 从对数据的一些手动检查中可以明显看出，大多数位置字段的格式为`city, state`，其中使用标准的两个字符的缩写表示州。

但是，有些记录会添加尾随的圆括号、句点或其他标点符号。 其他一些根本不是这种格式。 出于我们的目的，我们很乐意丢弃这些记录，并将重点放在那些具有我们要查找的尾随两个字符的州缩写的记录上。

Map 方法使用另一个正则表达式从 Location 字段中提取它，并以缩写的大写形式和一个简单的计数给出输出。

作业的驱动程序更改最多，因为涉及单个`map`类的先前配置被替换为对`ChainMapper`类的多个调用。

一般模型是为每个映射器创建一个新的配置对象，然后将映射器及其输入和输出的规范以及对整个作业配置对象的引用一起添加到`ChainMapper`类。

请注意，这两个映射器具有不同的签名。 两者都输入类型为`LongWritable`的键和类型为`Text`的值，它们也是`UFORecordValidationMapper`的输出类型。 然而，`UFOLocationMapper`输出的是文本类型的键和`LongWritable`类型的值，反之亦然。

这里重要的是将来自链中最终映射器的输入(`UFOLocationMapper`)与`reduce`类期望的输入(`LongSumReducer`)进行匹配。 使用`ChainMapper`类时，只要满足以下条件，链中的映射器可以具有不同的输入和输出：

*   对于除最终映射器之外的所有映射器，每个映射器输出都与链中后续映射器的输入匹配
*   对于最终映射器，其输出与减少器的输入匹配

我们编译这些类，并将它们放入相同的 JAR 文件中。 这是我们第一次将来自多个 Java 源文件的输出捆绑在一起。 正如预期的那样，这里没有什么神奇之处；JAR 文件、路径和类名的常规规则适用。 因为在本例中，我们的两个类都在同一个包中，所以我们不必担心在驱动程序类文件中需要额外的导入。

然后，我们运行 MapReduce 作业并检查输出，这并不完全像预期的那样。

## 来个围棋英雄

使用 Java API 和前面的 ChainMapper 示例重新实现以前用 Ruby 编写的映射器，这些映射器生成形状、频率和持续时间报告。

### 缩写过多

以下是上一个作业的结果文件中的前几个条目：

```
AB      286
AD      6
AE      7
AI      6
AK      234
AL      548
AM      22
AN      161
…
```

该文件有 186 个不同的双字符条目。 显然，我们从位置域提取最终字符有向图的方法不够健壮。

我们的数据存在许多问题，在手动分析源文件后，这些问题变得非常明显：

*   国家缩略语的大写不一致
*   有相当数量的目击事件来自美国以外，尽管它们可能遵循类似的`(city, area)`模式，但这个缩写并不是我们预期的 50 个缩写之一
*   有些字段根本不遵循该模式，但我们的正则表达式仍会捕获这些字段

我们需要过滤这些结果，理想的做法是将美国记录归一化为正确的州输出，并将其他所有数据收集到更广泛的类别中。

要执行这项任务，我们需要向映射器添加一些关于有效的美国州缩写是什么的概念。 当然，我们可以将其硬编码到映射器中，但这似乎不正确。 虽然我们目前将所有非美国目击事件作为一个单一类别对待，但我们可能希望随着时间的推移延长这一类别，或许还可以按国家进行分类。 如果我们硬编码缩写，则每次都需要重新编译我们的映射器。

### 使用分布式缓存

Hadoop 为我们提供了另一种机制来实现跨作业中的所有任务共享引用数据的目标，即分布式缓存。 这可用于有效地使映射或减少任务使用的个公共只读文件可用于所有节点。 文件可以是文本数据，就像本例中一样，但也可以是其他 JAR、二进制数据或归档；任何事情都是可能的。

要分发的文件放在 HDFS 上，并添加到作业驱动程序中的 DistributedCache。 Hadoop 在作业执行之前将文件复制到每个节点的本地文件系统，这意味着每个任务都可以本地访问这些文件。

另一种方法是将所需文件捆绑到提交给 Hadoop 的作业 JAR 中。 这确实将数据绑定到作业 JAR，使得跨作业共享变得更加困难，并且需要在数据更改时重新构建 JAR。

# 行动时间-使用分布式缓存改善位置输出

现在，让我们使用分布式缓存在整个集群中共享美国州名称和缩写的列表：

1.  在本地文件系统上创建名为`states.txt`的数据文件。 它应该将州缩写和全名制表符分开，每行一个。 或从本书主页检索该文件。 该文件的开头应如下所示：

    ```
    AL      Alabama
    AK      Alaska
    AZ      Arizona
    AR      Arkansas
    CA      California

    …
    ```

2.  将文件放在 HDFS 上：

    ```
    $ hadoop fs -put states.txt states.txt

    ```

3.  将前面的个`UFOLocation.java`文件复制到 UFOLocation2.java 文件，并通过添加以下导入语句进行更改：

    ```
    import java.io.* ;
    import java.net.* ;
    import java.util.* ;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.filecache.DistributedCache ;
    ```

4.  在设置作业名称后，将以下行添加到驱动程序主方法：

    ```
    DistributedCache.addCacheFile(new URI ("/user/hadoop/states.txt"), conf) ;
    ```

5.  按如下方式替换`map`类：

    ```
        public static class MapClass extends MapReduceBase
    implements Mapper<LongWritable, Text, Text, LongWritable>
        {

            private final static LongWritable one = new LongWritable(1);
            private static Pattern locationPattern = Pattern.compile(
    "[a-zA-Z]{2}[^a-zA-Z]*$") ;
            private Map<String, String> stateNames ;

            @Override
            public void configure( JobConf job)
            {
                try
                {
                    Path[] cacheFiles = DistributedCache.getLocalCacheFiles(job) ;
                    setupStateMap( cacheFiles[0].toString()) ;
                } catch (IOException e) 
    {
    System.err.println("Error reading state file.") ;
                        System.exit(1) ;
    }
            }

            private void setupStateMap(String filename) 
    throws IOException
            {
                Map<String, String> states = new HashMap<String, 
    String>() ;
                BufferedReader reader = new BufferedReader( new FileReader(filename)) ;
                String line = reader.readLine() ;
                while (line != null)
                {
                    String[] split = line.split("\t") ;
                    states.put(split[0], split[1]) ;
                    line = reader.readLine() ;
                }

                stateNames = states ;
            }

            public void map(LongWritable key, Text value,
                OutputCollector<Text, LongWritable> output,
                Reporter reporter) throws IOException
            {
                String line = value.toString();
            String[] fields = line.split("\t") ;
            String location = fields[2].trim() ;
            if (location.length() >= 2)
            {

                Matcher matcher = locationPattern.matcher(location) ;
                if (matcher.find() )
                {
                    int start = matcher.start() ;
                    String state = location.substring(start, start+2) ;

                    output.collect(newText(lookupState(state.toUpperCase())), one);
                }
            }
        }

        private String lookupState( String state)
        {
            String fullName = stateNames.get(state) ;

            return fullName == null? "Other": fullName ;
            }
    }
    ```

6.  编译这些类并将作业提交给 Hadoop。 然后检索结果文件。

## *刚刚发生了什么？*

我们首先创建了将在作业中使用的查找文件，并将其放在 HDFS 上。 要添加到分布式缓存的文件最初必须复制到 HDFS 文件系统。

创建新作业文件后，我们添加了所需的类导入。 然后，我们修改了 Driver 类，将每个节点上要添加到 DistributedCache 的文件添加到其中。 可以通过多种方式指定文件名，但最简单的方式是使用 HDFS 上文件位置的绝对路径。

我们的映射器类有很多变化。 我们添加了一个被覆盖的`configure`方法，该方法用于填充一个映射，该映射将用于将州缩写与其全名相关联。

`configure`方法在任务启动时调用，默认实现不执行任何操作。 在我们的覆盖版本中，我们检索已添加到分布式缓存的文件数组。 因为我们知道缓存中只有一个文件，所以使用该数组中的第一个索引是安全的，并将其传递给一个`utility`方法，该方法解析该文件并使用其内容填充州缩写查找映射。 请注意，一旦检索到文件引用，我们就可以使用标准 Java I/O 类访问该文件；它毕竟只是本地文件系统上的一个文件。

我们添加另一个方法来执行查找，该方法获取从 Location 字段提取的字符串，如果匹配则返回州的全名，否则返回字符串`Other`。 这是在通过`OutputCollector`类写入映射结果之前调用的。

此作业的结果应与以下数据类似：

```
Alabama548
Alaska234
Arizona2097
Arkansas534
California7679
…
Other4531…
…
```

这很好用，但我们在此过程中丢失了一些信息。 在我们的验证映射器中，我们只需删除不符合我们的六个字段标准的任何行。 虽然我们不关心单个丢失的记录，但我们可能会关心丢失的记录数量是否很多。 目前，我们确定这一点的唯一方法是将每个识别的州的记录数相加，然后从文件中的总记录数中减去。 我们还可以尝试让这些数据流经作业的其余部分，以收集在一个特殊的简化密钥中，但这似乎也是错误的。 幸运的是，还有更好的办法。

# 计数器、状态和其他输出

在每个MapReducejob结束时，我们会看到与计数器相关的输出，如下所示：

```
12/02/12 06:28:51 INFO mapred.JobClient: Counters: 22
12/02/12 06:28:51 INFO mapred.JobClient:   Job Counters 
12/02/12 06:28:51 INFO mapred.JobClient:     Launched reduce tasks=1
12/02/12 06:28:51 INFO mapred.JobClient:     Launched map tasks=18
12/02/12 06:28:51 INFO mapred.JobClient:     Data-local map tasks=18
12/02/12 06:28:51 INFO mapred.JobClient:   SkippingTaskCounters
12/02/12 06:28:51 INFO mapred.JobClient:     MapProcessedRecords=61393
…
```

可以添加用户定义的计数器，这些计数器同样将从所有任务中聚合，并在最终输出和 MapReduce web UI 中报告。

# 执行操作的时间-创建计数器、任务状态和写入日志输出

我们将修改我们的`UFORecordValidationMapper`以报告有关跳过记录的统计信息，并突出显示用于记录有关作业信息的其他一些工具：

1.  创建以下内容作为`UFOCountingRecordValidationMapper.java`文件：

    ```
    import java.io.IOException;

    import org.apache.hadoop.io.* ;
    import org.apache.hadoop.mapred.* ;
    import org.apache.hadoop.mapred.lib.* ;

    public class UFOCountingRecordValidationMapper extends MapReduceBase
    implements Mapper<LongWritable, Text, LongWritable, Text>
    {

        public enum LineCounters
        {
            BAD_LINES,
            TOO_MANY_TABS,
            TOO_FEW_TABS
        } ;

        public void map(LongWritable key, Text value,
            OutputCollector<LongWritable, Text> output,
            Reporter reporter) throws IOException
        {
            String line = value.toString();

            if (validate(line, reporter))
    Output.collect(key, value);
        }

        private boolean validate(String str, Reporter reporter)
        {
            String[] parts = str.split("\t") ;

            if (parts.length != 6)
            {
                if (parts.length < 6)
                {
    reporter.incrCounter(LineCounters.TOO_FEW_TABS, 1) ;
                }
                else
                {
                    reporter.incrCounter(LineCounters.TOO_MANY_TABS, 1) ;
                }

                reporter.incrCounter(LineCounters.BAD_LINES, 1) ;

    if((reporter.getCounter(
    LineCounters.BAD_LINES).getCounter()%10)
    == 0)
                {
                    reporter.setStatus("Got 10 bad lines.") ;
                    System.err.println("Read another 10 bad lines.") ;
                }

                return false ;
            }
            return true ;
        }
            }
    ```

2.  复制`UFOLocation2.java`文件作为`UFOLocation3.java`文件，以使用此新映射器而不是`UFORecordValidationMapper`：

    ```
    …
            JobConf mapconf1 = new JobConf(false) ;
            ChainMapper.addMapper( conf, 
    UFOCountingRecordValidationMapper.class,
                LongWritable.class, Text.class, LongWritable.class, 
    Text.class,
                true, mapconf1) ;
    ```

3.  编译文件，将它们打包，然后将作业提交给 Hadoop：

    ```
    …
    12/02/12 06:28:51 INFO mapred.JobClient: Counters: 22
    12/02/12 06:28:51 INFO mapred.JobClient:   UFOCountingRecordValidationMapper$LineCounters
    12/02/12 06:28:51 INFO mapred.JobClient:     TOO_MANY_TABS=324
    12/02/12 06:28:51 INFO mapred.JobClient:     BAD_LINES=326
    12/02/12 06:28:51 INFO mapred.JobClient:     TOO_FEW_TABS=2
    12/02/12 06:28:51 INFO mapred.JobClient:   Job Counters 

    ```

4.  Use a web browser to go to the MapReduce web UI (remember by default it is on port 50030 on the JobTracker host). Select the job at the bottom of the **Completed Jobs** list and you should see a screen similar to the following screenshot:

    ![Time for action – creating counters, task states, and writing log output](graphics/7300_04_01.jpg)

5.  Click on the link to the map tasks and you should see an overview screen like the following screenshot:

    ![Time for action – creating counters, task states, and writing log output](graphics/7300_04_02.jpg)

6.  For one of the tasks with our custom status message, click on the link to its counters. This should give a screen similar to the one shown as follows:

    ![Time for action – creating counters, task states, and writing log output](graphics/7300_04_03.jpg)

7.  Go back to the task list and click on the task ID to get the task overview similar to the following screenshot:

    ![Time for action – creating counters, task states, and writing log output](graphics/7300_04_04.jpg)

8.  Under the **Task Logs** column are options for the amount of data to be displayed. Click on **All** and the following screenshot should be displayed:

    ![Time for action – creating counters, task states, and writing log output](graphics/7300_04_05.jpg)

9.  现在登录到其中一个任务节点，查看存储在`hadoop/logs/userlogs`下的文件。 每个任务尝试都有一个目录，每个目录中都有几个文件；要查找的文件是`stderr`。

## *刚刚发生了什么？*

为了添加新的计数器，我们需要做的第一件事是创建一个标准的 Java 枚举来容纳它们。 在本例中，我们创建了 Hadoop 认为的名为**LineCounters**的计数器组，其中有三个计数器表示坏行的总数，而更细粒度的计数器表示字段太少或太多的行数。 这就是创建一组新计数器所需做的全部工作；定义枚举，一旦开始设置计数器值，框架将自动理解它们。

要添加到计数器，我们只需通过`Reporter`对象将其递增，在这里的每种情况下，我们都会在每次遇到坏行时添加一行，一行的字段少于六个，另一行的字段多于六个。

我们还检索任务的`BAD_LINE`计数器，如果它是 10 的倍数，则执行以下操作：

*   设置任务状态以反映此事实
*   使用标准的 Java`System.err.println`机制向`stderr`编写类似的消息

然后，我们转到 MapReduce UI，验证是否可以在作业概述中看到计数器总数，以及在任务列表中看到带有自定义状态消息的任务。

然后，我们浏览 Web 用户界面，查看单个作业的计数器，然后在我们看到的任务的详细信息页面下，我们可以单击该任务的日志文件。

然后，我们查看了其中一个节点，发现 Hadoop 还捕获了文件系统`{HADOOP_HOME}/logs/userlogs`目录下的目录中每个任务的日志。 在每个任务尝试的子目录下，有用于标准流和常规任务日志的文件。 正如您将看到的，繁忙的节点最终可能会有大量的任务日志目录，并且识别感兴趣的任务目录并不总是那么容易。 事实证明，Web 界面对这些数据的查看效率更高。

### 提示

如果您使用的是 Hadoop`context`对象 API，则可以通过`Context.getCounter().increment()`方法访问计数器。

## 信息太多了！

在不太担心如何从我们的工作中获得地位和其他信息之后，我们似乎突然有了太多令人困惑的选择。 事实是，尤其是在运行完全分布式集群时，确实无法避免数据可能分布在每个节点上的事实。 使用 Java 代码，我们不能像使用 Ruby 流任务那样，在命令行上轻松地模拟它的用法；因此，需要仔细考虑在运行时需要哪些信息。 这应包括有关一般作业操作(附加统计数据)以及可能需要进一步调查的问题指标的详细信息。

计数器、任务状态消息和老式的 Java 日志记录可以协同工作。 如果存在您关心的情况，请将其设置为将记录每次发生的计数器，并考虑设置遇到该情况的任务的状态消息。 如果有特定数据，则将其写入`stderr`。 由于计数器非常容易看到，因此如果发生感兴趣的情况，您可以很快知道作业完成后的情况。 从这里，您可以转到 web 用户界面，并一目了然地看到遇到这种情况的所有任务。 在这里，您可以单击查看任务的更详细日志。

事实上，您不需要等到作业完成；计数器和任务状态消息会随着作业的进行而在 Web 用户界面中更新，因此您可以在计数器或任务状态消息提醒您注意此情况时立即开始调查。 这在运行时间很长的作业中特别有用，因为错误可能会导致您中止作业。

# 摘要

本章介绍了 MapReduce 作业的开发，重点介绍了您可能经常遇到的一些问题和方法。 特别是，我们了解了 Hadoop Streaming 如何提供一种使用脚本语言编写映射和减少任务的方法，以及使用 Streaming 如何成为作业原型和初始数据分析早期阶段的有效工具。

我们还了解到，使用脚本语言编写任务可以提供使用命令行工具直接测试和调试代码的额外好处。 在 Java API 中，我们看到了`ChainMapper`类，它提供了一种将复杂的地图任务分解成一系列更小、更集中的任务的有效方法。

然后，我们了解了分布式缓存如何提供在所有节点之间高效共享数据的机制。 它将文件从 HDFS 复制到每个节点上的本地文件系统，从而提供对数据的本地访问。 我们还了解了如何通过为计数器组定义 Java 枚举并使用框架方法递增其值来添加作业计数器，以及如何组合使用计数器、任务状态消息和调试日志来开发高效的作业分析工作流。

我们希望您在开发 MapReduce 作业时会经常遇到这些技术和想法。 在下一章中，我们将探索一系列更高级的技术，这些技术不太经常遇到，但当它们出现时，它们是无价的。