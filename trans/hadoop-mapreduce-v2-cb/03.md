# 三、Hadoop 基础知识——配置、单元测试和其他 API

在本章中，我们将介绍：

*   针对群集部署优化 Hadoop Yarn和 MapReduce 配置
*   共享用户 Hadoop 群集-使用公平和容量调度器
*   将类路径优先级设置为用户提供的 JAR
*   推测性地执行分散的任务
*   使用 MRUnit 对 Hadoop MapReduce 应用进行单元测试
*   使用 MiniYarnCluster 集成测试 Hadoop MapReduce 应用
*   添加新的 DataNode
*   停用数据节点
*   使用多个磁盘/卷并限制 HDFS 磁盘使用
*   设置 HDFS 块大小
*   设置文件复制系数
*   使用 HDFS Java API

# 简介

本章介绍如何在 Hadoop 群集中执行高级管理步骤，如何为 Hadoop MapReduce 程序开发单元和集成测试，以及如何使用 HDFS 的 Java API。 本章假设您已经阅读了第一章，并且已经在集群或伪分布式安装中安装了 Hadoop。

### 备注

**示例代码和数据**

本书的示例代码文件可以在 giHub 的[https://github.com/thilg/hcb-v2](https://github.com/thilg/hcb-v2)中找到。 代码库的`chapter3`文件夹包含本章的示例源代码文件。

可以通过在代码库的`chapter3`文件夹中发出`gradle build`命令来编译和构建示例代码。 Eclipse IDE 的项目文件可以通过在代码库的主文件夹中运行`gradle eclipse`命令来生成。 IntelliJ IDEA IDE 的项目文件可以通过在代码存储库的主文件夹中运行`gradle idea`命令来生成。

# 针对群集部署优化 Hadoop Yarn和 MapReduce 配置

在本食谱中，我们探索了 Hadoop Yarn和 Hadoop MapReduce 的一些重要配置选项。 商业 Hadoop 发行版通常提供基于 GUI 的方法来指定 Hadoop 配置。

Yarn根据应用发出的资源请求和集群的可用资源容量为应用分配资源容器。 应用的资源请求将由所需容器的数量和每个容器的资源要求组成。 目前，大多数容器资源需求都是使用内存量指定的。 因此，我们在本配方中的重点将主要放在配置Yarn集群的内存分配上。

## 做好准备

按照第一章中的食谱设置 Hadoop 集群。

## 怎么做……

以下说明将向您展示如何在Yarn集群中配置内存分配。 每个节点的任务数是使用以下配置得出的：

1.  以下属性指定辅助节点中Yarn容器可以使用的内存量(RAM)。 建议将其设置为略小于节点中存在的物理 RAM 的大小，从而为操作系统和其他非 Hadoop 进程留出一些内存。 在`yarn-site.xml`文件中添加或修改以下行：

    ```scala
    <property>
      <name>yarn.nodemanager.resource.memory-mb</name>
      <value>100240</value>
    </property>
    ```

2.  The following property specifies the minimum amount of memory (RAM) that can be allocated to a YARN container in a worker node. Add or modify the following lines in the `yarn-site.xml` file to configure this property.

    如果我们假设所有Yarn资源请求请求容器只具有最小内存量，则在一个节点中可以执行的最大并发资源容器数等于*(步骤 1 中指定的每个节点的Yarn内存)/(下面配置的Yarn最小分配)*。 基于此关系，我们可以使用以下属性的值来实现每个节点所需的资源容器数量。

    每个节点的资源容器数量建议小于或等于*(2*个 CPU 核)*或*(2*磁盘数量)*中的最小值。

    ```scala
    <property>
      <name>yarn.scheduler.minimum-allocation-mb</name>
      <value>3072</value>
    </property>
    ```

3.  通过运行`HADOOP_HOME`目录中的`sbin/stop-yarn.sh`和`sbin/start-yarn.sh`来重新启动 SPAINE ResourceManager 和 NodeManager 服务。

以下说明将向您展示如何配置 MapReduce 应用的内存要求。

1.  以下属性定义了每个 Map 和 Reduce 任务可用的最大内存量(RAM)。 当 MapReduce 应用为 Map 和 Reduce 任务容器从 YAR 请求资源时，将使用这些内存值。 将以下行添加到`mapred-site.xml`文件：

    ```scala
    <property>
      <name>mapreduce.map.memory.mb</name>
      <value>3072</value>
    </property>
    <property>
      <name>mapreduce.reduce.memory.mb</name>
      <value>6144</value>
    </property>
    ```

2.  以下属性分别定义了 Map 和 Reduce 任务的 JVM 堆大小。 将这些值设置为略小于步骤 4 中的对应值，以便它们不会超过Yarn容器的资源限制。 将以下行添加到`mapred-site.xml`文件：

    ```scala
    <property>
      <name>mapreduce.map.java.opts</name>
      <value>-Xmx2560m</value>
    </property>
    <property>
      <name>mapreduce.reduce.java.opts</name>
      <value>-Xmx5120m</value>
    </property>
    ```

## 它是如何工作的.

我们可以通过以下四个配置文件控制 Hadoop 配置。 Hadoop在群集重新启动后从以下配置文件重新加载配置：

*   `core-site.xml`：包含整个 Hadoop 发行版通用的配置
*   `hdfs-site.xml`：包含 HDFS 的配置
*   `mapred-site.xml`：包含 MapReduce 的配置
*   `yarn-site.xml`：包含Yarn资源管理器和节点管理器进程的配置

每个配置文件都有以 XML 格式表示的名称-值对，定义了 Hadoop 的不同方面的配置。 下面是配置文件中的属性示例。 `<configuration>`标记是顶级父 XML 容器，定义各个属性的`<property>`标记被指定为`<configuration>`标记内的子标记：

```scala
<configuration>
   <property>
      <name>mapreduce.reduce.shuffle.parallelcopies</name>
      <value>20</value>
   </property>
...
</configuration>
```

某些配置可以使用 Hadoop MapReduce 作业驱动程序代码中的`job.getConfiguration().set(name, value)`方法按作业进行配置。

## 还有更多...

在 Hadoop 中定义了许多类似的重要配置属性。 以下是其中一些建议：

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

**conf/core-site.xml**

 |
| --- |
| **名称** | **默认值** | **说明** |
| `fs.inmemory.size.mb` | `200` | 分配给内存中文件系统的内存量，用于合并减少器上的映射输出(MB |
| `io.file.buffer.size` | `131072` | 序列文件使用的读/写缓冲区的大小 |

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

**conf/mapred-site.xml**

 |
| --- |
| **名称** | **默认值** | **说明** |
| `mapreduce.reduce.shuffle.parallelcopies` | `20` | Reduce 步骤将执行的从多个并行作业获取输出的最大并行副本数 |
| `mapreduce.task.io.sort.factor` | `50` | 排序文件时合并的最大流数 |
| `mapreduce.task.io.sort.mb` | `200` | 以 MB 为单位对数据进行排序时的内存限制 |

<colgroup><col style="text-align: left"> <col style="text-align: left"> <col style="text-align: left"></colgroup> 
| 

**conf/hdfs-site.xml**

 |
| --- |
| 

**名称**

 | 

**默认值**

 | 

**说明**

 |
| --- | --- | --- |
| `dfs.blocksize` | `134217728` | HDFS 数据块大小 |
| `dfs.namenode.handler.count` | `200` | 在 NameNode 中处理 RPC 调用的服务器线程数 |

### 备注

您可以在[http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/DeprecatedProperties.html](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/DeprecatedProperties.html)中找到最新版本的 Hadoop 中不推荐使用的属性列表以及它们的新替换属性。

以下文档提供了属性列表、它们的默认值以及前面提到的每个配置文件的说明：

*   **通用****配置**：[http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml)
*   **hdfs****配置**：[https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml)
*   **Yarn****配置**：[http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)
*   **MapReduce****配置**：[http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml](http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml)

# 共享用户 Hadoop 群集-使用公平和容量调度器

HadoopYarn调度程序负责将资源分配给用户提交的应用。 在 Hadoop Yarn中，除了 MapReduce 应用之外，这些应用还可以是任何Yarn应用。 目前默认的Yarn资源分配是基于应用的内存需求，也可以额外配置基于 CPU 等其他资源的资源分配。

Hadoop Yarn支持可插拔调度框架，其中集群管理员可以选择为集群选择合适的调度器。 默认情况下，YAINE支持**先进先出**(**FIFO**)调度器，该调度器使用作业队列以与作业到达时相同的顺序执行作业。 但是，FIFO 调度可能不是大型多用户 Hadoop 部署的最佳选择，在这种部署中，群集资源必须在不同的用户和不同的应用之间共享，以确保最大限度地提高群集的工作效率。 请注意，商业 Hadoop 版本可能使用不同的调度器，如公平调度器(例如 Cloudera CDH)或容量调度器(例如 Hortonworks HDP)作为默认Yarn调度器。

在中，除了默认的 FIFO调度器之外，Yarn还包含以下两个调度器(如果需要，您也可以编写自己的调度器)：

*   **公平调度器**：公平调度器允许所有作业接收相等份额的资源。 当资源可用时，会将资源分配给新提交的作业，直到所有提交和运行的作业都具有相同的资源量。 公平调度程序可确保短作业以实际速度完成，同时不会使长时间运行的较大作业处于饥饿状态。 使用公平调度器，还可以定义多个队列和队列层次结构，并保证每个队列的最小资源，其中特定队列中的作业平等地共享资源。 分配给任何空队列的资源在具有活动作业的队列之间分配。 公平调度程序还允许我们设置作业优先级，用于计算队列中的资源分配比例。
*   **容量调度器**：容量调度器允许在个组织实体之间共享大型集群，同时确保每个实体的有保证的容量，并且没有单个用户或作业占用所有资源。 这使组织可以通过维护在不同实体之间共享的集中式 Hadoop 群集来实现规模经济。 为了实现这一点，容量调度器定义队列和队列层次结构，每个队列具有保证的容量。 容量调度程序允许作业使用其他队列中的多余资源(如果有的话)。

## 怎么做……

本食谱介绍如何在 Hadoop 中更改调度程序：

1.  关闭 Hadoop 群集。
2.  将以下内容添加到`yarn-site.xml file`：

    ```scala
    <property>
      <name>yarn.resourcemanager.scheduler.class</name>
     <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
    </property>
    ```

3.  重新启动 Hadoop 群集。
4.  转到安装中的`http://<master-noe>:8088/cluster/scheduler`，验证是否已应用新的调度程序。

## 它是如何工作的.

执行上述步骤后，Hadoop 将在启动时加载新的调度程序设置。 公平调度程序在用户之间共享等量的资源，除非另有配置。

我们可以提供 XML 格式的分配文件，使用`yarn-site.xml`文件中的`yarn.scheduler.fair.allocation.file`属性为公平调度器定义队列。

有关公平调度器及其配置的更多详细信息，请参阅[https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html)。

## 还有更多...

您可以通过将以下内容添加到`yarn-site.xml file`并重新启动群集来启用容量计划程序：

```scala
<property>
  <name>yarn.resourcemanager.scheduler.class</name>
<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
</property>
```

可以使用 ResourceManager 节点的 Hadoop 配置目录中的`capacity-scheduler.xml`文件配置容量调度器。 在 Yar ResourceManager 节点中发出以下命令以加载配置并刷新队列：

```scala
$ yarn rmadmin -refreshQueues

```

有关容量调度器及其配置的更多详细信息，请参阅[http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html)。

# 将类路径优先级设置为用户提供的 JAR

在开发 Hadoop MapReduce 应用时，您可能会遇到 MapReduce 应用需要 Hadoop 中已包含的较新版本的辅助库的情况。 默认情况下，Hadoop 将类路径优先于 Hadoop 包含的库，这可能会导致与您的应用提供的库的版本冲突。 本食谱向您展示了如何配置 Hadoop，以便为用户提供的库提供类路径优先级。

## 怎么做……

以下步骤向您展示了如何将外部库添加到 Hadoop 任务类路径，以及如何为用户提供的 JAR 提供优先级：

1.  在 MapReduce 计算的驱动程序中设置以下属性：

    ```scala
    job.getConfiguration().set("mapreduce.job.user.classpath.first","true");
    ```

2.  使用 hadoop 命令中的`–libjars`选项提供您的库，如下所示：

    ```scala
    $hadoop jar hcb-c3-samples.jar \ 
    chapter3.WordCountWithTools \
    –libjars guava-15.0.jar \
    InDir OutDir …

    ```

## 它是如何工作的.

Hadoop 将把`–libjars`指定的 JAR 复制到 Hadoop`DistributedCache`中，并且它们将可用于属于该特定作业的所有任务的类路径。 设置`mapreduce.user.classpath.first`时，用户提供的 JAR 将附加到类路径中的默认 Hadoop JAR 和 Hadoop 依赖项之前。

# 推测性地执行分散的任务

使用 Hadoop MapReduce 的主要优势之一是框架管理的容错。 在执行大规模分布式计算时，部分计算可能会由于网络故障、磁盘故障和节点故障等外部原因而失败。 当 Hadoop 检测到无响应的任务或失败的任务时，Hadoop 将在新节点中重新执行这些任务。

Hadoop 群集可能由异构节点组成，因此可能会有速度很慢的节点，也可能会有速度很快的节点。 一些速度较慢的节点和在这些节点上执行的任务可能会控制计算的执行时间。 Hadoop 引入了推测性的执行优化，以避免这些运行缓慢的任务，这些任务被称为**落后者**。

当计算的大部分 Map(或 Reduce)任务完成时，Hadoop 推测性执行功能将在可用的备用节点中调度剩余缓慢任务的重复执行。 任务的缓慢程度取决于相同计算的其他任务所花费的运行时间。 Hadoop 将从一组重复任务中选择第一个已完成任务的结果，并终止该任务的任何其他重复执行。

## 怎么做……

默认情况下，Hadoop 中为 Map 和 Reduce 任务启用了推测性执行。 如果由于某种原因，您的计算不需要这样的重复执行，您可以禁用(或启用)推测性执行，如下所示：

1.  运行将以下选项作为参数传递的 wordcount 示例：

    ```scala
    $ hadoop jar hcb-c32-samples.jar chapter3.WordCountWithTools \
     –Dmapreduce.map.speculative=false \
     –Dmapreduce.reduce.speculative=false \
     /data/input1 /data/output1

    ```

2.  但是，仅当作业实现`org.apache.hadoop.util.Tools`接口时，上述命令才有效。 否则，请使用以下方法在 MapReduce 驱动程序中设置这些属性：
    *   对于整个作业，使用`job.setSpeculativeExecution(boolean specExec)`
    *   对于地图任务，使用`job.setMapSpeculativeExecution(boolean specExec)`
    *   对于 Reduce 任务，使用`Job.setReduceSpeculativeExecution(boolean specExec)`

## 还有更多...

您可以使用属性`mapreduce.map.maxattempts`和`mapreduce.reduce.maxattempts`分别为 Map 和 Reduce 任务配置任务的最大重试次数。 Hadoop 在任务超过给定的重试次数后将其声明为失败。 您还可以使用`JobConf.setMaxMapAttempts()`和`JobConf.setMaxReduceAttempts()`函数来配置这些属性。 这些属性的默认值为`4`。

# 使用 MRUnit 单元测试 Hadoop MapReduce 应用

**MRUnit**是一个基于 JUnit 的 Java 库，允许用户对 Hadoop MapReduce 程序进行单元测试。 这使得开发和维护 Hadoop MapReduce 代码库变得容易。 MRUnit 支持单独测试`Mappers and Reducers`以及整体测试 MapReduce 计算。 在本食谱中，我们将探索所有三个测试场景。 本食谱中使用的测试程序的源代码位于 Git 存储库的`chapter3\test\chapter3\WordCountWithToolsTest.java`文件中。

## 做好准备

我们使用 Gradle 作为示例代码库的构建工具。 如果您还没有安装 Gradle，请按照[章](01.html "Chapter 1. Getting Started with Hadoop v2")，*Hadoopv2*入门介绍部分中的说明安装 Gradle。

## 怎么做……

以下步骤说明如何使用 MRUnit 执行映射器的单元测试：

1.  在测试类的`setUp`方法中，使用要测试的 Mapper 类初始化 MRUnit`MapDriver`实例。 在本例中，我们将测试我们在前面的食谱中讨论的 Wordcount MapReduce 应用的映射器：

    ```scala
    public class WordCountWithToolsTest {

      MapDriver<Object, Text, Text, IntWritable> mapDriver;

      @Before
      public void setUp() {
        WordCountWithTools.TokenizerMapper mapper = new WordCountWithTools.TokenizerMapper();
        mapDriver = MapDriver.newMapDriver(mapper);
      }
    ……
    }
    ```

2.  Write a test function to test the Mapper logic. Provide the test input to the Mapper using the `MapDriver.withInput` method. Then, provide the expected result of the Mapper execution using the `MapDriver.withOutput` method. Now, invoke the test using the `MapDriver.runTest` method. The `MapDriver.withAll` and `MapDriver.withAllOutput` methods allow us to provide a list of test inputs and a list of expected outputs, rather than adding them individually.

    ```scala
    @Test
      public void testWordCountMapper() throws IOException {
        IntWritable inKey = new IntWritable(0);
        mapDriver.withInput(inKey, new Text("Test Quick"));
        ….
        mapDriver.withOutput(new Text("Test"),new IntWritable(1));
        mapDriver.withOutput(new Text("Quick"),new IntWritable(1));
        …
        mapDriver.runTest();
      }
    ```

    以下步骤将向您展示如何使用 MRUnit 执行 Reducer 的单元测试。

3.  Similar to step 1 and 2, initialize a `ReduceDriver` by providing the Reducer class under test and then configure the `ReduceDriver` with the test input and the expected output. The input to the `reduce` function should conform to a key with a list of values. Also, in this test, we use the `ReduceDriver.withAllOutput` method to provide a list of expected outputs.

    ```scala
    public class WordCountWithToolsTest {
      ReduceDriver<Text,IntWritable,Text,IntWritable> reduceDriver;

    @Before
      public void setUp() {
        WordCountWithTools.IntSumReducer reducer = new WordCountWithTools.IntSumReducer();
        reduceDriver = ReduceDriver.newReduceDriver(reducer);
      }

    @Test
      public void testWordCountReduce() throws IOException {
        ArrayList<IntWritable> reduceInList = new ArrayList<IntWritable>();
        reduceInList.add(new IntWritable(1));
        reduceInList.add(new IntWritable(2));

        reduceDriver.withInput(new Text("Quick"), reduceInList);
        ...
        ArrayList<Pair<Text, IntWritable>> reduceOutList = new ArrayList<Pair<Text,IntWritable>>();
        reduceOutList.add(new Pair<Text, IntWritable> (new Text("Quick"),new IntWritable(3)));
        ...
        reduceDriver.withAllOutput(reduceOutList);
        reduceDriver.runTest();
      }
    }
    ```

    以下步骤向您展示了如何使用 MRUnit 对整个 MapReduce 计算执行单元测试。

4.  在此步骤中，通过提供要测试的 MapReduce 程序的 Mapper 类和 Reducer 类来初始化a`MapReduceDriver`。 然后，使用测试输入数据和预期输出数据配置`MapReduceDriver`。 执行时，此测试将执行从 Map 输入阶段到 Reduce 输出阶段的 MapReduce 执行流。 也可以为该测试提供组合器实现。

    ```scala
    public class WordCountWithToolsTest {
      ……
      MapReduceDriver<Object, Text, Text, IntWritable, Text, IntWritable> mapReduceDriver;

    @Before
      public void setUp() {
        ....
        mapReduceDriver = MapReduceDriver.newMapReduceDriver(mapper, reducer);
      }

    @Test
      public void testWordCountMapReduce() throws IOException {

        IntWritable inKey = new IntWritable(0);
        mapReduceDriver.withInput(inKey, new Text("Test Quick"));
        ……
        ArrayList<Pair<Text, IntWritable>> reduceOutList = new ArrayList<Pair<Text,IntWritable>>();
        reduceOutList.add(new Pair<Text, IntWritable>(new Text("Quick"),new IntWritable(2)));
        ……
        mapReduceDriver.withAllOutput(reduceOutList);
        mapReduceDriver.runTest();
      }
    }
    ```

5.  Gradle Build脚本(或任何其他 Java 构建机制)可以配置为对每个构建执行这些单元测试。 我们可以将 MRUnit 依赖项添加到 Gradle 构建(`chapter3/build.gradle`)文件，如下所示：

    ```scala
    dependencies {
      testCompile group: 'org.apache.mrunit', name: 'mrunit', version: '1.1.+',classifier: 'hadoop2'
    ……
    }
    ```

6.  使用以下 Gradle 命令仅执行`WordCountWithToolsTest`单元测试。 此命令执行与模式`**/WordCountWith*.class`：

    ```scala
    $ gradle –Dtest.single=WordCountWith test
    :chapter3:compileJava UP-TO-DATE
    :chapter3:processResources UP-TO-DATE
    :chapter3:classes UP-TO-DATE
    :chapter3:compileTestJava UP-TO-DATE
    :chapter3:processTestResources UP-TO-DATE
    :chapter3:testClasses UP-TO-DATE
    :chapter3:test
    BUILD SUCCESSFUL
    Total time: 27.193 secs

    ```

    匹配的任何测试类
7.  您还可以在 IDE 中执行基于 MRUnit 的单元测试。 您可以使用`gradle eclipse`或`gradle idea`命令分别为 Eclipse 和 IDEA IDE 生成项目文件。

## 另请参阅

*   本章中的*使用 MiniYarnCluster*配方集成测试 Hadoop MapReduce 应用
*   有关使用 MRUnit 的更多信息，请访问[https://cwiki.apache.org/confluence/display/MRUNIT/MRUnit+Tutorial](https://cwiki.apache.org/confluence/display/MRUNIT/MRUnit+Tutorial)

# 使用 MiniYarnCluster 集成测试 Hadoop MapReduce 应用

虽然使用 MRUnit 的单元测试非常有用，但是可能有某些集成测试场景需要在集群环境中进行测试。 Hadoop Yarn的 MiniYARNCluster 是一个集群模拟器，我们可以使用它来为这样的集成测试创建测试环境。 在本食谱中，我们将使用 MiniYARNCluster 执行 WordCountWithTools MapReduce 应用的集成测试。

本食谱中使用的测试程序的源代码位于 Git 存储库的`chapter3\test\chapter3\minicluster\WordCountMiniClusterTest.java`文件中。

## 做好准备

我们使用 Gradle 作为示例代码库的构建工具。 如果您还没有安装 Gradle，请按照[章](01.html "Chapter 1. Getting Started with Hadoop v2")，*Hadoopv2*入门介绍部分中的说明安装 Gradle。 导出指向 JDK 安装的`JAVA_HOME`环境变量。

## 怎么做……

以下步骤显示如何使用`MiniYarnCluster`环境执行 MapReduce 应用的集成测试：

1.  在 JUnit 测试的`setup`方法中，使用`MiniMRClientClusterFactory`创建`MiniYarnCluster`的实例，如下所示。 `MiniMRClientCluster`是`MiniMRYarnCluster`的包装器接口，用于使用 Hadoop 1.x 集群提供支持测试。

    ```scala
    public class WordCountMiniClusterTest {
      private static MiniMRClientCluster mrCluster;
      private class InternalClass {
      }

    @BeforeClass
      public static void setup() throws IOException {
        // create the mini cluster to be used for the tests
        mrCluster = MiniMRClientClusterFactory.create(InternalClass.class, 1,new Configuration());
      }
    }
    ```

2.  确保停止测试的`setup`方法内的集群：

    ```scala
    @AfterClass
      public static void cleanup() throws IOException {
        // stopping the mini cluster
        mrCluster.stop();
      }
    ```

3.  在您的测试方法中，使用我们刚刚创建的`MiniYARNCluster`的 Configuration 对象准备 MapReduce 计算。 提交作业并等待其完成。 然后测试作业是否成功。

    ```scala
    @Test
      public void testWordCountIntegration() throws Exception{
    ……
        Job job = (new WordCountWithTools()).prepareJob(testInput,outDirString, mrCluster.getConfig());
        // Make sure the job completes successfully
        assertTrue(job.waitForCompletion(true));
        validateCounters(job.getCounters(), 12, 367, 201, 201);
      }
    ```

4.  在本例中，我们将使用计数器来验证 MapReduce 计算的预期结果。 您还可以实现逻辑，将计算的输出数据与预期的计算输出进行比较。 但是，必须小心处理由于存在多个 Reduce 任务而可能有多个输出文件的情况。

    ```scala
     private void validateCounters(Counters counters, long mapInputRecords,…) {
     assertEquals("MapInputRecords", mapInputRecords, counters.findCounter("org.apache.hadoop.mapred.Task$Counter", "MAP_INPUT_RECORDS").getValue());
     ………
     }

    ```

5.  使用以下 Gradle 命令仅执行`WordCountMiniClusterTest`JUnit 测试。 该命令执行与模式`**/WordCountMini*.class`匹配的任何测试类。

    ```scala
    $ gradle -Dtest.single=WordCountMini test
    :chapter3:compileJava UP-TO-DATE
    :chapter3:processResources UP-TO-DATE
    :chapter3:classes UP-TO-DATE
    :chapter3:compileTestJava UP-TO-DATE
    :chapter3:processTestResources UP-TO-DATE
    :chapter3:testClasses UP-TO-DATE
    :chapter3:test UP-TO-DATE

    BUILD SUCCESSFUL

    ```

6.  您还可以在 IDE 中执行基于`MiniYarnCluster`的单元测试。 您可以使用`gradle eclipse`或`gradle idea`命令分别为 Eclipse 和 IDEA IDE 生成项目文件。

## 另请参阅

*   本章中的*单元测试 Hadoop MapReduce 应用使用 MRUnit*菜谱
*   [第 4 章](04.html "Chapter 4. Developing Complex Hadoop MapReduce Applications")，*《开发复杂 Hadoop MapReduce 应用》中用于报告自定义指标的*Hadoop 计数器*配方*

# 添加新的 DataNode

此配方向您展示了如何在不重新启动整个集群的情况下向现有 HDFS 集群添加新节点，以及如何在添加新节点后强制 HDFS 重新平衡。 商业 Hadoop 发行版通常提供基于 GUI 的方法来添加和删除 DataNode。

## 做好准备

1.  在新节点上安装 Hadoop 并复制现有 Hadoop 群集的配置文件。 您可以使用`rsync`从另一个节点复制 Hadoop 配置；例如：

    ```scala
    $ rsync -a <master_node_ip>:$HADOOP_HOME/etc/hadoop/ $HADOOP_HOME/etc/hadoop

    ```

2.  确保 Hadoop/HDFS 集群的主节点可以对新节点执行无密码 SSH。 如果您不打算从主节点使用`bin/*.sh`脚本启动/停止群集，则无密码 SSH 设置是可选的。

## 怎么做……

以下步骤将向您展示如何向现有 HDFS 集群添加新的 DataNode：

1.  将新节点的 IP 或 DNS 添加到主节点的`$HADOOP_HOME/etc/hadoop/slaves`文件中。
2.  Start the DataNode on the newly added slave node by using the following command:

    ```scala
    $ $HADOOP_HOME/sbin/hadoop-deamons.sh start datanode

    ```

    ### 提示

    您还可以从主节点使用`$HADOOP_HOME/sbin/start-dfs.sh`脚本在新添加的节点中启动 DataNode 守护进程。 如果要向集群添加多个新的 DataNode，这将非常有用。

3.  检查新从节点中的`$HADOOP_HOME/logs/hadoop-*-datanode-*.log`是否有任何错误。

这些步骤既适用于添加新节点，也适用于重新加入已崩溃并重新启动的节点。

## 还有更多...

同样，您也可以向 Hadoop Yarn集群添加新节点：

1.  使用以下命令在新节点中启动 NodeManager：

    ```scala
    > $HADOOP_HOME/sbin/yarn-deamons.sh start nodemanager

    ```

2.  检查新从节点中的`$HADOOP_HOME/logs/yarn-*-nodemanager-*.log`是否有任何错误。

### 重新平衡 HDFS

当您添加新节点时，HDFS 不会自动重新平衡。 但是，HDFS 提供了可以手动调用的重新平衡器工具。 此工具将跨群集平衡数据块，最高可达可选的阈值百分比。 如果您在其他现有节点中遇到空间问题，重新平衡将非常有用。

1.  Execute the following command:

    ```scala
    > $HADOOP_HOME/sbin/start-balancer.sh –threshold 15

    ```

    (可选)`–threshold`参数指定在标识节点未充分利用或过度利用时要考虑的磁盘容量回旋余地百分比。 未充分利用的数据节点是其利用率小于*(平均利用率阈值)*的节点。 过度利用的 DataNode 是利用率大于(平均利用率+阈值)的节点。 较小的阈值将实现更均匀的节点平衡，但重新平衡将需要更多时间。 默认阈值为 10%。

2.  通过执行`sbin/stop-balancer.sh`命令可以停止重新平衡。
3.  `$HADOOP_HOME/logs/hadoop-*-balancer*.out`文件中提供了重新平衡的摘要。

## 另请参阅

本章中的*取消数据节点*配方。

# 停用数据节点

可能有种情况，您希望从 HDFS 群集中停用一个或多个 DataNode。 本食谱展示了如何优雅地使 DataNode 退役，而不会导致数据丢失。

## 怎么做……

以下步骤向您展示了如何正常停用 DataNode：

1.  如果您的集群没有`exclude`文件，请向集群添加一个`exclude`文件。 在 NameNode 中创建一个空文件，并通过添加以下属性从`$HADOOP_HOME/etc/hadoop/hdfs-site.xml`文件指向该文件。 重新启动 NameNode：

    ```scala
    <property>
    <name>dfs.hosts.exclude</name>
    <value>FULL_PATH_TO_THE_EXCLUDE_FILE</value>
    <description>Names a file that contains a list of hosts that are not permitted to connect to the namenode. The full pathname of the file must be specified. If the value is empty, no hosts are excluded.</description>
    </property>
    ```

2.  将要停用的节点的主机名添加到`exclude`文件。
3.  Run the following command to reload the NameNode configuration:

    ```scala
    $ hdfs dfsadmin –refreshNodes

    ```

    这将启动退役过程。 此过程可能需要大量时间，因为它需要复制数据块，而不会使群集的其他任务不堪重负。

4.  停用过程的进度显示在**停用节点**页面下的 HDFS UI 中。 也可以使用以下命令监视进度。 在停用完成之前不要关闭节点。

    ```scala
    $ hdfs dfsadmin -report
    .....
    .....
    Name: myhost:50010
    Decommission Status : Decommission in progress
    Configured Capacity: ....
    .....

    ```

5.  当您想要将节点重新添加到集群中时，可以从`exclude`文件中删除节点并执行`hdfs dfsadmin –refreshNodes`命令。
6.  可以通过从`exclude`文件中删除节点名称，然后执行`hdfs dfsadmin –refreshNodes`命令来停止停用过程。

## 它是如何工作的.

当某个节点处于停用过程中时，HDFS 会将该节点中的数据块复制到群集中的其他节点。 停用可能是一个缓慢的过程，因为 HDFS 故意缓慢地停用，以避免使群集不堪重负。 在不停用的情况下关闭节点可能会导致数据丢失。

停用完成后，排除文件中提到的节点不允许与 NameNode 通信。

## 另请参阅

本章中*添加新的 DataNode*配方的*重新平衡 HDFS*部分。

# 使用多个磁盘/卷并限制 HDFS 磁盘使用

Hadoop支持为DataNode 数据目录指定多个目录。 此功能允许我们利用多个磁盘/卷在 DataNode 中存储数据块。 Hadoop 尝试在每个目录中存储等量的数据。 它还支持限制 HDFS 使用的磁盘空间量。

## 怎么做……

以下步骤将向您展示如何添加多个磁盘卷：

1.  在每个卷中创建 HDFS 数据存储目录。
2.  找到`hdfs-site.xml`配置文件。 在`dfs.datanode.data.dir`属性下提供与每个卷中的数据存储位置相对应的以逗号分隔的目录列表，如下所示：

    ```scala
    <property>
             <name>dfs.datanode.data.dir</name>
             <value>/u1/hadoop/data, /u2/hadoop/data</value>
    </property>
    ```

3.  为了限制磁盘使用，请将以下属性添加到`hdfs-site.xml`文件中，以便为非 DFS 使用保留空间。 该值指定 HDFS 不能在每个卷上使用的字节数：

    ```scala
      <property>
        <name>dfs.datanode.du.reserved</name>
        <value>6000000000</value>
        <description>Reserved space in bytes per volume. Always leave this much space free for non dfs use.
        </description>
      </property>
    ```

# 设置 HDFS 块大小

HDFS通过将文件分解成粗粒度、固定大小的块来跨群集中存储文件。 默认 HDFS 数据块大小为 64 MB。 数据产品的数据块大小可能会影响文件系统操作的性能，在存储和处理非常大的文件时，较大的数据块大小会更有效。 数据产品的块大小也会影响 MapReduce 计算的性能，因为 Hadoop 的默认行为是为输入文件的每个数据块创建一个 Map 任务。

## 怎么做……

以下步骤显示如何使用 NameNode 配置文件设置 HDFS 块大小：

1.  在`$HADOOP_HOME/etc/hadoop/hdfs-site.xml`文件中添加或修改以下代码。 块大小是使用字节数提供的。 此更改不会更改 HDFS 中已有文件的块大小。 只有更改后复制的文件才具有新的块大小。

    ```scala
    <property>
            <name>dfs.blocksize</name>
            <value>134217728</value>
    </property>
    ```

2.  您还可以为特定文件路径指定不同的 HDFS 块大小。 还可以在从命令行将文件上载到 HDFS 时指定块大小，如下所示：

    ```scala
    $ hdfs dfs \
     -Ddfs.blocksize=134217728 \
     -put data.in foo/test

    ```

## 还有更多...

您还可以在使用 HDFS Java API 创建文件时指定块大小，方法如下：

```scala
public FSDataOutputStream create(Path f,boolean overwrite, int bufferSize, short replication,long blockSize)
```

您可以使用`fsck`命令查找 HDFS 中特定文件路径的块大小和块位置。 您也可以通过从 HDFS 监控控制台浏览文件系统来查找此信息。

```scala
 > $HADOOP_HOME/bin/hdfs fsck \
 /user/foo/data.in \
 -blocks -files -locations
......
/user/foo/data.in 215227246 bytes, 2 block(s): ....
0\. blk_6981535920477261584_1059 len=134217728 repl=1 [hostname:50010]
1\. blk_-8238102374790373371_1059 len=81009518 repl=1 [hostname:50010]

......

```

## 另请参阅

本章中的*设置文件复制因子*配方。

# 设置文件复制系数

HDFS通过将文件分解成粗粒度、固定大小的块来跨群集中存储文件。 这些粗粒度的数据块被复制到不同的 DataNode，主要是出于容错目的。 数据块复制还能够增加 MapReduce 计算的数据局部性，并增加总数据访问带宽。 降低复制系数有助于节省 HDFS 中的存储空间。

**HDFS 复制因子**是可以按文件设置的文件级属性。 本食谱向您展示如何更改 HDFS 部署的默认复制系数(影响随后将创建的新文件)，如何在 HDFS 中创建文件时指定自定义复制系数，以及如何更改 HDFS 中现有文件的复制系数。

## 怎么做……

按照以下说明使用 NameNode 配置设置文件复制因子：

1.  在`$HADOOP_HOME/etc/hadoop/hdfs-site.xml`中添加或修改`dfs.replication`属性。 此更改不会更改 HDFS 中已有文件的复制系数。 只有更改后复制的文件才具有新的复制因子。 请注意，降低复制系数会降低存储文件的可靠性，在处理该数据时还可能导致性能下降。

    ```scala
    <property>
            <name>dfs.replication</name>
            <value>2</value>
    </property>
    ```

2.  上传文件时设置文件复制因子。 您可以在从命令行上传文件时指定复制因子，如下所示：

    ```scala
    $ hdfs dfs \
     -Ddfs.replication=1 \
     -copyFromLocal \
     non-critical-file.txt /user/foo

    ```

3.  更改现有文件路径的文件复制系数。 `setrep`命令可用于通过以下方式更改 HDFS 中已存在的文件或文件路径的复制因子：

    ```scala
    $ hdfs dfs \
     -setrep 2 non-critical-file.txt

    Replication 2 set: hdfs://myhost:9000/user/foo/non-critical-file.txt

    ```

## 它是如何工作的.

看一下下面的命令：

```scala
hdfs dfs -setrep [-R] <path>

```

`setrep`命令的`<path>`参数指定必须更改复制因子的 HDFS 路径。 `–R`选项递归地设置目录中文件和目录的复制因子。

## 还有更多...

使用`ls`命令列出文件时，会显示文件的复制系数：

```scala
$ hdfs fs -ls
Found 1 item
-rw-r--r-- 2 foo supergroup ... /user/foo/non-critical-file.txt

```

文件的复制系数也会显示在监控 UI 的 HDFS 中。

## 另请参阅

本章中的*设置 HDFS 块大小*配方。

# 使用 HDFS Java API

**HDFS Java API**可用于从任何 Java 程序与 HDFS 交互。 此 API 使我们能够从其他 Java程序利用存储在 HDFS 中的数据，并使用其他非 Hadoop 计算框架处理这些数据。 有时，您可能还会遇到希望从 MapReduce 应用中直接访问 HDFS 的用例。 但是，如果您直接从 Map 或 Reduce 任务在 HDFS 中写入或修改文件，请注意您违反了 MapReduce 的无副作用特性，这可能会导致基于您的使用案例的数据一致性问题。

## 怎么做……

以下步骤显示了如何使用 HDFS Java API 通过 Java 程序在 HDFS 安装上执行文件系统操作：

1.  下面的示例程序在 HDFS 中创建一个新文件，在新创建的文件中写入一些文本，然后从 HDFS 读回该文件：

    ```scala
    import java.io.IOException;

    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.FSDataInputStream;
    import org.apache.hadoop.fs.FSDataOutputStream;
    import org.apache.hadoop.fs.FileSystem;
    import org.apache.hadoop.fs.Path;

    public class HDFSJavaAPIDemo {
      public static void main(String[] args) throws IOException {
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(conf);
        System.out.println(fs.getUri());

        Path file = new Path("demo.txt");

        if (fs.exists(file)) {
          System.out.println("File exists.");
        } else {
          // Writing to file
          FSDataOutputStream outStream = fs.create(file);
          outStream.writeUTF("Welcome to HDFS Java API!!!");
          outStream.close();
        }

        // Reading from file
        FSDataInputStream inStream = fs.open(file);
        String data = inStream.readUTF();
        System.out.println(data);
        inStream.close();

        fs.close();
      }
    ```

2.  在源库的`chapter3`文件夹中发出`gradle build`命令，编译并打包前面的程序。 将在`build/libs`文件夹中创建`hcb-c3-samples.jar`文件。
3.  您可以使用以下命令执行前面的示例。 使用`hadoop`脚本运行此示例可确保它使用当前配置的 HDFS 和 Hadoop*类路径*中的必要依赖项。

    ```scala
    $ hadoop jar \
     hcb-c3-samples.jar \
     chapter3.hdfs.javaapi.HDFSJavaAPIDemo

    hdfs://yourhost:9000
    Welcome to HDFS Java API!!!

    ```

4.  使用`ls`命令列出新创建的文件，如下图所示：

    ```scala
    $ hdfs dfs -ls
    Found 1 items
    -rw-r--r--   3 foo supergroup         20 2012-04-27 16:57 /user/foo/demo.txt

    ```

## 它是如何工作的.

为了以编程方式与 HDFS 交互，我们首先需要获取当前配置的文件系统的句柄。 为此，我们实例化一个`Configuration`对象并获得一个`FileSystem`句柄，它将指向我们运行该程序的 Hadoop 环境的 HDFS NameNode。 本章的配置文件系统对象一节讨论了配置`FileSystem`对象的几种替代方法：

```scala
Configuration conf = new Configuration();
FileSystem fs = FileSystem.get(conf);
```

`FileSystem.create(filePath)` 方法在给定的路径中创建一个新文件，并为新创建的文件提供一个`FSDataOutputStream`对象。 `FSDataOutputStream`包装`java.io.DataOutputStream`并允许程序将原始 Java 数据类型写入文件。 如果文件存在，则`FileSystem.Create()`方法重写。 在本例中，该文件将相对于 HDFS 主目录创建，这将产生类似于`/user/<user_name>/demo.txt`的路径。 您的 HDFS 主目录必须事先创建。

```scala
Path file = new Path("demo.txt");
FSDataOutputStream outStream = fs.create(file);
outStream.writeUTF("Welcome to HDFS Java API!!!");
outStream.close();
```

`FileSystem.open(filepath)`打开给定文件的`FSDataInputStream`。 `FSDataInputStream`包装`java.io.DataInputStream`并允许程序从文件中读取原始 Java 数据类型。

```scala
FSDataInputStream inStream = fs.open(file);
String data = inStream.readUTF();
System.out.println(data);
inStream.close();
```

## 还有更多...

HDFS Java API 支持的文件系统操作比我们在前面的示例中使用的多得多。 完整的接口文档可以在[http://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html](http://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html)找到。

### 配置文件系统对象

我们也可以从 Hadoop 环境外部使用 HDFS Java API。 在执行此操作时，我们必须显式配置 HDFS NameNode 和端口。 以下是执行该配置的几种方法：

*   您可以在检索`FileSystem`对象之前将配置文件加载到`configuration`对象，如下所示。 确保将所有 Hadoop 和依赖库添加到类路径中。

    ```scala
    Configuration conf = new Configuration();
    conf.addResource(new Path("/etc/hadoop/core-site.xml"));
    conf.addResource(new Path("/etc/hadoop/conf/hdfs-site.xml"));
    FileSystem fileSystem = FileSystem.get(conf);
    ```

*   您还可以按如下方式指定 NameNode 和端口。 将`NAMENODE_HOSTNAME`和`PORT`替换为 HDFS 安装的 NameNode 的主机名和端口。

    ```scala
    Configuration conf = new Configuration();
    conf.set("fs.defaultFS, "hdfs://NAMENODE_HOSTNAME:PORT");
    FileSystem fileSystem = FileSystem.get(conf);
    ```

HDFS 文件系统 API 是支持多个文件系统的抽象。 如果前面的程序没有找到有效的 HDFS 配置，它将指向本地文件系统，而不是 HDFS。 您可以使用`getUri()`函数标识`fileSystem`对象的当前文件系统，如下所示。 如果它使用的是正确配置的 HDFS，则会产生`hdfs://your_namenode:port`，如果使用的是本地文件系统，则会产生`file:///`。

```scala
fileSystem.getUri();
```

### 检索文件的数据块列表

`the fileSystem`对象的`getFileBlockLocations()`函数允许您检索 HDFS 中存储的文件的数据块列表，以及存储这些块的主机名和块偏移量。 如果您计划在上使用除 Hadoop MapReduce 之外的框架对文件数据执行任何本地操作，此信息将非常有用。

```scala
FileStatus fileStatus = fs.getFileStatus(file);
BlockLocation[] blocks = fs.getFileBlockLocations(
  fileStatus, 0, fileStatus.getLen());
```