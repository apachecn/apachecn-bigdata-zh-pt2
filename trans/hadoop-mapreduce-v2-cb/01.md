# 第 1 章：Hadoop v2 入门

在本章中，我们将介绍以下食谱：

*   在本地计算机上设置独立 Hadoop v2
*   编写字数统计 MapReduce 应用程序，捆绑它，并使用 Hadoop 本地模式运行它
*   向 Wordcount MapReduce 程序添加组合器步骤
*   设置 HDFS
*   使用 Hadoop v2 在分布式群集环境中设置 Hadoop 纱线
*   使用 Hadoop 发行版在分布式群集环境中设置 Hadoop 生态系统
*   HDFS 命令行文件操作
*   在分布式集群环境中运行 WordCount 程序
*   使用 DFSIO 对 HDFS 进行基准测试
*   使用 TeraSort 对 Hadoop MapReduce 进行基准测试

# 简介

我们生活在大数据时代，网络、社交网络、智能手机等现象的指数级增长每天产生数万亿字节的数据。 从分析这些海量数据中获得洞察力已成为许多行业必备的*竞争优势。 然而，这些数据源的大小和可能的非结构化性质使得不可能使用诸如关系数据库之类的传统解决方案来存储和分析这些数据集。*

 *要以有意义和及时的方式存储、处理和分析 PB 级数据，需要许多具有数千个磁盘和数千个处理器的计算节点，以及它们之间高效通信海量数据的能力。 这样的规模使得磁盘故障、计算节点故障、网络故障等故障屡见不鲜，使得容错成为此类系统的一个非常重要的方面。 出现的其他常见挑战包括巨大的资源成本、处理通信延迟、处理异构计算资源、跨节点同步以及负载平衡。 正如您可以推断的那样，开发和维护分布式并行应用程序以在处理所有这些问题的同时处理大量数据并非易事。 这就是 Apache Hadoop 拯救我们的地方。

### 备注

谷歌是首批面临处理海量数据问题的组织之一。 Google 借鉴了函数式编程领域的**map**和**Reduce**范例，构建了一个用于大规模数据处理的框架，并将其命名为**MapReduce**。 在 Google 的基础上，MapReduce是 Google 文件系统，它是一个高吞吐量的并行文件系统，能够使用商用计算机可靠地存储海量数据。 介绍 Google MapReduce 和 Google File System概念的开创性研究出版物可以在[http://research.google.com/archive/mapreduce.html](http://research.google.com/archive/mapreduce.html)和[http://research.google.com/archive/gfs.html](http://research.google.com/archive/gfs.html)找到。

Apache Hadoop MapReduce 是 Google MapReduce 范例中最广为人知、使用最广泛的开源实现。 Apache**Hadoop 分布式文件系统**(**HDFS**)提供了 Google 文件系统概念的开源实现。

Apache Hadoop MapReduce、HDFS 和 YAR 为跨商用计算机群集存储和处理超大型数据集提供了一个可伸缩的、容错的分布式平台。 与传统的**高性能计算**(**HPC**)群集不同，Hadoop使用相同的计算节点集进行数据存储和执行计算，从而允许 Hadoop 通过将计算与存储进行配置来提高大规模计算的性能。 此外，由于使用商用硬件和商用互连，Hadoop 群集的硬件成本比 HPC 群集和数据库设备便宜数量级。 总之，基于 Hadoop 的框架已经成为存储和处理大数据的事实标准。

## Hadoop 分布式文件系统-HDFS

HDFS 是一种块结构的分布式文件系统，旨在将 PB 级的数据可靠地存储在由商用硬件组成的计算集群上。 HDFS 覆盖在计算节点的现有文件系统之上，并通过将文件拆分成更粗粒度的块(例如，128 MB)来存储文件。 HDFS 在处理大文件时性能更好。 HDFS 将大文件的数据块分布到群集的所有节点，以便在处理数据时实现极高的并行聚合读取带宽。 HDFS还将这些数据块的冗余副本存储在多个节点中，以确保可靠性和容错性。 数据处理框架(如 MapReduce)利用这些分布式数据块集和冗余来最大化大型数据集的数据本地处理，其中大多数数据块将在存储它们的同一物理节点中进行本地处理。

HDFS 由**NameNode**和**DataNode**服务组成，为分布式文件系统提供基础。 NameNode 存储、管理和服务文件系统的元数据。 NameNode 不存储任何实际数据块。 DataNode 是一项针对每个节点的服务，用于管理 DataNode 中的实际数据块存储。 在检索数据时，客户端应用程序首先联系 NameNode 以获取请求的数据所在位置的列表，然后直接联系 DataNode 以检索实际数据。 下图概括介绍了 HDFS 的结构：

![Hadoop Distributed File System – HDFS](graphics/5471OS_01_01.jpg)

Hadoop v2 为 HDFS 带来了几个性能、可伸缩性和可靠性改进。 其中最重要的是**High Availability**(**HA**)对 HDFSNameNode 的支持，它为 HDFS NameNode 服务提供手动和自动故障转移功能。 这解决了 HDFS 广为人知的 NameNode 单点故障弱点。 自动 NameNode Hadoop v2 的高可用性使用 Apache ZooKeeper 进行故障检测和活动 NameNode 选举。 另一个重要的新特性是对 HDFS 联合的支持。 HDFS 联合支持在单个 HDFS 群集中使用个独立的 HDFS 命名空间。 这些命名空间将由独立的 NameNode 管理，但是共享集群的 DataNode 来存储数据。 HDFS 联合功能通过允许我们分配 NameNode 的工作负载，提高了 HDFS 的水平可伸缩性。 Hadoop v2 中 HDFS 的其他重要改进包括对 HDFS 快照的支持、异构存储层次结构支持(Hadoop 2.3 或更高版本)、内存数据缓存支持(Hadoop 2.3 或更高版本)以及许多性能改进。

几乎所有 Hadoop 生态系统数据处理技术都使用 HDFS 作为主要数据存储。 HDFS 可以被认为是 Hadoop 生态系统中最重要的组件，因为它在 Hadoop 体系结构中具有中心性质。

## Hadoop 纱

**纱线**(**还有另一个资源谈判者**)是 Hadoopv2 中引入的主要新改进。 Yar 是一个资源管理系统，它允许多个分布式处理框架有效地共享 Hadoop 集群的计算资源，并利用存储在 HDFS 中的数据。 Year 是 Hadoopv2 生态系统中的核心组件，为许多不同类型的分布式应用程序提供了一个公共平台。

基于批处理的 MapReduce 框架是 Hadoopv1 中唯一本地支持的数据处理框架。 虽然 MapReduce 可以很好地分析大量数据，但 MapReduce 本身不足以支持不断增加的其他分布式处理用例，例如实时数据计算、图形计算、迭代计算和实时数据查询。 YAIN 的目标是允许用户利用多个分布式应用程序框架，这些框架并排提供这样的功能，共享单个集群和 HDFS 文件系统。 当前纱线应用程序的一些示例包括 MapReduce 框架、TEZ 高性能处理框架、Spark 处理引擎和 Storm 实时流处理框架。 下图描述了纱线生态系统的高级架构：

![Hadoop YARN](graphics/5471OS_01_02.jpg)

YAR ResourceManager 进程是中央资源调度器，用于管理资源并将其分配给提交给集群的不同应用程序(也称为作业)。 Yanson NodeManager 是一个针对每个节点的进程，用于管理单个计算节点的资源。 ResourceManager 的调度器组件响应于应用程序做出的资源请求来分配资源，考虑到集群容量和可以通过纱线策略插件框架指定的其他调度策略。

纱线有一个叫做容器的概念，它是资源分配的单位。 每个已分配的容器都有权访问特定计算节点中的一定数量的 CPU 和内存。 应用程序可以通过指定所需的容器数量以及每个容器所需的 CPU 和内存来请求来自纱线的资源。

ApplicationMaster 是一个针对每个应用程序的进程，它协调单个应用程序的计算。 执行纱线应用程序的第一步是部署 ApplicationMaster。 在 YAINE 客户端提交应用程序后，ResourceManager 为该应用程序分配一个容器并部署 ApplicationMaster。 部署后，ApplicationMaster 负责向 ResourceManager 请求和协商必要的资源容器。 一旦 ResourceManager 分配了资源，ApplicationMaster 就会与 NodeManagers 协调以启动和监视分配的资源中的应用程序容器。 将应用程序协调职责转移到 ApplicationMaster 减轻了 ResourceManager 的负担，使其能够只专注于管理群集资源。 此外，对于每个提交的应用程序都有单独的 ApplicationMaster 可以提高集群的可伸缩性，而不是使用单个进程瓶颈来协调所有应用程序实例。 下图描述了将 MapReduce 应用程序提交到群集时各种纱线组件之间的交互：

![Hadoop YARN](graphics/5471OS_01_03.jpg)

虽然Yar 支持许多不同的分布式应用执行框架，但本书主要关注传统的 MapReduce 和相关技术。

## Hadoop MapReduce

HadoopMapReduce 是一个数据处理框架，可用于处理存储在 HDFS 中的海量数据。 正如我们前面提到的，以可靠和高效的方式分布式处理海量数据并非易事。 Hadoop MapReduce 旨在通过提供程序的自动并行化和框架管理的容错支持，为程序员提供清晰的抽象，从而简化用户。

MapReduce 编程模型由 Map 和 Reduce 函数组成。 Map 函数接收输入数据的每条记录(文件的行、数据库的行等)作为键-值对，并输出键-值对作为结果。 通过设计，每个 Map 函数调用都是相互独立的，从而允许框架使用分而治之的方式并行执行计算。 这还允许在出现故障或负载不平衡的情况下重复执行或重新执行 Map 任务，而不会影响计算结果。 通常，Hadoop 会为输入数据的每个 HDFS 数据块创建一个 Map 任务实例。 Map 任务实例内的 Map 函数调用数等于特定 Map 任务实例的输入数据块中的数据记录数。

Hadoop MapReduce 使用**键**对计算的所有 Map 任务的输出键值记录进行分组，并将它们分发给 Reduce 任务。 这种将数据分发和传输到 Reduce 任务的过程称为 MapReduce 计算的无序阶段。 每个 Reduce 任务的输入数据也将按键进行排序和分组。 将按键的排序顺序为每个键和该键的值组(*Reduce<key，list_of_value>*)调用 Reduce 函数。 在典型的 MapReduce 程序中，用户只需实现 Map 和 Reduce 函数，Hadoop 负责并行调度和执行它们。 Hadoop 将重新运行任何失败的任务，并提供缓解任何不平衡计算的措施。 为了更好地理解 MapReduce 数据流和计算流，请查看下图：

![Hadoop MapReduce](graphics/5471OS_01_04.jpg)

在 Hadoop1.x 中，MapReduce(MR1)组件由**JobTracker**进程和**TaskTracker**组成，前者在管理集群和协调作业的主节点上运行，后者在每个计算节点上运行，启动和协调在该节点中执行的任务。 Hadoop 2.x MapReduce(MR2)中没有这两个进程。 在 MR2 中，JobTracker 的工作协调职责由 ApplicationMaster 处理，该 ApplicationMaster 将通过纱线按需部署。 JobTracker 的集群管理和作业调度职责在 MR2 中由 YAR ResourceManager 处理。 JobHistoryServer 已经接管了提供有关已完成的 MR2 作业的信息的责任。 通过在计算节点中管理资源和启动容器(在 MapReduce 2 中启动容器，在 MapReduce 2 中包含 Map 或 Reduce 任务)，Yanson NodeManager 提供了与 MR1 TaskTracker 有些类似的功能。

## Hadoop 安装模式

Hadoopv2 提供了三个安装选项：

*   **本地模式**：本地模式允许我们仅使用解压的 Hadoop 发行版运行 MapReduce 计算。 这种非分布式模式在单个 Java 进程中执行 Hadoop MapReduce 的所有部分，并使用本地文件系统作为存储。 本地模式对于本地测试/调试 MapReduce 应用程序非常有用。
*   **伪分布式模式**：使用此模式，我们可以在模拟分布式集群的单机上运行 Hadoop。 此模式将 Hadoop 的不同服务作为不同的 Java 进程运行，但在一台机器内运行。 这种模式很适合让您玩和体验 Hadoop。
*   **分布式模式**：这是真正的分布式模式，支持从几个节点到数千个节点的集群。 对于生产集群，我们建议使用众多打包的 Hadoop 发行版之一，而不是使用 Hadoop 发行版二进制文件从头开始安装 Hadoop，除非您有需要普通 Hadoop 安装的特定用例。 有关 Hadoop 发行版的更多信息，请参阅*使用 Hadoop 发行版在分布式集群环境中建立 Hadoop 生态系统*食谱。

### 备注

本书的示例代码文件可以在 giHub 上的[https://github.com/thilg/hcb-v2](https://github.com/thilg/hcb-v2)获得。 代码库的`chapter1`文件夹包含本章的示例源代码文件。 您还可以使用[https://github.com/thilg/hcb-v2/archive/master.zip](https://github.com/thilg/hcb-v2/archive/master.zip)链接下载存储库中的所有文件。

本书的示例代码使用 Gradle 自动编译和构建项目。 您可以按照[http://www.gradle.org/docs/current/userguide/installation.html](http://www.gradle.org/docs/current/userguide/installation.html)提供的指南安装 Gradle。 通常，您只需从[http://www.gradle.org/downloads](http://www.gradle.org/downloads)下载并解压缩 Gradle 发行版，并将提取的 Gradle 发行版的 bin 目录添加到您的 PATH 变量中。

所有示例代码都可以通过在代码库的主文件夹中发出`gradle build`命令来构建。

Eclipse IDE 的项目文件可以通过在代码库的主文件夹中运行`gradle eclipse`命令来生成。

IntelliJ IDEA IDE 的项目文件可以通过在代码库的主文件夹中运行`gradle idea`命令来生成。

# 在本地计算机上设置 Hadoop v2

本菜谱介绍了如何使用本地模式在本地计算机上设置 Hadoop v2。 本地模式是一种非分布式模式，可用于测试和调试 Hadoop 应用程序。 在本地模式下运行 Hadoop 应用程序时，所有必需的 Hadoop 组件和应用程序都在单个**Java 虚拟机**(**JVM**)进程内执行。

## 做好准备

下载并安装 JDK 1.6 或更高版本，最好是 Oracle JDK 1.7。 Oracle JDK 可以从[http://www.oracle.com/technetwork/java/javase/downloads/index.html](http://www.oracle.com/technetwork/java/javase/downloads/index.html)下载。

## 怎么做……

现在，让我们开始 Hadoop v2 安装：

1.  从[http://hadoop.apache.org/releases.html](http://hadoop.apache.org/releases.html)下载最新的 Hadoopv2 分支发行版(Hadoop2.2.0 或更高版本)。
2.  使用以下命令解压缩 Hadoop 发行版。 您必须将文件名中的`x.x.`更改为您下载的实际版本。 从现在开始，我们将把解压后的 Hadoop 目录命名为`{HADOOP_HOME}`：

    ```scala
    $ tar -zxvf hadoop-2.x.x.tar.gz

    ```

3.  现在，您可以通过`{HADOOP_HOME}/bin/hadoop`命令运行 Hadoop 作业，我们将在下一个菜谱中进一步详细说明这一点。

## 它是如何工作的.

Hadoop 本地模式不启动任何服务器，但在单个 JVM 内完成所有工作。 当您以本地模式向 Hadoop 提交作业时，Hadoop 会启动 JVM 来执行该作业。 该作业的输出和行为与分布式 Hadoop 作业相同，不同之处在于该作业仅使用当前节点运行任务，而本地文件系统用于数据存储。 在下一个菜谱中，我们将了解如何使用 Hadoop 本地模式运行 MapReduce 程序。

# 编写字数统计 MapReduce 应用程序，将其捆绑在一起，并使用 Hadoop 本地模式运行它

这个配方解释了如何实现一个简单的 MapReduce 程序来计算数据集中单词的出现次数。 Wordcount 作为 Hadoop MapReduce 的 HelloWorld 等价物而闻名。

要运行 MapReduce 作业，用户应该提供`map`函数、`reduce`函数、输入数据和存储输出数据的位置。 执行时，Hadoop 执行以下步骤：

1.  Hadoop 使用提供的**InputFormat**将输入数据分解为键-值对，并为每个键-值对调用`map`函数，提供键-值对作为输入。 执行时，`map`函数可以输出零个或多个键值对。
2.  Hadoop 将映射器发出的键值对传输到还原器(此步骤称为无序)。 Hadoop 然后按键对这些键-值对进行排序，并将属于同一键的值分组在一起。
3.  对于每个不同的键，Hadoop 调用一次 Reduce 函数，同时传递该键的特定键和值列表作为输入。
4.  `reduce`函数可以输出零个或多个键值对，Hadoop 将它们写入输出数据位置作为最终结果。

## 做好准备

从本书的源代码存储库中选择第一章的源代码。 导出指向解压缩的 Hadoop 发行版的根的`$HADOOP_HOME`环境变量。

## 怎么做……

现在，让我们编写我们的第一个 Hadoop MapReduce 程序：

1.  WordCount 示例使用 MapReduce 计算一组输入文档中出现的单词数。 示例代码位于本章源文件夹的`chapter1/Wordcount.java`文件中。 代码由三部分组成-映射器、Reducer 和主程序。
2.  映射器从`org.apache.hadoop.mapreduce.Mapper`接口扩展。 Hadoop InputFormat 将输入文件中的每一行作为输入键-值对提供给`map`函数。 函数`map`使用空格字符(如分隔符)将每行分成子字符串，并为每个标记(Word)发出`(word,1)`作为输出。

    ```scala
    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
      // Split the input text value to words
      StringTokenizer itr = new StringTokenizer(value.toString());

      // Iterate all the words in the input text value
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, new IntWritable(1));
      }
    }
    ```

3.  每个`reduce`函数调用都接收一个键和该键的所有值作为和输入。 `reduce`函数输出键和键的出现次数作为输出。

    ```scala
    public void reduce(Text key, Iterable<IntWritable>values, Context context) throws IOException, InterruptedException
    {
      int sum = 0;
      // Sum all the occurrences of the word (key)
      for (IntWritableval : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
    ```

4.  `main`驱动程序配置 MapReduce 作业并将其提交给 Hadoop 纱线集群：

    ```scala
    Configuration conf = new Configuration();
    ……
    // Create a new job
    Job job = Job.getInstance(conf, "word count");
    // Use the WordCount.class file to point to the job jar
    job.setJarByClass(WordCount.class);

    job.setMapperClass(TokenizerMapper.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);

    // Setting the input and output locations
    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
    FileOutputFormat.setOutputPath(job, newPath(otherArgs[1]));
    // Submit the job and wait for it's completion
    System.exit(job.waitForCompletion(true) ? 0 : 1);
    ```

5.  通过从示例代码库的`chapter1`文件夹发出`gradle build`命令，使用本章介绍中提到的使用Gradle 版本编译示例。 或者，您也可以通过发出`ant compile`命令来使用提供的 Apache Ant 构建文件。
6.  使用以下命令运行 wordcount 示例。 在此命令中，`chapter1.WordCount`是`main`类的名称。 `wc-input`是输入数据目录，`wc-output`是输出路径。 源库的`wc-input`目录包含一个示例文本文件。 或者，您可以将任何文本文件复制到`wc-input`目录。

    ```scala
    $ $HADOOP_HOME/bin/hadoop jar \
    hcb-c1-samples.jar \
    chapter1.WordCount wc-input wc-output

    ```

7.  输出目录(`wc-output`)将有一个名为`part-r-XXXXX`的文件，该文件将包含文档中每个单词的计数。 祝贺你!。 您已经成功运行了您的第一个 MapReduce 程序。

    ```scala
    $ cat wc-output/part*

    ```

## 它是如何工作的.

在前面的示例中，MapReduce 在本地模式下工作，不启动任何服务器，并使用本地文件系统作为输入、输出和工作数据的存储系统。 下图显示了隐藏在 WordCount 程序中发生的情况：

![How it works...](graphics/5471OS_01_05.jpg)

字数统计 MapReduce 工作流的工作方式如下：

1.  Hadoop读取输入，使用新行个字符作为分隔符来中断它，然后运行`map`函数，将每行作为参数传递，行号作为键，行内容作为值。
2.  函数的作用是：标记该行，并为每个标记(Word)发出一个键-值对`(word,1)`。
3.  Hadoop 收集所有`(word,1)`对，按单词对它们进行排序，根据每个唯一键对发出的所有值进行分组，并为每个唯一键调用一次`reduce`函数，将该键的键和值作为参数传递。
4.  `reduce`函数使用这些值计算每个单词的出现次数，并将其作为键-值对发出。
5.  Hadoop 将最终输出写入输出目录。

## 还有更多...

作为可选步骤，您可以直接从您喜欢的**Java 集成开发环境**(**IDE**)设置和运行 WordCount 应用程序。 Eclipse IDE 和 IntelliJ IDEA IDE 的项目文件可以通过在代码库的主文件夹中分别运行`gradle eclipse`和`gradle idea`命令来生成。

对于其他 IDE，您必须将以下目录中的 JAR 文件添加到为示例代码创建的 IDE 项目的类路径中：

*   `{HADOOP_HOME}/share/hadoop/common`
*   `{HADOOP_HOME}/share/hadoop/common/lib`
*   `{HADOOP_HOME}/share/hadoop/mapreduce`
*   `{HADOOP_HOME}/share/hadoop/yarn`
*   `{HADOOP_HOME}/share/hadoop/hdfs`

通过传递`wc-input`和`wc-output`作为参数来执行`chapter1.WordCount`类。 这将像以前一样运行示例。 以这种方式从 IDE 运行 MapReduce 作业对于调试 MapReduce 作业非常有用。

## 另请参阅

尽管您在本地机器上安装了 Hadoop 来运行示例，但是您可以使用分布式 Hadoop 集群设置和 HDFS 分布式文件系统来运行它。 本章的*在分布式集群环境中运行 Wordcount 程序*配方将讨论如何在分布式设置中运行此示例。

# 向 Wordcount MapReduce 程序添加组合器步骤

一个单个映射任务可能会输出许多具有相同键的键值对，导致 Hadoop 通过网络**混洗**(移动)所有这些值到 Reduce 任务，这会产生很大的开销。 例如，在前面的 Wordcount MapReduce 程序中，当 Mapper 在单个 Map 任务中遇到同一单词的多次出现时，`map`函数将输出许多`<word,1>`个中间键-值对以通过网络传输。 但是，如果在通过网络将数据发送到减少器之前，我们可以将`<word,1>`对的所有实例求和为单个`<word, count>`对，则可以优化此场景。

为了优化这样的场景，Hadoop 支持一个称为**组合器**的特殊函数，该函数执行 Map 任务输出键-值对的本地聚合。 如果提供，Hadoop 会在将数据持久化到磁盘之前调用 Map 任务输出上的组合器函数，以调整 Reduce 任务。 这可以显著减少从 Map 任务转移到 Reduce 任务的数据量。 应该注意，组合器是 MapReduce 流的一个可选步骤。 即使您提供了组合器实现，Hadoop 也可能决定只为 Map 输出数据的子集调用它，或者根本不调用它。

本食谱解释了如何将组合器与前面食谱中介绍的 Wordcount MapReduce 应用程序结合使用。

## 怎么做……

现在，让我们向 Wordcount MapReduce 应用程序添加一个组合器：

1.  组合器必须与`reduce`函数具有相同的接口。 组合器发出的输出键-值对类型应该与 Reducer 输入键-值对的类型匹配。 对于 wordcount 示例，我们可以重用 wordcount`reduce`函数作为组合器，因为 wordcount `reduce`函数的输入和输出数据类型是相同的。
2.  取消注释`WordCount.java`文件中的以下行，以启用 WordCount 应用程序的组合器：

    ```scala
    job.setCombinerClass(IntSumReducer.class);
    ```

3.  通过重新运行 Gradle(`gradle build`)或 Ant 构建(`ant compile`)重新编译代码。
4.  使用以下命令运行 wordcount 示例。 在运行作业之前，请确保删除旧的输出目录(`wc-output`)。

    ```scala
    $ $HADOOP_HOME/bin/hadoop jar \
    hcb-c1-samples.jar \
    chapter1.WordCount wc-input wc-output

    ```

5.  最终结果将从`wc-output`目录中获得。

## 它是如何工作的.

如果提供，Hadoop 会在将数据持久存储到磁盘上以转移到 Reduce 任务之前，调用 Map 任务输出上的组合器函数。 组合器可以在将映射器生成的数据发送到 Reducer 之前对其进行预处理，从而减少需要传输的数据量。

在 WordCount 应用程序中，组合器接收*N*个`(word,1)`对作为输入，并输出单个`(word, N)`对。 例如，如果由 Map 任务处理的输入有 1,000 个单词“the”出现，映射器将生成 1,000 个`(the,1)`对，而组合器将生成一个`(the,1000)`对，从而减少需要传输到 Reduce 任务的数据量。 下图显示了 Wordcount MapReduce 应用程序中合并器的用法：

![How it works...](graphics/5471OS_01_06.jpg)

## 还有更多...

仅当`reduce`函数输入和输出键值数据类型相同时，才能将作业的`reduce`函数用作组合器。 在不能重用`reduce`函数作为组合器的情况下，可以编写专用的`reduce`函数实现来充当组合器。 组合器输入和输出键-值对类型应该与映射器输出键-值对类型相同。

我们重申，组合器是 MapReduce 流的一个可选步骤。 即使您提供了组合器实现，Hadoop 也可能决定只为 Map 输出数据的子集调用它，或者根本不调用它。 注意不要使用组合器执行计算的任何基本任务，因为 Hadoop 不保证组合器的执行。

在非分布式模式下使用组合器不会产生显著的增益。 但是，在*使用 Hadoop v2*配方在分布式群集环境中设置 Hadoop 纱线中所述的分布式设置中，组合器可以提供显著的性能提升。

# 设置 HDFS

HDFS是一个块结构的分布式文件系统，设计用于在商用硬件组成的集群上可靠地存储 PB 级的数据。 HDFS 支持存储海量数据，并提供对数据的高吞吐量访问。 HDFS 通过冗余跨多个节点存储文件数据，以确保容错和高聚合带宽。

HDFS 是 Hadoop MapReduce 计算使用的默认分布式文件系统。 Hadoop 支持对存储在 HDFS 中的数据进行数据位置感知处理。 HDFS 体系结构主要由处理文件系统元数据的集中式 NameNode 和存储实际数据块的 DataNode 组成。 HDFS 数据块通常粒度较粗，在大型流读取时性能更好。

要设置 HDFS，我们首先需要配置 NameNode 和 DataNodes，然后在`slaves`文件中指定 DataNode。 当我们启动 NameNode 时，启动脚本将启动 DataNode。

### 提示

建议使用本食谱中提到的 Hadoop 版本构件直接安装 HDFS，仅用于开发测试和高级使用情形。 对于常规生产集群，我们建议使用打包的 Hadoop 发行版，如*使用 Hadoop 发行版*配方在分布式集群环境中设置 Hadoop 生态系统中所述。 打包的 Hadoop 发行版使安装、配置、维护和更新 Hadoop 生态系统的组件变得更加容易。

## 做好准备

您可以使用一台机器或多台机器遵循此食谱。 如果您使用多台计算机，则应选择一台计算机作为运行 HDFS NameNode 的主节点。 如果您使用的是单台计算机，请同时将其用作名称节点和数据节点。

1.  在将用于设置 HDFS 集群的所有计算机上安装 JDK 1.6 或更高版本(首选 Oracle JDK 1.7)。 将`JAVA_HOME`环境变量设置为指向 Java 安装。
2.  按照*在本地计算机上设置 Hadoop v2 的方法*下载 Hadoop。

## 怎么做……

现在，让我们在分布式模式下设置 HDFS：

1.  Set up password-less SSH from the master node, which will be running the NameNode, to the DataNodes. Check that you can log in to localhost and to all other nodes using SSH without a passphrase by running one of the following commands:

    ```scala
    $ ssh localhost
    $ ssh <IPaddress>

    ```

    ### 提示

    **配置无密码 SSH**

    如果步骤 1 中的命令返回错误或要求输入密码，请通过执行以下命令创建 SSH 密钥(您可能需要事先手动启用 SSH，具体取决于您的操作系统)：

    ```scala
    $ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa

    ```

    将`~/.ssh/id_dsa.pub`文件移动到集群中的所有节点。 然后通过运行以下命令将 SSH 密钥添加到每个节点的`~/.ssh/authorized_keys`文件中(如果`authorized_keys`文件不存在，请运行以下命令。 否则，跳到`cat`命令)：

    ```scala
    $ touch ~/.ssh/authorized_keys && chmod 600 ~/.ssh/authorized_keys

    ```

    现在，设置权限后，将您的密钥添加到`~/.ssh/authorized_keys`文件：

    ```scala
    $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys

    ```

    然后，您应该能够成功执行以下命令，而无需提供密码：

    ```scala
    $ ssh localhost

    ```

2.  在每台服务器中，创建一个用于存储 HDFS 数据的目录。 让我们将该目录命名为`{HADOOP_DATA_DIR}`。 在数据目录内创建两个子目录`{HADOOP_DATA_DIR}/data`和`{HADOOP_DATA_DIR}/name`。 通过对每个目录运行以下命令，将目录权限更改为`755`：

    ```scala
    $ chmod –R 755 <HADOOP_DATA_DIR>

    ```

3.  在 NameNode 中，将所有从节点的 IP 地址添加到`{HADOOP_HOME}/etc/hadoop/slaves`文件中，每个从节点都在单独的一行上。 当我们启动 NameNode 时，它将使用这个`slaves`文件来启动 DataNode。
4.  将以下配置添加到`{HADOOP_HOME}/etc/hadoop/core-site.xml`。 在添加配置之前，请将`{NAMENODE}`字符串替换为主节点的 IP：

    ```scala
    <configuration>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://{NAMENODE}:9000/</value>
      </property>
    </configuration>
    ```

5.  将以下配置添加到`{HADOOP_HOME}/etc/hadoop`目录下的`{HADOOP_HOME}/etc/hadoop`/`hdfs-site.xml`文件中。 在添加配置之前，请将`{HADOOP_DATA_DIR}`替换为您在第一步中创建的目录。 将我们在步骤 4 和 5 中修改的`core-site.xml`和`hdfs-site.xml`文件复制到所有节点。

    ```scala
    <configuration>
      <property>
        <name>dfs.namenode.name.dir</name>
        <!-- Path to store namespace and transaction logs -->
        <value>{HADOOP_DATA_DIR}/name</value>
      </property>
      <property>
        <name>dfs.datanode.data.dir</name>
        <!-- Path to store data blocks in datanode -->
        <value>{HADOOP_DATA_DIR}/data</value>
      </property>
    </configuration>
    ```

6.  From the NameNode, run the following command to format a new filesystem:

    ```scala
    $ $HADOOP_HOME/bin/hdfs namenode –format

    ```

    成功完成上一个命令后，您将在输出中看到以下行：

    ```scala
    …
    13/04/09 08:44:51 INFO common.Storage: Storage directory /…/dfs/name has been successfully formatted.
    ….
    ```

7.  Start the HDFS using the following command:

    ```scala
    $ $HADOOP_HOME/sbin/start-dfs.sh

    ```

    此命令将首先在主节点中启动 NameNode。 然后，它将在`slaves`文件中提到的计算机中启动 DataNode 服务。 最后，它将启动辅助 NameNode。

8.  HDFS附带一个监控 Web 控制台，用于验证安装和监控 HDFS 群集。 它还允许用户浏览 HDFS 文件系统的内容。 可以从`http://{NAMENODE}:50070/`访问 HDFS 监控控制台。 访问监控控制台，确认是否可以看到 HDFS 启动页面。 在这里，将`{NAMENODE}`替换为运行 HDFS NameNode 的节点的 IP 地址。
9.  或者，您可以使用以下命令获取有关 HDFS 状态的报告：

    ```scala
    $ $HADOOP_HOME/bin/hadoop dfsadmin -report

    ```

10.  最后，使用以下命令关闭 HDFS 集群：

    ```scala
    $ $HADOOP_HOME/sbin/stop-dfs.sh

    ```

## 另请参阅

*   在*HDFS 命令行文件操作*配方中，我们将探索如何使用 HDFS 存储和管理文件。
*   HDFS 设置只是 Hadoop 安装的一部分。 *使用 Hadoop v2*在分布式集群环境中设置 Hadoop 纱线食谱介绍了如何设置 Hadoop 的其余部分。
*   *使用 Hadoop 发行版在分布式集群环境中设置 Hadoop 生态系统*食谱探索了如何使用打包的 Hadoop 发行版在集群中安装 Hadoop 生态系统。

# 使用 Hadoop v2 在分布式群集环境中设置 Hadoop 纱线

Hadoop v2 纱线部署包括在主节点上部署 ResourceManager 服务和在从节点上部署 NodeManager 服务。 Yar ResourceManager 是对集群的所有资源进行仲裁的服务，NodeManager 是管理单个节点中的资源的服务。

Hadoop MapReduce 应用程序可以在纱线上运行，使用一个纱线 ApplicationMaster 来协调每个作业和一组资源容器来运行 Map 和 Reduce 任务。

### 提示

建议使用本食谱中提到的 Hadoop 版本构件直接安装 Hadoop，仅用于开发测试和高级用例。 对于常规生产集群，我们建议使用打包的 Hadoop 发行版，如*使用 Hadoop 发行版*配方在分布式集群环境中设置 Hadoop 生态系统中所述。 打包的 Hadoop 发行版使安装、配置、维护和更新 Hadoop 生态系统的组件变得更加容易。

## 做好准备

您可以使用台机器作为伪分布式安装，也可以使用多台机器集群来遵循这个配方。 如果您使用多台计算机，则应该选择一台计算机作为运行 HDFS NameNode 和 Yar ResourceManager 的主节点。 如果您使用的是单台计算机，请同时将其用作主节点和从节点。

按照*设置 HDFS*配方设置 HDFS。

## 怎么做……

让我们通过设置纱线资源管理器和节点管理器来设置 Hadoop 纱线。

1.  在每台机器上，创建一个名为 local inside`{HADOOP_DATA_DIR}, which`的目录，该目录是您在*设置 HDFS*配方中创建的。 将目录权限更改为`755`。
2.  将以下内容添加到`{HADOOP_HOME}/etc/hadoop/mapred-site.xml`模板中，并将其另存为`{HADOOP_HOME}/etc/hadoop/mapred-site.xml`：

    ```scala
    <property>
      <name>fs.default.name</name>
      <value>hdfs://localhost:9000</value>
    </property>
    ```

3.  将以下内容添加到`{HADOOP_HOME}/etc/hadoop/yarn-site.xml`文件：

    ```scala
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
    <property>
      <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    ```

4.  使用以下命令启动 HDFS：

    ```scala
    $ $HADOOP_HOME/sbin/start-dfs.sh

    ```

5.  运行以下命令以启动纱线服务：

    ```scala
    $ $HADOOP_HOME/sbin/start-yarn.sh
    starting yarn daemons
    starting resourcemanager, logging to ………
    xxx.xx.xxx.xxx: starting nodemanager, logging to ………

    ```

6.  运行以下命令以启动 MapReduce JobHistoryServer。 这将启用用于 MapReduce 作业历史记录的 Web 控制台：

    ```scala
    $ $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver

    ```

7.  通过`jps`命令列出进程来验证安装。 主节点将列出 NameNode、ResourceManager 和 JobHistoryServer 服务。 从节点将列出 DataNode 和 NodeManager 服务：

    ```scala
    $ jps
    27084 NameNode
    2073 JobHistoryServer
    2106 Jps
    2588
    1536 ResourceManager

    ```

8.  请访问`http://{MASTER_NODE}:8088/`上提供的 ResourceManager 的基于 Web 的监控页面。

## 它是如何工作的.

如本章简介中所述，Hadoopv2 安装由 HDFS 节点、Yanth ResourceManager 和 Worker 节点组成。 当我们启动 NameNode 时，它通过`HADOOP_HOME/slaves`文件查找从节点，并在启动时使用 SSH 启动远程服务器中的 DataNode。 此外，当我们启动 ResourceManager 时，它会通过`HADOOP_HOME/slaves`文件查找从属对象并启动 NodeManagers。

## 另请参阅

*使用 Hadoop 发行版在分布式集群环境中设置 Hadoop 生态系统*食谱探索了如何使用打包的 Hadoop 发行版在集群中安装 Hadoop 生态系统。

# 使用 Hadoop 发行版在分布式群集环境中设置 Hadoop 生态系统

Hadoop纱线生态系统现在包含许多有用的组件，为存储在 HDFS 中的数据提供广泛的数据处理、存储和查询功能。 然而，使用单独的版本构件手动安装和配置所有这些组件以正确地协同工作是一项相当具有挑战性的任务。 这种方法的其他挑战包括监控和维护集群和多个 Hadoop 组件。

幸运的是，有几家商业软件供应商提供了集成良好的打包 Hadoop 发行版，使得在我们的集群中配置和维护 Hadoop 纱线生态系统变得更加容易。 这些发行版通常带有简单的基于 GUI 的安装程序，可以引导您完成整个安装过程，并允许您选择和安装 Hadoop 集群中所需的组件。 它们还提供工具来轻松监控群集和执行维护操作。 对于常规生产集群，我们建议使用知名供应商提供的打包 Hadoop 发行版，使您的 Hadoop 之旅更加轻松。 其中一些商业 Hadoop 发行版(或发行版)拥有许可，允许我们通过可选的付费支持协议免费使用它们。

**Hortonworks Data Platform**(**HDP**)就是这样一种免费提供的众所周知的 Hadoop 纱线分布。 HDP 的所有组件都是免费的开源软件。 您可以从[http://hortonworks.com/hdp/downloads/](http://hortonworks.com/hdp/downloads/)下载 hdp。 有关安装说明，请参阅下载页面中提供的安装指南。

**Cloudera CDH**是另一个著名的 Hadoop 纱线分布。 专用宿主机速成版免费提供。 Cloudera 发行版的一些组件是专有的，只对付费客户可用。 您可以从[http://www.cloudera.com/content/cloudera/en/products-and-services/cloudera-express.html](http://www.cloudera.com/content/cloudera/en/products-and-services/cloudera-express.html)下载 Cloudera Express。 有关安装说明，请参阅下载页面上提供的安装指南。

Hortonworks HDP、Cloudera CDH 和其他一些供应商提供完全配置的快速入门虚拟机映像，您可以使用虚拟化软件产品下载这些映像并在本地计算机上运行。 在决定集群的 Hadoop 发行版之前，这些虚拟机是学习和尝试不同 Hadoop 组件以及进行评估的极佳资源。

Apachebigtop 是一个开源项目，旨在为各种 Hadoop 生态系统组件提供打包和集成/互操作性测试。 Bigtop 还提供了厂商中立的打包 Hadoop 发行版。 虽然它不像商业发行版那么复杂，但 Bigtop 比使用每个 Hadoop 组件的二进制发行版更容易安装和维护。 在本食谱中，我们提供了使用 Apache bigtop 在本地计算机上安装 Hadoop 生态系统的步骤。

前面提到的任何发行版本(包括 bigtop)都适用于遵循本书中提供的食谱和执行示例的目的。 但是，如果可能，我们建议使用 Hortonworks HDP、Cloudera CDH 或其他商业 Hadoop 发行版。

## 做好准备

本食谱提供了 CENT OS 和 Red Hat 操作系统的说明。 停止您在前面的食谱中启动的所有 Hadoop 服务。

## 怎么做……

以下步骤将指导您完成使用 Apache bigtop for Cent OS 和 Red Hat 操作系统的 Hadoop 集群的安装过程。 请将这些命令相应地修改为适用于其他基于 Linux 的操作系统。

1.  安装 Bigtop 存储库：

    ```scala
    $ sudo wget -O \
    /etc/yum.repos.d/bigtop.repo \ 
    http://www.apache.org/dist/bigtop/stable/repos/centos6/bigtop.repo

    ```

2.  搜索 Hadoop：

    ```scala
    $ yum search hadoop

    ```

3.  使用 YUM 安装 Hadoop v2。 这将安装 Hadoopv2 组件(MapReduce、HDFS 和 YAR)以及 ZooKeeper 依赖项。

    ```scala
    $ sudo yum install hadoop\*

    ```

4.  使用您喜欢的编辑器将以下行添加到`/etc/default/bigtop-utils`文件。 建议将`JAVA_HOME`指向 JDK 1.6 或更高版本的安装(首选 Oracle JDK 1.7 或更高版本)。

    ```scala
    export JAVA_HOME=/usr/java/default/
    ```

5.  初始化并格式化 NameNode：

    ```scala
    $ sudo  /etc/init.d/hadoop-hdfs-namenode init

    ```

6.  启动 Hadoop NameNode 服务：

    ```scala
    $ sudo service hadoop-hdfs-namenode start

    ```

7.  启动 Hadoop DataNode 服务：

    ```scala
    $ sudo service hadoop-hdfs-datanode start

    ```

8.  运行以下脚本以在 HDFS 中创建必要的目录：

    ```scala
    $ sudo  /usr/lib/hadoop/libexec/init-hdfs.sh

    ```

9.  在 HDFS 中创建您的主目录并应用必要的权限：

    ```scala
    $ sudo su -s /bin/bash hdfs \
    -c "/usr/bin/hdfs dfs -mkdir /user/${USER}"
    $ sudo su -s /bin/bash hdfs \
    -c "/usr/bin/hdfs dfs -chmod -R 755 /user/${USER}"
    $ sudo su -s /bin/bash hdfs \
    -c "/usr/bin/hdfs dfs -chown ${USER} /user/${USER}"

    ```

10.  启动纱线资源管理器和节点管理器：

    ```scala
    $ sudo service hadoop-yarn-resourcemanager start
    $ sudo service hadoop-yarn-nodemanager start
    $ sudo service hadoop-mapreduce-historyserver start

    ```

11.  尝试使用以下命令验证安装：

    ```scala
    $ hadoop fs -ls  /
    $ hadoop jar \
    /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar \
    pi 10 1000

    ```

12.  您还可以使用`http://<namenode_ip>:50070`上提供的监控控制台监控 HDFS 的状态。
13.  使用 Bigtop 安装配置单元、HBase、Mahout 和 Pig，如下所示：

    ```scala
    $ sudo yum install hive\*, hbase\*, mahout\*, pig\*

    ```

## 还有更多...

*   您可以按照[https://cwiki.apache.org/confluence/display/BIGTOP/How+to+install+BigTop+0.7.0+hadoop+on+CentOS+with+puppet](https://cwiki.apache.org/confluence/display/BIGTOP/How+to+install+BigTop+0.7.0+hadoop+on+CentOS+with+puppet)中给出的步骤使用基于傀儡的 Bigtop 集群安装
*   您还可以在云环境中设置 Hadoop v2 群集，我们将在下一章中讨论

# HDFS 命令行文件操作

HDFS 是分布式文件系统，就像任何其他文件系统一样，它允许用户使用 shell 命令操作文件系统。 本食谱介绍了其中一些命令，并展示了如何使用 HDFS shell 命令。

值得注意的是，一些 HDFS 命令与最常用的 Unix 命令具有一一对应关系。 例如，考虑以下命令：

```scala
$ bin/hdfs dfs –cat /user/joe/foo.txt

```

该命令读取`/user/joe/foo.txt`文件并将其打印到屏幕上，就像 Unix 系统中的`cat`命令一样。

## 做好准备

按照*设置 HDFS*食谱或*使用 Hadoop 分发版*食谱在分布式集群环境中设置 Hadoop 生态系统来启动 HDFS 服务器。

## 怎么做……

1.  运行以下命令以列出 HDFS 主目录的内容。 如果您的 HDFS 主目录不存在，请按照*中的步骤 9 使用 Hadoop 分发版*在分布式群集环境中设置 Hadoop 生态系统来创建 HDFS 主目录。

    ```scala
    $ hdfs dfs -ls

    ```

2.  运行以下命令，在 HDFS 的主目录中创建名为`test`的新目录：

    ```scala
    $ hdfs dfs -mkdir test

    ```

3.  HDFS 文件系统以`/`作为根目录。 运行以下命令以列出 HDFS 中新创建的目录的内容：

    ```scala
    $ hdfs dfs -ls test

    ```

4.  运行以下命令将本地自述文件复制到`test`：

    ```scala
    $ hdfs dfs -copyFromLocal README.txt test

    ```

5.  运行以下命令以列出`test`目录：

    ```scala
    $ hdfs dfs -ls test
    Found 1 items
    -rw-r--r--   1 joesupergroup1366 2013-12-05 07:06 /user/joe/test/README.txt

    ```

6.  运行以下命令将`/test/README.txt`文件复制回本地目录：

    ```scala
    $ hdfs dfs –copyToLocal \
    test/README.txt README-NEW.txt

    ```

## 它是如何工作的.

当命令发出时，HDFS 客户端将代表我们与 HDFS NameNode 对话并执行操作。 客户端将从`HADOOP_HOME/etc/hadoop/conf`目录中的配置中获取 NameNode。

但是，如果需要，我们可以使用完全限定路径强制客户端与特定的 NameNode 对话。 例如，`hdfs://bar.foo.com:9000/data`将要求客户端在端口`9000`处与在`bar.foo.com`上运行的 NameNode 对话。

## 还有更多...

HDFS 支持大多数 Unix 命令，如`cp`、`mv`和`chown`，它们遵循与前面讨论的命令相同的模式。 以下命令会列出所有可用的 HDFS shell 命令：

```scala
$ hdfs dfs -help

```

将特定命令与`help`一起使用将显示该命令的用法。

```scala
$ hdfs dfs –help du

```

# 在分布式集群环境中运行 wordcount 程序

本配方描述了如何在分布式 Hadoopv2 集群中运行 MapReduce 计算。

## 做好准备

按照*设置 HDFS*食谱或*使用 Hadoop 分发版*食谱在分布式集群环境中设置 Hadoop 生态系统来启动 Hadoop 群集。

## 怎么做……

现在，让我们在分布式 Hadoop v2 设置中运行wordcount 示例：

1.  将源存储库中的`wc-input`目录上载到 HDFS 文件系统。 或者，您也可以上传任何其他文本文档集。

    ```scala
    $ hdfs dfs -copyFromLocal wc-input .

    ```

2.  执行`HADOOP_HOME`目录中的 WordCount 示例：

    ```scala
    $ hadoop jar hcb-c1-samples.jar \
    chapter1.WordCount \
    wc-input wc-output

    ```

3.  运行以下命令以列出输出目录，然后查看结果：

    ```scala
    $hdfs dfs -ls wc-output
    Found 3 items
    -rw-r--r--   1 joesupergroup0 2013-11-09 09:04 /data/output1/_SUCCESS
    drwxr-xr-x   - joesupergroup0 2013-11-09 09:04 /data/output1/_logs
    -rw-r--r--   1 joesupergroup1306 2013-11-09 09:04 /data/output1/part-r-00000

    $ hdfs dfs -cat wc-output/part*

    ```

## 它是如何工作的.

当我们提交作业时，Yar 会安排一个 MapReduce ApplicationMaster 来协调和执行计算。 ApplicationMaster 从 ResourceManager 请求必要的资源，并使用从资源请求接收的容器执行 MapReduce 计算。

## 还有更多...

您还可以通过 HDFS 监控 UI 访问`http://NAMANODE:50070`来查看 WordCount 应用程序的结果。

# 使用 DFSIO 对 HDFS 进行基准测试

Hadoop包含几个基准测试，您可以使用它们来验证 HDFS 集群是否设置正确并按预期执行。 DFSIO 是 Hadoop 附带的基准测试，可用于分析 HDFS 集群的 I/O 性能。 本食谱展示了如何使用 DFSIO 对 HDFS 群集的读/写性能进行基准测试。

## 做好准备

在运行这些基准测试之前，您必须设置和部署 HDFS 和 Hadoop v2 纱线 MapReduce。 在 Hadoop 安装中找到`hadoop-mapreduce-client-jobclient-*-tests.jar`文件。

## 怎么做……

以下步骤将向您展示如何运行写入和读取 DFSIO 性能基准：

1.  执行以下命令以运行 HDFS 写入性能基准。 `–nrFiles`参数指定基准要写入的文件数。 使用一个足够大的数字来饱和群集中的任务槽。 `-fileSize`参数指定每个文件的文件大小(以 MB 为单位)。 根据您的 Hadoop 安装，在以下命令中更改`hadoop-mapreduce-client-jobclient-*-tests.jar`文件的位置。

    ```scala
    $ hadoop jar \
    $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar \
    TestDFSIO -write -nrFiles 32 –fileSize 1000

    ```

2.  写入基准测试将结果写入控制台，并将其附加到名为`TestDFSIO_results.log`的文件中。 您可以使用`–resFile`参数提供您自己的结果文件名。
3.  以下步骤将向您展示如何运行 HDFS 读取性能基准测试。 读性能基准使用第一步中写基准写入的文件，因此在运行读基准之前应该先执行写基准，并且写基准写入的文件必须在 HDFS 中存在，读基准才能正常工作。 基准测试将结果写入控制台，并将结果附加到日志文件，这与写入基准测试类似。

    ```scala
    $hadoop jar \
    $HADOOP_HOME/share/Hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar \
    TestDFSIO -read \
    -nrFiles 32 –fileSize 1000

    ```

4.  可以使用以下命令清理由上述基准测试生成的文件：

    ```scala
    $hadoop jar \
    $HADOOP_HOME/share/Hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar \
    TestDFSIO -clean

    ```

## 它是如何工作的.

DFSIO 执行 MapReduce 作业，其中 Map 任务并行写入和读取文件，而 Reduce 任务用于收集和汇总性能数字。 您可以将此基准测试的吞吐量和 IO 速率结果与磁盘总数及其原始速度进行比较，以验证您是否从群集获得了预期的性能。 验证写入性能结果时，请注意复制因素。 由于某些原因，这些测试中的高标准偏差可能暗示有一个或多个表现不佳的节点。

## 还有更多...

将这些测试与监控系统一起运行可以帮助您轻松识别 Hadoop 集群的瓶颈。

# 使用 TeraSort 对 Hadoop MapReduce 进行基准测试

HadoopTeraSort 是一个众所周知的基准测试，旨在使用 Hadoop MapReduce 尽可能快地对 1TB 数据进行排序。 TeraSort 基准测试强调了 Hadoop MapReduce 框架的几乎每个部分以及 HDFS 文件系统，这使得它成为微调 Hadoop 集群配置的理想选择。

最初的 TeraSort 基准对 1000 万条 100 字节的记录进行排序，总数据大小为 1TB。 但是，我们可以指定记录的数量，从而可以配置数据的总大小。

## 做好准备

在运行这些基准测试之前，您必须设置和部署 HDFS 和 Hadoop v2 纱线 MapReduce，并在 Hadoop 安装中找到`hadoop-mapreduce-examples-*.jar`文件。

## 怎么做……

以下步骤将向您展示如何在 Hadoop 群集上运行 TeraSort 基准测试：

1.  The first step of the TeraSort benchmark is the data generation. You can use the `teragen` command to generate the input data for the TeraSort benchmark. The first parameter of `teragen` is the number of records and the second parameter is the HDFS directory to generate the data. The following command generates 1 GB of data consisting of 10 million records to the `tera-in` directory in HDFS. Change the location of the `hadoop-mapreduce-examples-*.jar` file in the following commands according to your Hadoop installation:

    ```scala
    $ hadoop jar \
    $HADOOP_HOME/share/Hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
    teragen 10000000 tera-in

    ```

    ### 提示

    为`teragen`计算指定 Map 任务的数量是一个好主意，以加快数据生成速度。 这可以通过指定`–Dmapred.map.tasks`参数来实现。

    此外，您还可以增加生成数据的 HDFS 块大小，以便 TeraSort 计算的 Map 任务的粒度更粗(Hadoop 计算的 Map 任务的数量通常等于输入数据块的数量)。 这可以通过指定`–Ddfs.block.size`参数来实现。

    ```scala
    $ hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
    teragen –Ddfs.block.size=536870912 \
    –Dmapred.map.tasks=256 10000000 tera-in

    ```

2.  The second step of the TeraSort benchmark is the execution of the TeraSort MapReduce computation on the data generated in step 1 using the following command. The first parameter of the `terasort` command is the input of HDFS data directory, and the second part of the `terasort` command is the output of the HDFS data directory.

    ```scala
    $ hadoop jar \
    $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
    terasort tera-in tera-out

    ```

    ### 提示

    最好为 TeraSort 计算指定 Reduce 任务的数量，以加快计算的 Reducer 部分。 这可以通过指定`–Dmapred.reduce.tasks`参数来实现，如下所示：

    ```scala
    $ hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar terasort –Dmapred.reduce.tasks=32 tera-in tera-out

    ```

3.  TeraSort 基准测试的最后一步是验证结果。 这可以通过使用`teravalidate`应用程序来完成，如下所示。 第一个参数是包含排序数据的目录，第二个参数是存储包含结果的报告的目录。

    ```scala
    $ hadoop jar \
    $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
    teravalidate tera-out tera-validate

    ```

## 它是如何工作的.

TeraSort 使用 MapReduce 框架的排序功能和自定义范围**分割器**在 Reduce 任务之间划分 Map 输出，以确保全局排序顺序。*