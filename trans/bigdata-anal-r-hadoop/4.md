# 四、使用 Hadoop 流

在前一章中，我们学习了如何在 RHIPE 和 RHadoop 的帮助下集成 R 和 Hadoop，以及示例。在本章中，我们将讨论以下主题:

*   了解 Hadoop 流的基础知识
*   了解如何使用 R 运行 Hadoop 流
*   探索 HadoopStreaming R 包

# 了解 Hadoop 流的基础知识

Hadoop 流是一个 Hadoop 实用程序，用于运行 Hadoop MapReduce 作业以及可执行脚本，如映射器和减速器。这类似于 Linux 中的管道操作。这样，文本输入文件被打印在流(`stdin`)上，作为映射器的输入，映射器的输出(`stdout`)作为减速器的输入；最后，Reducer 将输出写入 HDFS 目录。

Hadoop 流实用程序的主要优点是，它允许在 Hadoop 集群上执行 Java 和非 Java 编程的 MapReduce 作业。此外，它还负责运行 MapReduce 作业的进度。Hadoop 流支持 Perl、Python、PHP、R 和 C++编程语言。要运行用其他编程语言编写的应用程序，开发人员只需将应用程序逻辑翻译成带有键和值输出元素的 Mapper 和 Reducer 部分。我们在[第 2 章](2.html "Chapter 2. Writing Hadoop MapReduce Programs")、*编写 Hadoop MapReduce 程序*中了解到，要创建 Hadoop MapReduce 作业，我们需要映射器、减速器和驱动程序作为三个主要组件。在这里，当我们用 R 和 Hadoop 实现 MapReduce 时，创建用于运行 MapReduce 作业的驱动程序文件是可选的。

这一章是为了集成 R 和 Hadoop 而写的。所以我们将看到带有 Hadoop 流的 R 的例子。现在，我们将看到如何使用 Hadoop 流和用 Mapper 和 Reducer 编写的 R 脚本。从下图中，我们可以识别 Hadoop 流 MapReduce 作业的各个组件。

![Understanding the basics of Hadoop streaming](graphics/3282OS_04_01.jpg)

Hadoop 流组件

现在，假设我们已经将映射器和缩减器实现为`code_mapper.R`和`code_reducer.R`。我们将看看如何在 R 和 Hadoop 的集成环境中运行它们。这可以通过带有各种通用和流选项的 Hadoop 流命令来运行。

让我们看看 Hadoop 流命令的格式:

```r
 bin/hadoop command [generic Options] [streaming Options]

```

下图显示了 Hadoop 流的执行示例，这是一个具有多个流选项的 MapReduce 作业。

![Understanding the basics of Hadoop streaming](graphics/3282OS_04_02.jpg)

Hadoop 流命令选项

在上图中，整个 Hadoop 流 MapReduce 作业需要大约六个独特的重要组件。除了 jar，所有都是流选项。

以下是前面的 Hadoop 流命令的逐行描述:

*   **第 1 行**:用于指定 Hadoop jar 文件(设置 Hadoop jar 的类路径)
*   **第 2 行**:用于指定 HDFS 的输入目录
*   **第 3 行**:用于指定 HDFS 的输出目录
*   **第 4 行**:用于使文件对本地机器可用
*   **第 5 行**:用于将可用的 R 文件定义为映射器
*   **第 6 行**:用于使文件对本地机器可用
*   **第 7 行**:用于将可用的 R 文件定义为减速器

前面命令的六个主要 Hadoop 流组件如下所示:

*   **jar:** 这个选项用于运行一个带有编码类的 jar，这些编码类是为 Java 以及其他编程的 Mappers 和 Reducers 提供流功能而设计的。它被称为 Hadoop 流罐。
*   **输入** **:** 该选项用于指定 Hadoop 流 MapReduce 作业的输入数据集(存储在 HDFS)的位置。
*   **输出:**这个选项用来告诉 HDFS 输出目录(MapReduce 作业的输出将被写入的地方)到 Hadoop 流式 MapReduce 作业。
*   **文件:**该选项用于将映射器、缩减器和组合器等 MapReduce 资源复制到计算机节点(任务跟踪器)，使其成为本地的。
*   **映射器:**该选项用于识别可执行文件`Mapper`。
*   **减速器:**该选项用于识别可执行的`Reducer`文件。

还有其他 Hadoop 流命令选项，但它们是可选的。让我们来看看它们:

*   `inputformat`:通过指定 Java 类名来定义输入数据格式。默认为`TextInputFormat`。
*   `outputformat`:通过指定 Java 类名来定义输出数据格式。默认为`TextOutputFormat`。
*   `partitioner`:此用于包含用代码编写的类或文件，用于将输出划分为映射器阶段的(键、值)对。
*   `combiner`:这个是用来包含用聚合键值来减少 Mapper 输出的代码编写的类或者文件。此外，我们可以使用默认组合器，在将映射器的输出提供给缩减器之前，它将简单地组合所有关键属性值。
*   `cmdenv`:该选项将环境变量传递给流命令。比如我们可以通过`R_LIBS = /your /path /to /R /libraries`。
*   `inputreader`:这个可以代替`inputformat`类来指定记录阅读器类。
*   `verbose`:这个是用来啰嗦输出的。
*   `numReduceTasks`:此用于指定减速器的数量。
*   `mapdebug`:这个用来在 Mapper 任务失败时调试`Mapper`文件的脚本。
*   `reducedebug`:此用于在减速器任务失败时调试`Reducer`文件的脚本。

现在，是时候看看 Hadoop 流 MapReduce 作业的一些通用选项了。

*   `conf`:用于指定应用配置文件。

    ```r
    -conf configuration_file

    ```

*   `D`:用于定义特定 MapReduce 或 HDFS 属性的值。例如:
*   `-D property = value or to specify the temporary HDFS directory`.

    ```r
    -D dfs.temp.dir=/app/tmp/Hadoop/

    ```

    或者指定零减速器的总数:

    ```r
    -D mapred.reduce.tasks=0

    ```

    ### 注

    `-D`选项仅在工具实现时有效。

*   `fs`:用于定义 Hadoop 名称节点。

    ```r
    -fs localhost:port

    ```

*   `jt`:用于定义 Hadoop 作业跟踪器。

    ```r
    -jt localhost:port

    ```

*   `files`:用于指定来自 HDFS 的大型或多个文本文件。

    ```r
    -files hdfs://host:port/directory/txtfile.txt

    ```

*   `libjars`:这是用来指定要包含在类路径中的多个 jar 文件。

    ```r
    -libjars  /opt/ current/lib/a.jar, /opt/ current/lib/b.jar

    ```

*   `archives`:用于指定要在本地机器上取消归档的 jar 文件。

    ```r
    -archives hdfs://host:fs_port/user/testfile.jar

    ```

# 了解如何用 R 运行 Hadoop 流

现在，我们理解了什么是 Hadoop 流，以及如何使用 Hadoop 通用和流选项调用它。接下来，是时候知道如何用 R 开发和运行 R 脚本了。为此，我们可以考虑一个比简单的字数统计程序更好的例子。

MapReduce 操作的四个不同阶段解释如下:

*   理解 MapReduce 应用程序
*   了解如何编写 MapReduce 应用程序
*   了解如何运行 MapReduce 应用程序
*   了解如何探索 MapReduce 应用程序的输出

## 了解地图缩减应用程序

问题定义:问题是通过地理位置分割一个页面访问。在这个问题中，我们要考虑的是[http://www.gtuadmissionhelpline.com/](http://www.gtuadmissionhelpline.com/)网站，该网站是为那些正在寻找古吉拉特理工大学入学机会的学生提供指导而开发的。该网站包含各种领域的学院详细信息，如工程(文凭、学位和硕士)、医学、酒店管理、建筑、药学、工商管理硕士和 MCA。有了这个 MapReduce 应用程序，我们将从地理上确定访问者感兴趣的领域。

例如，来自瓦尔萨德市的大多数在线访问者更频繁地访问 MBA 院校的页面。基于此，我们可以识别 Valsad 学生的心态；他们对获得工商管理硕士领域的录取非常感兴趣。因此，有了这个网站流量数据集，我们可以确定城市的兴趣水平。现在，如果瓦尔萨德没有 MBA 学院，对他们来说将是一个大问题。他们需要搬迁到其他城市；这可能会增加他们的教育成本。

通过使用这种类型的数据，古吉拉特理工大学可以为来自不同城市的学生提供信息深刻的见解。

输入数据集来源:要执行这种类型的分析，我们需要该网站的网络流量数据。谷歌分析是一种流行的免费服务，用于跟踪网站上在线访问者的元数据。谷歌分析根据不同维度的广告指标存储网络流量数据。我们需要设计一个特定的查询来从谷歌分析中提取数据集。

输入数据集:提取的谷歌分析数据集包含以下四个数据列:

*   `date`:这是访问日期，格式为 YYYY/MM/DD。
*   `country`:这是来访者的国家。
*   `city`:这是游客的城市。
*   `pagePath`:这是网站某个页面的 URL。

输入数据集的头部如下:

```r
$ head -5 gadata_mr.csv
20120301,India,Ahmedabad,/
20120302,India,Ahmedabad,/gtuadmissionhelpline-team
20120302,India,Mumbai,/
20120302,India,Mumbai,/merit-calculator
20120303,India,Chennai,/

```

下图显示了预期的输出格式:

![Understanding a MapReduce application](graphics/3282OS_04_03.jpg)

以下是的一个样本输出:

![Understanding a MapReduce application](graphics/3282OS_04_04.jpg)

## 了解如何编写 MapReduce 应用程序

在部分，我们将了解以下两个 MapReduce 应用程序单元:

*   映射代码
*   减速器代码

让我们从映射器代码开始。

映射器代码:这个名为`ga-mapper.R`的 R 脚本将处理一个 MapReduce 作业的映射阶段。

映射器的工作是处理每一行，提取一对(键、值)，并将其传递给 Reducer 进行分组/聚合。在本例中，每一行都是 Mapper 的输入，输出为`City:PagePath`。`City`是关键，`PagePath`是价值。现在 Reducer 可以获取给定城市的所有页面路径；因此，它可以很容易地分组。

```r
# To identify the type of the script, here it is RScript
#! /usr/bin/env Rscript
# To disable the warning massages to be printed
options(warn=-1)
# To initiating the connection to standard input
input <- file("stdin", "r")
Each line has these four fields (date, country, city, and pagePath) in the same order. We split the line by a comma. The result is a vector which has the date, country, city, and pathPath in the indexes 1,2,3, and 4 respectively.

```

我们分别为城市和页面路径提取第三和第四个元素。然后，它们将作为键值对写入流，并被馈送到 Reducer 进行进一步处理。

```r
# Running while loop until all the lines are read
while(length(currentLine <- readLines(input, n=1, warn=FALSE)) > 0) {

# Splitting the line into vectors by "," separator 
 fields <- unlist(strsplit(currentLine, ","))

# Capturing the city and pagePath from fields
 city <- as.character(fields[3])
 pagepath <- as.character(fields[4])

# Printing both to the standard output
print(paste(city, pagepath,sep="\t"),stdout())

}

# Closing the connection to that input stream
close(input)

```

一旦映射器阶段 As(键，值)对的输出可用于标准输出，Reducers 将从`stdout`读取面向行的输出，并将其转换为最终的聚合键-值对。

来看看Mapper 输出格式是怎样的，Reducer 的输入数据格式是怎样的。

减速器代码:这个名为`ga_reducer.R`的 R 脚本将处理 MapReduce 作业的减速器部分。

正如我们所讨论的，Mapper 的输出将被视为 Reducer 的输入。Reducer 将读取这些城市和页面路径对，并将所有值与其各自的关键元素相结合。

```r
# To identify the type of the script, here it is RScript
#! /usr/bin/env Rscript

# Defining the variables with their initial values
city.key <- NA
page.value <- 0.0

# To initiating the connection to standard input
input <- file("stdin", open="r")

# Running while loop until all the lines are read
while (length(currentLine <- readLines(input, n=1)) > 0) {

# Splitting the Mapper output line into vectors by 
# tab("\t") separator
 fields <- strsplit(currentLine, "\t")

# capturing key and value form the fields
# collecting the first data element from line which is city
 key <- fields[[1]][1]
# collecting the pagepath value from line 
 value <- as.character(fields[[1]][2])

```

映射器输出写入两个主字段，以`\t`为分隔符，逐行写入数据；因此，我们通过使用`\t`从流输入中获取两个主要属性(键和值)来分割数据。

采集到密钥和值后，减速器会将其与之前采集的值进行比较。如果之前没有设置，那就设置它；否则，使用 R 中的`combine`功能将其与之前的字符值组合，最后将其打印到 HDFS 输出位置。

```r
# setting up key and values

# if block will check whether key attribute is 
# initialized or not. If not initialized then it will be # assigned from collected key attribute with value from # mapper output. This is designed to run at initial time.
 if (is.na(city.key)) {
 city.key <- key
 page.value <- value
 }
 else {

# Once key attributes are set, then will match with the previous key attribute value. If both of them matched then they will combined in to one.
 if (city.key == key) {
 page.value <- c(page.value, value)

 }
 else {

# if key attributes are set already but attribute value # is other than previous one then it will emit the store #p agepath values along with associated key attribute value of city,

 page.value <- unique(page.value)
# printing key and value to standard output
print(list(city.key, page.value),stdout())
 city.key <- key
 page.value <- value
 }
 }
}

print(list(city.key, page.value), stdout())

# closing the connection
close(input)

```

## 了解如何运行 MapReduce 应用程序

在用 R 语言开发了 Mapper 和 Reducer 脚本之后，是时候在 Hadoop 环境中运行它们了。在我们执行这个脚本之前，建议用简单的管道操作在样本数据集上测试它们。

```r
$ cat gadata_sample.csv | ga_mapper.R |sort | ga_reducer.R

```

前面的命令将在本地计算机上运行开发的映射器和缩减器脚本。但是它将类似于 Hadoop 流作业运行。我们需要针对运行时可能出现的任何问题或识别编程或逻辑错误来测试这一点。

现在，我们已经测试了映射器和减速器，并准备好使用 Hadoop 流命令运行。这个 Hadoop 流操作可以通过调用通用的`jar`命令和流命令选项来执行，正如我们在本章的*了解 Hadoop 流的基础知识*一节中所学习的。我们可以通过以下方式执行 Hadoop 流作业:

*   从命令提示符
*   r 或 RStudio 控制台

对于这两种方式，带有通用和流命令选项的执行命令是相同的。

### 从命令提示符执行 Hadoop 流作业

正如我们已经在部分*了解了 Hadoop 流*的基础知识，使用 R 开发的 Hadoop 流 MapReduce 作业的执行可以使用以下命令运行:

```r
$ bin/hadoop jar {HADOOP_HOME}/contrib/streaming/hadoop-streaming-1.0.3.jar 
 -input /ga/gadaat_mr.csv 
 -output /ga/output1 
 -file /usr/local/hadoop/ga/ga_mapper.R  
 -mapper ga_mapper.R 
 -file /usr/local/hadoop/ga/ga_ reducer.R 
 -reducer ga_reducer.R

```

### 从 R 或 RStudio 控制台执行 Hadoop 流作业

作为一个 R 用户，它将更适合从 R 控制台运行Hadoop 流作业。这可以通过`system`命令完成:

```r
system(paste("bin/hadoop jar”, “{HADOOP_HOME}/contrib/streaming/hadoop-streaming-1.0.3.jar",
 "-input /ga/gadata_mr.csv", 
 "-output /ga/output2", 
 "-file /usr/local/hadoop/ga/ga_mapper.R",
"-mapper ga_mapper.R", 
 "-file /usr/local/hadoop/ga/ga_reducer.R", 
 "-reducer ga_reducer.R")) 

```

前面的命令类似于您已经在命令提示符中使用的命令，使用通用选项和流选项执行 Hadoop 流作业。

## 了解如何探索 MapReduce 应用程序的输出

在成功完成执行之后，是时候探索输出以检查生成的输出是否重要了。输出将与两个目录`_logs`和`_SUCCESS`一起生成。`_logs`将用于跟踪所有操作以及错误；`_SUCCESS`将仅在 MapReduce 作业成功完成时生成。

同样，命令可以通过以下两种方式触发:

*   从命令提示符
*   从控制台

### 探索命令提示符的输出

要在输出目录中列出生成的文件，将调用以下命令:

```r
$ bin/hadoop dfs -cat /ga/output/part-* > temp.txt
$ head -n 40 temp.txt

```

检查输出的快照如下:

![Exploring an output from the command prompt](graphics/3282OS_04_05.jpg)

### 探索来自 R 或 RStudio 控制台的输出

相同的命令可以与 R(带 RSTU 迪奥)控制台中的`system`方法一起使用。

```r
dir <- system("bin/hadoop dfs -ls /ga/output",intern=TRUE)
out <- system("bin/hadoop dfs -cat /ga/output2/part-00000",intern=TRUE)

```

上述功能的屏幕截图如下所示:

![Exploring an output from R or an RStudio console](graphics/3282OS_04_06.jpg)

## 了解 Hadoop MapReduce 脚本中使用的基本 R 函数

现在，我们将看到 Hadoop Mapper 和 Reducer 中用于数据处理的一些基本实用函数:

*   `file`:此功能用于创建与文件的连接，以进行读取或写入操作。也用于从/向`stdin`或`stdout`读写。该功能将在映射器和减速器阶段开始时使用。

    ```r
    Con <- file("stdin", "r")

    ```

*   `write`:此功能用于将数据写入文件或标准输入。它将在映射器中设置键和值对后使用。

    ```r
    write(paste(city,pagepath,sep="\t"),stdout())

    ```

*   `print`:此功能用于将数据写入文件或标准输入。它将在映射器中的键和值对准备就绪后使用。

    ```r
    print(paste(city,pagepath,sep="\t"),stdout())

    ```

*   `close`:该功能可以是在读写操作完成后关闭与文件的连接。当所有过程完成后，可在关闭(`conn`)端与映射器和减速器一起使用。
*   `stdin`:这是输入对应的标准连接。`stdin()`功能是返回连接对象的文本模式连接。该功能将在 Mapper 和 Reducer 中使用。

    ```r
    conn <- file("stdin", open="r")

    ```

*   `stdout`:这个是对应输出的标准连接。`stdout()`功能是文本模式连接，也返回对象。该功能将在 Mapper 和 Reducer 中使用。

    ```r
    print(list(city.key, page.value),stdout())

    ## where city.key is key and page.value is value of that key

    ```

*   `sink` : `sink`驱动 R 输出到连接。如果有文件或流连接，输出将返回文件或流。这将在 Mapper 和 Reducer 中用于跟踪所有功能输出和错误。

    ```r
    sink("log.txt")
    k <- 1:5
    for(i in 1:k){
    print(paste("value of k",k))
    }sink()
    unlink("log.txt")
    ```

## 监控 Hadoop MapReduce 作业

reduce阶段的一个小的语法错误导致了 MapReduce 作业的失败。Hadoop MapReduce 作业失败后，我们可以从 Hadoop MapReduce 管理页面跟踪问题，在该页面中，我们可以获得关于正在运行的作业以及已完成作业的信息。

如果作业失败，我们可以看到已完成/失败的映射和缩减作业的总数。单击失败的作业将提供这些特定数量的映射器或缩减器失败的原因。

另外，我们可以用 JobTracker 控制台查看那个正在运行的 MapReduce 作业的实时进度，如下图截图所示:

![Monitoring the Hadoop MapReduce job](graphics/3282OS_04_07.jpg)

监控 Hadoop MapReduce 作业

通过该命令，我们可以通过使用以下命令指定特定 MapReduce 作业的输出目录来检查该作业的历史记录:

```r
$ bin/hadoop job –history /output/location 

```

以下命令将打印 MapReduce 作业的详细信息、失败的作业以及终止作业的原因。

```r
$ bin/hadoop job -history all /output/location 

```

前面的命令将打印成功的任务以及每个任务的任务尝试。

# 探索 HadoopStreaming R 包

HadoopStreaming 是由*大卫·s·罗森伯格*开发的 R 包。我们可以说这是一个简单的用于 MapReduce 脚本的框架。这也可以在没有 Hadoop 的情况下运行，以流式方式操作数据。我们可以将这个 R 包看作是一个 Hadoop MapReduce 发起者。对于任何不能在命令提示符下调用 Hadoop streaming 命令的分析师或开发人员来说，这个包将有助于快速运行 Hadoop MapReduce 作业。

该软件包的三个主要特性如下:

*   分块数据读取:该包允许为 Hadoop 流进行分块数据读取和写入。该功能将克服内存问题。
*   支持各种数据格式:该包允许以三种不同的数据格式读写数据。
*   Hadoop 流命令的强大实用程序:该包还允许用户为 Hadoop 流指定命令行参数。

该软件包主要设计有三个功能，用于高效读取数据:

*   `hsTableReader`
*   `hsKeyValReader`
*   `hsLineReader`

现在，让我们理解这些功能及其用例。之后我们将借助字数统计 MapReduce 作业了解这些功能。

## 理解 hsTableReader 功能

`hsTableReader`功能是为了读取表格格式的数据而设计的。这个函数假设有一个与文件建立的输入连接，所以它将检索整个行。它假设具有相同键的所有行都连续存储在输入文件中。

由于 Hadoop 流作业保证了 Mappers 的输出行在提供给 Reduce 之前会被排序，因此在 Hadoop 流 MapReduce 作业中不需要使用`sort`函数。当我们没有在 Hadoop 上运行这个时，我们需要在`Mapper` 函数被执行后明确地调用`sort`函数。

定义`hsTableReader`的功能:

```r
hsTableReader(file="", cols='character',
 chunkSize=-1, FUN=print,
 ignoreKey=TRUE, singleKey=TRUE, skip=0,
 sep='\t', keyCol='key',
 FUN=NULL, ,carryMemLimit=512e6,
 carryMaxRows=Inf,
 stringsAsFactors=FALSE)

```

前面代码中的术语如下:

*   `file`:这是一个连接对象、流或字符串。
*   `chunkSize`:表示函数一次可以读取的最大行数。`-1`指一次所有的线路。
*   `cols`:这意味着要扫描的列名列表作为“what”参数。
*   `skip`:用于跳过前 n 个数据行。
*   `FUN`:该功能将使用用户输入的数据。
*   `carryMemLimit`:表示单键数值的最大内存限制。
*   `carryMaxRows`:这表示要考虑或从文件中读取的最大行数。
*   `stringsAsFactors`:定义字符串是否转换为因子(`TRUE`或`FALSE`)。

例如，文件中的数据:

```r
# Loading libraries
Library("HadoopStreaming")
# Input data String with collection of key and values
str <- " key1\t1.91\nkey1\t2.1\nkey1\t20.2\nkey1\t3.2\nkey2\t1.2\nkey2\t10\nkey3\t2.5\nkey3\t2.1\nkey4\t1.2\n"cat(str)

```

前面代码的输出如下图所示:

![Understanding the hsTableReader function](graphics/3282OS_04_08.jpg)

`hsTableReader`读取的数据为如下:

```r
# A list of column names, as'what' arg to scan
cols = list(key='',val=0)

# To make a text connection
con <- textConnection(str, open = "r")
# To read the data with chunksize 3
hsTableReader(con,cols,chunkSize=3,FUN=print,ignoreKey=TRUE)

```

前面代码的输出如下图所示:

![Understanding the hsTableReader function](graphics/3282OS_04_09.jpg)

## 理解 hsKeyValReader 功能

`hsKeyValReader`功能用于读取键值对格式的数据。该功能还使用`chunkSize`定义一次要读取的行数，每行由一个键串和值串组成。

```r
hsKeyValReader(file = "", chunkSize = -1, skip = 0, sep = "\t",FUN = function(k, v) cat(paste(k, v))

```

该功能的术语类似`hsTablereader()`。

示例:

```r
# Function for reading chunkwise dataset
printkeyval <- function(k,v) {cat('A chunk:\n')cat(paste(k,v,sep=': '),sep='\n')}
str <- "key1\tval1\nkey2\tval2\nkey3\tval3\n"
con <- textConnection(str, open = "r")

hsKeyValReader(con, chunkSize=1, FUN=printFn)

```

前面代码的输出如下图所示:

![Understanding the hsKeyValReader function](graphics/3282OS_04_10.jpg)

## 理解 hsLineReader 功能

`hsLineReader`函数被设计为将整行作为字符串读取，而不执行数据解析操作。它从文件中反复读取`chunkSize`行数据，并将这些字符串的字符向量传递给`FUN`。

```r
hsLineReader(file = "", chunkSize = 3, skip = 0, FUN = function(x) cat(x, sep = "\n"))

```

该功能的术语类似`hsTablereader()`。

示例:

```r
str <- " This is HadoopStreaming!!\n here are,\n examples for chunk dataset!!\n in R\n  ?"

#  For defining the string as data source
con <- textConnection(str, open = "r")

# read from the con object
hsLineReader(con,chunkSize=2,FUN=print)

```

前面代码的输出如下图所示:

![Understanding the hsLineReader function](graphics/3282OS_04_11.jpg)

您可以在[http://cran . r-project . org/web/packages/Hadoop streaming/Hadoop streaming . pdf](http://cran.r-project.org/web/packages/HadoopStreaming/HadoopStreaming.pdf)上获得这些方法以及其他现有方法的更多信息。

现在，我们将使用在 Hadoop 上运行的 Hadoop MapReduce 程序来实现上述数据读取方法。在某些情况下，键值对或数据行不会被送入机器内存；所以按块读取数据比改进机器配置更合适。

问题定义:

Hadoop 字数统计:我们已经知道什么是字数统计应用程序，我们将用字数统计的概念来实现上面给出的方法。这个 R 脚本是从 HadoopStreaming R 包中复制过来的，可以作为示例代码与 HadoopStreaming R 库发行版一起下载。

输入数据集:取自俄罗斯作家*列夫·托尔斯泰*《T2》《安娜·卡列尼娜》《T3》(小说)第一章[。](1.html "Chapter 1. Getting Ready to Use R and Hadoop")

r 脚本:这个部分包含映射器、减速器的代码，以及其余的配置参数。

文件:`hsWordCnt.R`

```r
## Loading the library
library(HadoopStreaming)

## Additional command line arguments for this script (rest are default in hsCmdLineArgs)
spec = c('printDone','D',0,"logical","A flag to write DONE at the end.",FALSE)

opts = hsCmdLineArgs(spec, openConnections=T)

if (!opts$set) {
 quit(status=0)
}

# Defining the Mapper columns names
mapperOutCols = c('word','cnt')

# Defining the Reducer columns names
reducerOutCols = c('word','cnt')

# printing the column header for Mapper output
if (opts$mapcols) {
 cat( paste(mapperOutCols,collapse=opts$outsep),'\n', file=opts$outcon )
} 

# Printing the column header for Reducer output 
if (opts$reducecols) {
 cat( paste(reducerOutCols,collapse=opts$outsep),'\n', file=opts$outcon )
}

## For running the Mapper
if (opts$mapper) {
 mapper <- function(d) {
    words <- strsplit(paste(d,collapse=' '),'[[:punct:][:space:]]+')[[1]] # split on punctuation and spaces
    words <- words[!(words=='')]  # get rid of empty words caused by whitespace at beginning of lines
    df = data.frame(word=words)
    df[,'cnt']=1

# For writing the output in the form of key-value table format
hsWriteTable(df[,mapperOutCols],file=opts$outcon,sep=opts$outsep)
  }

## For chunk wise reading the Mapper output, to be feeded to Reducer hsLineReader(opts$incon,chunkSize=opts$chunksize,FUN=mapper)

## For running the Reducer
} else if (opts$reducer) {

  reducer <- function(d) {
    cat(d[1,'word'],sum(d$cnt),'\n',sep=opts$outsep)
  }
  cols=list(word='',cnt=0)  # define the column names and types (''-->string 0-->numeric)
  hsTableReader(opts$incon,cols,chunkSize=opts$chunksize,skip=opts$skip,sep=opts$insep,keyCol='word',singleKey=T, ignoreKey= F, FUN=reducer)
  if (opts$printDone) {
    cat("DONE\n");
  }
}

# For closing the connection corresponding to input
if (!is.na(opts$infile)) {
  close(opts$incon)
}

# For closing the connection corresponding to input
if (!is.na(opts$outfile)) {
  close(opts$outcon)
}

```

## 运行 Hadoop 流作业

由于这是一个 Hadoop 流作业，它将与前面执行的 Hadoop 流作业示例运行相同。对于这个例子，我们将使用一个 shell 脚本来执行`runHadoop.sh`文件，以运行 Hadoop 流。

设置系统环境变量:

```r
#! /usr/bin/env bash
HADOOP="$HADOOP_HOME/bin/hadoop"   # Hadoop command

HADOOPSTREAMING="$HADOOP jar
$HADOOP_HOME/contrib/streaming/hadoop-streaming-1.0.3.jar" # change version number as appropriate

RLIBPATH=/usr/local/lib/R/site-library  # can specify additional R Library paths here

```

设置 MapReduce 作业参数:

```r
INPUTFILE="anna.txt"
HFSINPUTDIR="/HadoopStreaming"
OUTDIR="/HadoopStreamingRpkg_output"

RFILE=" home/hduser/Desktop/HadoopStreaming/inst/wordCntDemo/ hsWordCnt.R"
#LOCALOUT="/home/hduser/Desktop/HadoopStreaming/inst/wordCntDemo/annaWordCnts.out"
# Put the file into the Hadoop file system
#$HADOOP fs -put $INPUTFILE $HFSINPUTDIR

```

删除现有的输出目录:

```r
# Remove the directory if already exists (otherwise, won't run)
#$HADOOP fs -rmr $OUTDIR

```

使用通用和流式选项设计 Hadoop MapReduce 命令:

```r
MAPARGS="--mapper" 
REDARGS="--reducer"
JOBARGS="-cmdenv R_LIBS=$RLIBPATH" # numReduceTasks 0
# echo $HADOOPSTREAMING -cmdenv R_LIBS=$RLIBPATH  -input $HFSINPUTDIR/$INPUTFILE -output $OUTDIR -mapper "$RFILE $MAPARGS" -reducer "$RFILE $REDARGS" -file $RFILE 

$HADOOPSTREAMING $JOBARGS   -input $HFSINPUTDIR/$INPUTFILE -output $OUTDIR -mapper "$RFILE $MAPARGS" -reducer "$RFILE $REDARGS" -file $RFILE 

```

从 HDFS 提取输出到本地目录:

```r
# Extract output
./$RFILE --reducecols > $LOCALOUT
$HADOOP fs -cat $OUTDIR/part* >> $LOCALOUT

```

### 执行 Hadoop 流作业

我们现在可以通过执行命令`runHadoop.sh`来执行 Hadoop 流作业。要执行此操作，我们需要设置用户权限。

```r
sudo chmod +x runHadoop.sh

```

通过以下命令执行:

```r
./runHadoop.sh

```

最后，它将执行整个 Hadoop 流作业，然后将输出复制到本地目录。

# 总结

我们已经学习了大多数集成 R 和 Hadoop 来执行数据操作的方法。在下一章中，我们将了解数据分析周期，以便在 R 和 Hadoop 的帮助下解决现实世界的数据分析问题。